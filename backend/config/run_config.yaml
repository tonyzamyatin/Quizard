# TODO: Add parameters, optimize and test!

model:
  temperature: 0.8
  top_p: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0
  max_tokens: 800   # Default is 800, but this value is dynamically adapted within the program depending on the prompt size


tokens:
  # Short text config for 4k model
  4k_model:
    prompt_limit: 3000
    base_prompt_limit: 2000
    completion_limit: 800
  # Medium text config for 16k model
  16k_model:
    prompt_limit: 13000
    base_prompt_limit: 4000
    completion_limit: 10000

  # For texts longer than 12k tokens text splitting is used
  # Window size will be calculated by the program automatically depending on the size of the base prompt (i.e. system prompt + example)
  # The window overlap can be either set relative to the text fragment being processed via relative_window_overlap or set to an absolute token value.
  # Depending on what setting is being used
  text_splitting:
    window_overlap_type: "absolute"         # "relative" or "absolute"
    relative_window_overlap: 0.167
    absolute_window_overlap: 100

  # Optimal parameters:
  # 3.000, 2.400 and 400 when using the 4k model
  # _____, 10.000 and 10.400 when using the 16k model


flashcard_generation:
  mode: "practice"       # "practice"....other options are coming soon -> can be found in backend/prompt/system
  detect_lang: False    # True or False, if False flashcards will be generated in the specified language below.
  lang: "German"
  example_prompt: "ex1_genetics_of_cancer"     # Name of the example directory
  additional_prompt: "ad1_top_instr"           # Change this field to a list of (position, prompt) to add multiple additional lines of prompt to text_input



export:
  export_type: "csv"