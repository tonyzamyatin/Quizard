# TODO: Add parameters, optimize and test!

model:
  name: "gpt-3.5-turbo"   # Base model to use for text splitting: gpt-3.5-turbo or gpt-3.5-turbo-16k
  temperature: 0.8
  top_p: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0


tokens:
  # Short text config for 4k model
  4k_model:
    prompt_limit: 3000
    base_prompt_limit: 2000
    completion_limit: 800
  # Medium text config for 16k model
  16k_model:
    prompt_limit: 13000
    base_prompt_limit: 4000
    completion_limit: 1000

  # For texts longer than 12k tokens text splitting is used
  # Window size will be calculated by the program automatically depending on the size of the base prompt (i.e. system prompt + example)
  # The window overlap can be either set relative to the text fragment being processed via relative_window_overlap or set to an absolute token value.
  # Depending on what setting is being used
  text_splitting:
    window_overlap_type: "absolute"         # "relative" or "absolute"
    relative_window_overlap: 0.167
    absolute_window_overlap: 100

  # Optimal parameters:
  # 3.000, 2.400 and 400 when using the 4k model
  # _____, 10.000 and 10.400 when using the 16k model


flashcard_generation:
  mode: "autogen"       # "autogen" or "open_ended"
  detect_lang: False   # True or False, if False flashcards will be generated in the specified language below.
  lang: "German"


export:
  export_type: "csv"