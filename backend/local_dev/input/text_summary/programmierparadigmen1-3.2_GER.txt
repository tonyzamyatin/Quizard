1 Grundlagen und Zielsetzungen
Im Zusammenhang mit der Programmierung verstehen wir unter dem Begriff
Paradigma eine bestimmte Denkweise oder Art der Weltanschauung. Entsprechend entwickeln wir Programme in einem gewissen Stil. Nicht jeder individuelle
Programmierstil ist gleich ein eigenes Paradigma, sondern nur solche Stile, die
sich grundlegend voneinander unterscheiden. Einander ähnliche Stile mit gemeinsamen charakteristischen Eigenschaften fallen unter dasselbe Paradigma.
Diese Eigenschaften betreffen ganz unterschiedliche Aspekte, etwa das zugrundeliegende Berechnungsmodell, die Strukturierung des Programmablaufs oder
Datenflusses, die Aufteilung großer Programme in überschaubare Einzelteile
und so weiter. Daraus ergibt sich eine Vielzahl an Paradigmen, die sich im
Laufe der Zeit entwickelt und eine mehr oder weniger große Verbreitung gefunden haben. In engem Zusammenhang mit dem Erfolg von Paradigmen steht die
Verfügbarkeit und Qualität entsprechender Programmiersprachen und Entwicklungswerkzeuge. Nicht selten definiert sich ein Paradigma durch seine Sprachen
und Werkzeuge. Entscheidend sind die verfolgten Ziele. Unterschiedliche Zielsetzungen führen zu unterschiedlichen Paradigmen, auch wenn gleiche Sprachen
und Werkzeuge eingesetzt und ähnliche Aspekte berücksichtigt werden.
Häufig wird das auf Maschinenbefehlen aufbauende imperative vom auf formalen Modellen beruhenden deklarativen Paradigma unterschieden. Das imperative Paradigma wird nach der vorherrschenden Programmstrukturierung
in das prozedurale und objektorientierte Paradigma unterteilt, das deklarative
nach dem formalen Modell in das funktionale und logikorientierte Paradigma.
Diese etablierte, aber heute nicht mehr hinreichende Einteilung lässt zahlreiche
Paradigmen unberücksichtigt, die in der praktischen Softwareentwicklung weit
verbreitet oder in bestimmten Nischen von Bedeutung sind, etwa verschiedene Formen der parallelen und verteilten Programmierung. Diese werden (mehr
oder weniger willkürlich) einem der etablierten Paradigmen zugeschlagen.
Wir wollen in diesem Kapitel etwas Struktur in die Paradigmenvielfalt bringen, ohne Paradigmen auszuschließen, die nicht in eine althergebrachte Untergliederung passen. Dazu versuchen wir, Schwerpunkte auf Aspekte und Zielsetzungen hinter den Paradigmen zu legen, obwohl sich durch eine solche Einteilung ein nicht immer ganz orthogonales Gefüge ergibt. Auf diese Weise lässt
sich der Gestaltungsspielraum entsprechender Sprachen und Werkzeuge am besten abschätzen. Außerdem gibt uns diese Vorgehensweise Gelegenheit, diverse
Aspekte und Konzepte der Programmierung auf einer hohen Ebene zu betrachten und zu verschiedenen Zielen in Beziehung zu setzen. Quasi im Vorbeigehen
führen wir einige Grundlagen ein, auf denen weitergehende Betrachtungen von
Programmierparadigmen aufbauen. Wir sprechen die Bedeutung von Themen
an, die in späteren Kapiteln genauer untersucht werden.
9
1 Grundlagen und Zielsetzungen
1.1 Berechnungsmodell
Hinter jedem Programmierparadigma steckt ein Berechnungsmodell. Berechnungsmodelle haben immer einen formalen Hintergrund. Sie müssen in sich
konsistent und in der Regel so mächtig wie die Turing-Maschine sein, also alles
ausdrücken können, was als berechenbar gilt. Ein Formalismus eignet sich aber
nur dann als Grundlage eines Paradigmas, wenn die praktische Umsetzung ohne
übermäßig großen Aufwand zu Programmiersprachen, Entwicklungsmethoden
und Werkzeugen hinreichender Qualität führt. Es braucht Experimente und
Erfahrungen, um die praktische Eignung festzustellen.
1.1.1 Formaler Hintergrund
Es entstehen ständig neue Theorien, Formalismen, etc., die in einem Zweig der
Informatik von Bedeutung sind. Wir stellen eine Auswahl davon kurz vor und
betrachten ihren Einfluss auf Programmierparadigmen. Details der Formalismen sind Gegenstand anderer Lehrveranstaltungen.
Funktionen: In fast jedem Programmierparadigma spielen Funktionen oder
ähnliche Konzepte eine zentrale Rolle. Es gibt unterschiedliche Formalismen zur Beschreibung von Funktionen [31]. Am einfachsten zu verstehen
sind primitiv-rekursive Funktionen, die von einer vorgegebenen Menge
einfacher Funktionen ausgehen und daraus durch Komposition und Rekursion neue Funktionen bilden, so wie es in der Programmierung häufig gemacht wird. Primitiv-rekursive Funktionen können vieles berechnen, aber nicht alles, was berechenbar ist. Die Mächtigkeit der TuringMaschine wird durch µ-rekursive Funktionen erreicht, wo ein hinzugefügter Fixpunkt-Operator µ angewandt auf partielle Funktionen das kleinste
aller möglichen Ergebnisse liefert. Der genau so mächtige λ-Kalkül wurde unabhängig von primitiv- und µ-rekursiven Funktionen entwickelt und
beschreibt in der untypisierten Variante Funktionen ohne Notwendigkeit
für einen speziellen Fixpunkt-Operator.
Historisch gesehen ist nicht ganz klar, wie groß der Einfluss dieser Formalismen, vor allem des λ-Kalküls, auf die Entwicklung der Programmierparadigmen war. Aus praktischer Sicht interessant ist vor allem die
Möglichkeit, neue Funktionen einfach aus bestehenden Funktionen zusammenzusetzen. Dazu braucht es keine Theorie. Frühe imperative Programmiersprachen wie Algol enthielten dennoch λ-Ausdrücke. Aber in der damaligen Form haben sie sich nicht bewährt und sind bald verschwunden.
John McCarthy, der Entwickler von Lisp und wesentlicher Mitbegründer
der funktionalen Programmierung, hatte sich nach eigenen Angaben zuvor
kaum mit der Theorie der Funktionen beschäftigt [27]. Erst später wurden
Lisp-Dialekte um λ-Ausdrücke erweitert. Moderne funktionale Sprachen
verwenden den λ-Kalkül deutlich erkennbar als Basis. Erst in jüngerer
Zeit werden vom λ-Kalkül inspirierte sogenannte Lambda-Ausdrücke oder
Lambdas auch in der objektorientierten Programmierung verwendet.
10
1.1 Berechnungsmodell
Prädikatenlogik: Die Prädikatenlogik ist ein etabliertes, mächtiges mathematisches Werkzeug. Zu Beginn der Informatik wurde automatisches Beweisen in der Prädikatenlogik nicht selten als gleichbedeutend mit künstlicher Intelligenz angesehen. Es zeigte sich, dass Horn-Klauseln, eine Teilmenge der Prädikatenlogik, die Basis eines Berechnungsmodells mit der
Mächtigkeit der Turing-Maschine bilden. Seit etwa 1965 sind mittels Resolution und Unifikation mehr oder weniger automatisch Beweise über
Horn-Klauseln ableitbar [32]. Daraus entstand in den 1970er-Jahren die
Programmiersprache Prolog [10]. Bis in die frühen 1990er-Jahre galt die logikorientierte Programmierung als der große Hoffnungsträger zur Lösung
unzähliger Probleme, etwa die Überwindung der Kluft zwischen Softwareentwicklung und -anwendung, die Automatisierung der Programmierung
und die Unterstützung verteilter und hochgradig paralleler Hardware.
Man sprach von Programmiersprachen der 5. Generation. Umfangreiche
Forschungsarbeiten führten zwar zu wichtigen Erkenntnissen, aber die viel
zu hoch gesteckten Ziele wurden nicht einmal annähernd erreicht. Heute
spielt die logikorientierte Programmierung im engeren Sinn nur mehr eine untergeordnete Rolle. Ihr Einfluss ist dennoch stark. So verwenden wir
häufig relationale Datenbanken mit logikorientierten Abfragesprachen.
Constraint-Programmierung: Ein Zweig der Programmiersprachen der 5. Generation verwendete Randbedingungen auf Variablen („constraints“) wie
etwa „x < 5“ zusätzlich zu Bedingungen wie „A oder B ist wahr“. Aktuelle
Beweistechniken können Randbedingungen vergleichsweise effizient auflösen. Dieser Zweig hat sich verselbständigt. Constraint-Programmierung
ist heute mit funktionalen und imperativen Sprachen kombinierbar, nicht
nur mit logikorientierten. Dafür werden vorwiegend fertige Bibliotheken
verwendet, die fast überall eingebunden werden können.
Temporale Logik und Petri-Netze: In temporaler Logik sind zeitliche Abhängigkeiten in logischen Ausdrücken recht einfach abbildbar. Beispielsweise
wird festgelegt, dass eine Aussage nur vor oder nach einem bestimmten
Ereignis gilt. Es gibt mehrere Arten der temporalen Logik. Häufig ist eine temporale Logik die erste Wahl, wenn zeitabhängige Aussagen oder
Ereignisse formal zu beschreiben sind, etwa die Synchronisation in nebenläufigen Programmen oder die Steuerung von Maschinen.
In denselben Bereichen sind auch verschiedene Arten von Petri-Netzen
verwendbar, die auf intuitiv einfach verständlichen Automaten aufbauen.
Temporale Logiken sind in der Regel in Petri-Netze umwandelbar und
Petri-Netze in temporale Logiken. Während Petri-Netze gute Möglichkeiten zur grafischen Veranschaulichung komplexer zeitlicher Abhängigkeiten
bieten, ist die Beweisführung in temporalen Logiken einfacher.
Freie Algebren: Algebra ist ein sehr altes und etabliertes Teilgebiet der Mathematik, das sich mit Eigenschaften von Rechenoperationen befasst. Gleichzeitig ist eine (universelle) Algebra auch ein mathematisches Objekt, etwa
11
1 Grundlagen und Zielsetzungen
eine Gruppe, ein Ring, ein Körper, etc. Von besonderer Bedeutung für die
Informatik sind sogenannte freie Algebren. Diese universellen Algebren
sind, stark vereinfacht formuliert, die allgemeinsten Algebren innerhalb
von Familien von Algebren mit gemeinsamen Eigenschaften. Freie Algebren erlauben uns die Spezifikation beinahe beliebiger Strukturen, beispielsweise von Datenstrukturen, über einfache Axiome. Auch wenn freie
Algebren im Zusammenhang mit Programmierparadigmen nicht so dominant sind wie z. B. Funktionen, so spielen sie doch in einigen Bereichen
eine wichtige Rolle, etwa im Zusammenhang mit Modulen und Typen.
Auf freien Algebren basieren aber auch viele Spezifikationssprachen.
Prozesskalküle: Eine Familie speziell dafür entwickelter Algebren eignet sich
gut zur Modellierung von Ausführungseinheiten (Prozesse genannt) in
nebenläufigen und parallelen Systemen. Die bekanntesten Prozesskalküle
sind CSP (Communicating Sequential Processes) [15] und π-Kalkül [30].
Primitive Operationen gibt es ausschließlich für das Senden und Empfangen von Daten. Sie sind durch Hintereinanderausführung, Parallelausführung sowie alternative Ausführung miteinander kombinierbar. Während
die Mächtigkeit der Turing-Maschine im λ-Kalkül durch die Übergabe beliebiger Argumente bei Funktionsaufrufen erreicht wird, geschieht dies im
π-Kalkül durch das Senden und Empfangen beliebiger Daten durch Prozesse. Dieser Unterschied hat wichtige Konsequenzen: Im λ-Kalkül lassen
sich nur transformatorische Systeme gut beschreiben, die zum Zeitpunkt
des Programmstarts vorliegende Eingabedaten in Ergebnisdaten transformieren. Dagegen eignet sich der π-Kalkül zur Beschreibung reaktiver
Systeme, die auf Ereignisse in der Umgebung reagieren, wann immer diese
auftreten. Endlosberechnungen im λ-Kalkül liefern keine Ergebnisse und
sind sinnlos. Dagegen sind Endlosberechnungen im π-Kalkül wohldefiniert
und sinnvoll, aber Prozesse können sich gegenseitig blockieren.
Automaten: Die klassische Automatentheorie wurde in der Frühzeit der Informatik entwickelt. Automaten unterschiedlicher Komplexität stellen gleichzeitig verschiedene Arten von Grammatiken und entsprechende Sprachklassen dar [16]. Obwohl Grammatiken nur Syntax beschreiben, sind die
mächtigsten unter ihnen so mächtig wie die Turing-Maschine. Zur Syntaxbeschreibung und in der Implementierung von Programmiersprachen
spielen Konzepte aus der Automatentheorie nach wie vor eine große Rolle,
als Grundlage von Programmierparadigmen aber nur am Rande. Wegen
ihrer anschaulichen Darstellung werden Automaten nicht selten zur Spezifikation des Systemverhaltens verwendet.
WHILE, GOTO und Co: So manches typische Sprachkonstrukt imperativer
Sprachen hatte zum Zeitpunkt seiner Entstehung keinerlei formalen Hintergrund. Entsprechende Formalismen mussten erst geschaffen und analysiert werden. Beispiele dafür sind die WHILE- und die GOTO-Sprache,
ganz einfache formale Sprachen, in denen es außer Zuweisungen, primitiven arithmetischen Operationen und bedingten Anweisungen nur entwe12
1.1 Berechnungsmodell
der eine While-Schleife oder eine Goto-Anweisung (Sprung an eine beliebige andere Programmstelle) gibt. Diese beiden Sprachen sind so mächtig
wie die Turing-Maschine. Im Gegensatz dazu ist die LOOP-Sprache, in
der es statt While bzw. Goto eine Schleife mit einer vorgegebenen Anzahl an Iterationen gibt, nur äquivalent zu primitiv-rekursiven Funktionen. PRAM-Sprachen (Parallel Random Access Memory) ändern obige
Sprachen dahingehend ab, dass die dahinter stehenden Maschinenmodelle mehrere Operationen gleichzeitig auf unterschiedlichen Speicherzellen
durchführen können. Es ist klar, dass derartige formale Sprachen eine
starke Verbindung zu imperativen Programmierparadigmen haben.
Diese Aufzählung ist ganz und gar nicht vollständig. Es gibt viele weitere,
oft eher exotische Modelle und Formalismen, die in dem einen oder anderen
Paradigma eine größere Bedeutung erlangt haben.
1.1.2 Lambda-Kalkül
Wir betrachten eine Variante des λ-Kalküls etwas genauer. Gegeben sei eine
unendliche Menge von Variablen V , wobei zur Vereinfachung auch Konstanten
und Literale in V vorkommen. Die Menge E aller wohlgeformten (das heißt,
syntaktisch korrekten) λ-Ausdrücke ist induktiv aufgebaut:
v ∈ E wenn v ∈ V
fe ∈ E wenn e, f ∈ E /∗ Anwendung von f auf e ∗/
λv.e ∈ E wenn v ∈ V ; e ∈ E /∗ Funktion; Parameter v, Ergebnis e ∗/
Funktionen heißen auch λ-Abstraktionen. Wir fügen nach Bedarf Klammern
hinzu, um die Struktur der λ-Ausdrücke eindeutig zu beschreiben. Beispielsweise steht λv.(v v) für eine λ-Abstraktion, die als Ergebnis v v zurückgibt,
wobei v durch das bei einer Anwendung an die Funktion übergebene Argument
ersetzt wird. Entsprechend steht (λv.(v v))(a b) für die Anwendung der Funktion auf a b, was äquivalent zu (a b)(a b) ist. Zur Vermeidung zu vieler Klammern
wird (e f)g üblicherweise als e f g geschrieben, λv.(ef) als λv.ef, λu.(λv.e) als
λu.λv.e und so weiter. Wir verwenden dennoch immer Klammern, um Missverständnisse zu vermeiden. Im Wesentlichen beschreibt der λ-Kalkül eine Äquivalenzrelation (genauer eine Kongruenzrelation, also eine Äquivalenzrelation mit
speziellen Eigenschaften) zwischen den Ausdrücken in E. Zwei λ-Ausdrücke
werden als äquivalent betrachtet, wenn sie durch Anwendung einiger weniger
Regeln ineinander umgewandelt werden können.
Wir benötigen die Menge fv(e) der in e frei vorkommenden Variablen:
fv(v) = {v} (v ∈ V )
fv(fe) = fv(f) ∪ fv(e) /∗ fv(f) vereinigt mit fv(e) ∗/
fv(λv.e) = fv(e) \ {v} /∗ fv(e) ohne v ∗/
fv(e) enthält also alle in e vorkommenden Variablen, außer jenen, die als Parameter gebunden vorkommen. Wir benötigen auch die Ersetzung: Der Ausdruck
13
1 Grundlagen und Zielsetzungen
[e/v]f mit v ∈ V wird gelesen als „e ersetzt alle freien Vorkommen von v in f“
und steht für den λ-Ausdruck, der durch die Ersetzung entsteht:
[e/v]v = e
[e/v]u = u (u ∈ V ; u 6= v)
[e/v](f g) = ([e/v]f)([e/v]g)
[e/v](λv.f) = λv.f
[e/v](λu.f) = λu.[e/v]f (u 6= v; u 6∈ fv(e))
Der Parameter v einer Funktion wird nicht ersetzt, da der Parameter für einen
anderen Ausdruck steht als v außerhalb der Funktion. Auffällig ist die Einschränkung u 6∈ fv(e) in der letzten Zeile: Die Ersetzung ist nicht möglich,
wenn u in e frei vorkommt, weil u als Parameter in der Funktion für einen anderen Ausdruck steht als u außerhalb der Funktion. Wir sehen gleich, dass die
erste Regel des λ-Kalküls solche Namenskonflikte beseitigen kann.
Hier sind die Äquivalenzregeln des λ-Kalküls (≡ steht für „ist äquivalent zu“):
λu.e ≡ λv.[v/u]e (v 6∈ fv(e)) /∗ α-Konversion ∗/
(λv.f)e ≡ [e/v]f /∗ β-Konversion ∗/
λv.(e v) ≡ e (v 6∈ fv(e)) /∗ η-Konversion ∗/
Die α-Konversion erlaubt uns, Parameter beliebig umzubenennen, solange dies
zu keinen Namenskonflikten führt; daher die Einschränkung v 6∈ fv(e). Die wichtigste Regel ist die β-Konversion, die besagt, dass eine Funktion λv.f angewandt auf ein Argument e äquivalent zum Funktionsergebnis [e/v]f, also dem
Ergebnis f, in dem alle freien Vorkommen des Parameters v durch das Argument e ersetzt sind. Dabei ist zu beachten, dass Vorkommen von v zwar in λv.f
gebunden sind, aber nicht in f (außer für jene möglicherweise existierenden v,
die Parameter einer anderen, in f enthaltenen Funktion sind). Die Ersetzung
beschreibt die Parameterübergabe. Die relativ unwichtige η-Konversion sorgt
dafür, dass alle Funktionen äquivalent sind, die für gleiche Argumente gleiche
Ergebnisse liefern; sie kann für Optimierungen eingesetzt werden. Betrachten
wir den λ-Ausdruck (λv.(fv))e. Durch Anwendung der β-Konversion erkennen
wir, dass dieser Ausdruck äquivalent zu fe ist. Aber auch die η-Konversion
ist anwendbar und führt zu fe. Wenn beide Regeln anwendbar sind, spielt es
keine Rolle, welche wir verwenden. Aber die η-Konversion ist auch anwendbar,
wenn der Teilausdruck e nicht dabei steht. Die β-Konversion ist dagegen auch
anwendbar, wenn das Ergebnis der Funktion nicht genau diese Form hat.
Obere Regeln sind ungerichtet, sie können von rechts nach links ebenso angewendet werden wie von links nach rechts. Das ist unpraktisch, weil wir viel Intuition benötigen, um herauszufinden, wie wir einen λ-Ausdruck in einen anderen,
äquivalenten λ-Ausdruck umformen können. Zur Vereinfachung verwenden wir
statt der Äquivalenzregeln meist Reduktionsregeln, die immer nur von links nach
rechts anwendbar sind und damit λ-Ausdrücke vereinfachen (reduzieren):
(λv.f)e → [e/v]f /∗ β-Reduktion ∗/
λv.(e v) → e (v 6∈ fv(e)) /∗ η-Reduktion ∗/
14
1.1 Berechnungsmodell
Der einzige Unterschied ist die vorgegebene Richtung, abgesehen davon, dass es
für die α-Konversion keine gerichtete Variante gibt, da eine Umbenennung in
jede Richtung gleich funktioniert. Wir verwenden β- und η-Reduktion zusammen mit α-Konversion. Wir sagen, ein λ-Ausdruck sei in Normalform, wenn er
maximal reduziert ist, darauf also keine β- oder η-Reduktion mehr anwendbar
ist. Reduktionen und Normalformen haben einige wichtige Eigenschaften:
• Nicht zu jedem λ-Ausdruck gibt es einen äquivalenten λ-Ausdruck in Normalform. Es gibt λ-Ausdrücke, die wir beliebig oft reduzieren können, ohne sie dabei kleiner werden zu lassen. Ein Beispiel ist (λv.(v v))(λv.(v v)):
β-Reduktion ist anwendbar und führt wieder zu genau dem gleichen λAusdruck – eine Endlosreduktionsfolge.
• Auch wenn es zu einem λ-Ausdruck einen äquivalenten λ-Ausdruck in
Normalform gibt, kann es dennoch unendlich lange Reihenfolgen von Reduktionen geben, die nicht zu einer Normalform führen. Das ist dann der
Fall, wenn Regeln immer wieder auf den gleichen Teilausdrücken angewandt werden, die nichts zur Erreichung der Normalform beitragen.
• Jede Reihenfolge von Anwendungen der Regeln auf einen λ-Ausdruck, die
zu einer Normalform führt, führt (bis auf α-Konversion) zur gleichen Normalform. Z. B. führt die Ableitung (λu.(λv.(u v))b)a → (λv.(a v))b → a b
zum gleichen Ausdruck wie (λu.(λv.(u v))b)a → (λu.(u b))a → a b. Wenn
wir eine Normalform finden, wissen wir, dass diese (bis auf Umbenennungen) die einzige ist. Umbenennungen durch α-Konversion sind immer
möglich, wenn Funktionen vorkommen.
• Wenn Anwendungen der Regeln auf zwei λ-Ausdrücke zu gleichen λAusdrücken führen, wissen wir, dass die λ-Ausdrücke äquivalent zueinander sind. Um Äquivalenz zu zeigen, versuchen wir auf Normalformen
zu reduzieren. Nur bei Äquivalenz sind die jeweiligen Normalformen gleich
(bis auf α-Konversion).
Wir haben schon erwähnt, dass der λ-Kalkül alles berechnen kann, was als
berechenbar gilt. Berechnen heißt, dass wir λ-Ausdrücke auf Normalformen reduzieren. Aufgrund des bekannten Halteproblems der Turing-Maschine muss es,
wie oben gezeigt, λ-Ausdrücke ohne Normalform geben. Überraschend ist eher,
mit welch einfachen Mitteln die Mächtigkeit der Turing-Maschine erreicht wird.
Im Vergleich zu Programmiersprachen kann der λ-Kalkül fast nichts. Es gibt
keine vorgegebenen Zahlen und Grundrechenoperationen, keine Schleifen, keine if-Anweisungen, nicht einmal Funktionen mit mehreren Parametern. Einzig
und alleine ganz einfache Funktionen mit je einem einzigen Parameter existieren. Daraus können wir alles andere aufbauen, auch die grundlegenden Objekte
der Mathematik wie Mengen und Zahlensysteme. Ermöglicht wird das dadurch,
dass im λ-Kalkül alle λ-Ausdrücke als Argumente an Funktionen übergeben
und als Ergebnisse zurückgegeben werden können, auch Funktionen selbst. Der
Umgang mit dem λ-Kalkül auf niedrigem Niveau ist zwar oft schwierig und
ineffizient, die Verwendung von Funktionen darin aber vergleichsweise einfach.
15
1 Grundlagen und Zielsetzungen
Betrachten wir einige Beispiele für grundlegende Funktionalität im λ-Kalkül.
Zunächst stellen wir Funktionen mit mehreren Parametern dar. Das gelingt,
indem wir eine Funktion mit einem Parameter verwenden, die eine weitere
Funktion mit einem Parameter zurückgibt, die eine weitere Funktion mit einem Parameter zurückgibt, ... (so viele Parameter wie nötig), die dann das
eigentliche Ergebnis zurückgibt. Folgende Funktion nimmt zwei Parameter:
λu.(λv.((u v)(v u)))
Angewandt auf a und b sind folgende β-Reduktionen ausführbar:
((λu.(λv.((u v)(v u))))a)b → (λv.((a v)(v a)))b → (a b)(b a)
Statt a und b könnten wir beliebig komplexe λ-Ausdrücke verwenden.
Folgende drei Funktionen zeigen den Umgang mit Wahrheitswerten:
λu.(λv.u) /∗ true: nimm erstes Argument ∗/
λu.(λv.v) /∗ false: nimm zweites Argument ∗/
λb.(λu.(λv.((b u)v))) /∗ if_then_else ∗/
Durch Anwendung von if_then_else auf true (Teilausdruck dafür zur besseren Übersicht unterstrichen), a und b ergeben sich folgende β-Reduktionen:
(((λb.(λu.(λv.((b u)v))))(λu.(λv.u)))a)b →
((λu.(λv.(((λu.(λv.u))u)v)))a)b →
(λv.(((λu.(λv.u))a)v))b →
((λu.(λv.u))a)b →
(λv.a)b →
a
Die Variablen u und v im unterstrichenen Teilausdruck sind gebunden und von
Ersetzungen gleichnamiger Variablen weiter außen nicht betroffen. Bei Verwendung von false statt true wäre der Ausdruck zu b reduzierbar.
Zur Darstellung von Zahlen gibt es viele Möglichkeiten. Beispielsweise ist die
Zahl 0 einfach durch 0 (mit 0 ∈ V ) darstellbar und darauf ein s (irgendein Name
mit s ∈ V ) anwendbar, um die Zahl um 1 zu erhöhen. Damit entspricht s 0 der
Zahl 1, s(s 0) der Zahl 2, s(s(s 0)) der Zahl 3 und so weiter. Darauf aufbauend
können wir grundlegende Rechenoperationen als Funktionen definieren.
1.1.3 Praktische Realisierung
Beim Programmieren denken wir kaum bewusst an Berechnungsmodelle, sondern eher an Programmierwerkzeuge und diverse Details der Syntax und Semantik einer Sprache. Es geht um die Lösung praktischer Aufgaben. Im Idealfall
unterstützen uns die Werkzeuge und Sprachen bei der Lösung der Aufgaben in
einer zum Berechnungsmodell passenden Weise. Trotz der Unterschiedlichkeit
der Aufgaben, Sprachen, Modelle und Werkzeuge bestimmen immer wieder dieselben Eigenschaften den Erfolg oder Misserfolg eines Programmierparadigmas
zu einem großen Teil mit:
16
1.1 Berechnungsmodell
Kombinierbarkeit: Bestehende Programmteile sollen sich möglichst einfach zu
größeren Einheiten kombinieren lassen, ohne dass diese größeren Einheiten dabei ihre einfache Kombinierbarkeit einbüßen. Funktionen liefern ein
gutes Beispiel dafür: Eine Funktion setzt sich einfach aus Aufrufen weiterer Funktionen zusammen. Nicht alle Formalismen liefern eine so gute
Basis für kombinierbare Sprachkonzepte. Beispielsweise entstehen aus der
Kombination mehrerer Automaten zu größeren Automaten meist äußerst
komplexe Strukturen. Mechanismen für die Kombination müssen gut skalieren, das heißt, auch große Einheiten müssen einfach kombinierbar sein,
nicht nur kleine. Gerade wenn es um die Entwicklung großer Programme geht, ist die einfache Kombinierbarkeit extrem wichtig. Vor allem in
der objektorientierten Programmierung reicht auch die gute Kombinierbarkeit von Funktionen nicht mehr aus und wir verwenden zusätzliche
Sprachkonzepte wie Objekte zur Verbesserung der Kombinierbarkeit.
Konsistenz: Programmiersprachen und -paradigmen müssen zahlreiche Erwartungen hinsichtlich verschiedenster Aspekte erfüllen. Ein einziger Formalismus reicht als Grundlage dafür nicht aus. Mehrere sprachliche Konzepte
– etwa solche zur Beschreibung von Algorithmen und andere zur Beschreibung von Datenstrukturen – müssen zu einer Einheit verschmolzen werden. Alle Konzepte sollen in sich und miteinander konsistent sein, also gut
zusammenpassen. Nun sind Formalismen von Haus aus oft nicht oder zumindest nicht vollständig kompatibel zueinander. Wir würden kaum ein
zufriedenstellendes Ergebnis erhalten, wenn wir die beste formale Grundlage für jeden Aspekt wählen und alle entsprechenden Formalismen zusammenfügen würden. Ein riesiges Konglomerat mit zahlreichen Widersprüchen würde entstehen. Gute Sprachen und Paradigmen kommen mit
wenigen konsistenten Konzepten aus. Im Umkehrschluss bedeutet das
aber auch, dass nicht in allen Aspekten auf den optimalen Grundlagen
aufgebaut werden kann, sondern viele Kompromisse nötig sind. Konsistenz ist in der Praxis wichtiger als Optimalität. Nur so kann sich die
nötige Einfachheit ergeben.
Abstraktion: Eines der ursprünglichsten und noch immer wichtigsten Ziele der
Verwendung höherer Programmiersprachen ist die Abstraktion über Details der Hardware und des (Betriebs-)Systems. Programme sollen nicht
von solchen Details abhängen, sondern möglichst portabel sein, also auf
ganz unterschiedlichen Systemen laufen können. Praktisch alle derzeit verwendeten Paradigmen erreichen dieses Ziel recht gut, sofern nicht sehr
große Systemnähe gefordert wird. Heute verstehen wir unter „Abstraktion“ nicht mehr nur die Abstraktion über das System, sondern abstrakte
Sichtweisen vieler weiterer Aspekte. Wir können Abstraktionen von fast
allem bilden und in Programmen darstellen. Im Laufe der Entwicklung
von Programmierparadigmen hat der erreichbare Abstraktionsgrad ständig zugenommen und ist heute sehr hoch. In den Abschnitten 1.3 und 1.4
werden wir verschiedene Formen der Abstraktion näher betrachten.
17
1 Grundlagen und Zielsetzungen
Systemnähe: Programme müssen effizient auf realer Hardware ausführbar sein.
Effizienz lässt sich scheinbar am leichtesten erreichen, wenn das Paradigma wesentliche Teile der Hardware und des Betriebssystems direkt widerspiegelt. Unverzichtbar ist Systemnähe dann, wenn Hardwarekomponenten direkt angesprochen werden müssen. Häufig wird ein systemnahes
Paradigma bevorzugt, um die Möglichkeiten eines Systems direkt einbinden zu können. Gelegentlich forcieren Anbieter aus wirtschaftlichen Gründen eine starke Betriebssystemabhängigkeit auf Kosten der Portabilität.
In anderen Bereichen möchten wir dagegen aus Sicherheits- und Portabilitätsgründen vor direkten Zugriffen auf das System geschützt sein. Ein
Paradigma, das überall passt, wird es kaum geben können.
Unterstützung: Ein Paradigma hat keinen Wert, wenn entsprechende Programmiersprachen, Entwicklungswerkzeuge sowie Bibliotheken vorgefertigter Programmteile fehlen. So manches ursprünglich nur mittelmäßige Paradigma (etwa das prozedurale, funktionale und objektorientierte)
hat sich wegen guter Unterstützung trotzdem durchgesetzt und wurde
im Laufe der Zeit verbessert. Ohne Personen, die an den Erfolg glauben
und viel Zeit und Geld in die Entwicklung investieren, kann sich kein Paradigma durchsetzen. Oft sind Kleinigkeiten und Zufälle für den Erfolg
ausschlaggebend. So hat die fast schon tot geglaubte Programmiersprache
Smalltalk zum Jahrtausendwechsel plötzlich eine Renaissance erlebt, weil
keine ganzen Zahlen mit beschränktem Wertebereich unterstützt wurden
und daher keine Zahlenüberläufe möglich waren, die in anderen Sprachen
wegen zu klein gewählter Wertebereiche häufig zu Fehlern und Abstürzen
führten. Manchmal ist die Unterstützung durch einflussreiche Unternehmen essentiell, wie die starke Verbreitung von Java und C# zeigt.
Beharrungsvermögen: Obwohl ständige Innovationen in der Softwareentwicklung üblich sind, werden größere Änderungen der Programmierparadigmen nur sehr langsam angenommen. Jeder Paradigmenwechsel bedeutet,
dass so manches Wissen verloren geht und neue Erfahrungen erst gemacht
werden müssen. Für einen Wechsel braucht es sehr überzeugende Gründe.
Ein solcher Grund wäre, dass jemand den Paradigmenwechsel in einem
vergleichbaren Bereich schon vollzogen hat und damit merklich erfolgreicher ist als im alten Paradigma. Wir sprechen von einer Killerapplikation,
also erfolgreicher Software, die den Erfolg einer Technik oder eines Paradigmas so deutlich aufzeigt, dass das Potential zur Ersetzung althergebrachter Techniken oder Paradigmen in diesem Bereich unübersehbar ist.
Ohne Killerapplikation kommt es zu keinem Paradigmenwechsel.
Wer danach sucht, wird viele weitere praktische Kriterien für den Erfolg von
Paradigmen finden, vor allem solche, die für bestimmte Paradigmen bedeutend
sind. Wir können die zukünftige Bedeutung vieler Kriterien nur schwer abschätzen. So manche Vermutung aus der Vergangenheit hat sich nicht bewahrheitet.
Die oben genannten Kriterien waren jedoch bis jetzt stets von Bedeutung, so18
1.2 Evolution und feine Programmstrukturen
dass wir mit hoher Wahrscheinlichkeit davon ausgehen können, dass sie es auch
in absehbarer Zukunft sein werden.
Die wichtigsten Werkzeuge in der Softwareentwicklung sind Compiler und
Interpreter. Seit den ersten Programmiersprachen hat sich auf diesem Gebiet
viel getan. Heute gibt es neue Implementierungstechniken und ausreichend Erfahrung, um manche Konzepte, die vor Jahrzehnten erfolglos versucht wurden,
nun erfolgreich einzusetzen. Ein Beispiel dafür ist die JIT-Übersetzung (JIT
= Just-In-Time); sie verbindet die Portabilität und einfache Bedienbarkeit von
Interpretern mit der Effizienz von Compilern; auch wenn in diesen Aspekten gewisse Abstriche nötig sind, ergibt sich insgesamt ein guter Kompromiss. Solche
technischen Entwicklungen beeinflussen Programmierparadigmen. Sprachkonzepte, die sich früher nicht durchgesetzt haben, können wegen neuer Implementierungstechniken plötzlich erfolgreich sein.
1.2 Evolution und feine Programmstrukturen
Vom Programmieren im Großen, Groben sprechen wir, wenn es um die Grobstruktur von Programmen, etwa die Modularisierung geht, wobei Programmdetails in der Regel unberücksichtigt bleiben. Entsprechend steht der Begriff der
Programmierung im Kleinen, Feinen für die feinen Programmstrukturen, vor
allem für alles, wo es auf Details ankommt. Keinesfalls geht es nur um kleine
Programme oder unwichtige Aspekte; ganz im Gegenteil, es geht um den Kern
der Programmierung. Zumindest der Umgang mit Variablen und Literalen sowie Ausdrücken, Anweisungen und Kontrollstrukturen bis hin zu Funktionen
(bzw. Methoden und Prozeduren) gehört zur Programmierung im Feinen.
Es wird vorausgesetzt, dass grundlegende Sprachkonzepte bekannt sind. Wir
betrachten, wie teilweise widersprüchliche Ziele hinter elementaren Sprachkonzepten zu unterschiedlichen Schwerpunktsetzungen und Programmierstilen führen. Danach sehen wir uns die strukturierte Programmierung und die Verwendung von Programmteilen als Daten etwas genauer an.
1.2.1 Widersprüche und typische Entwicklungen
Kaum jemand wird widersprechen, dass Programmiersprachen unter anderem
folgende Eigenschaften aufweisen sollten:
• Flexibilität und Ausdruckskraft sollen in kurzen Programmtexten die Darstellung aller vorstellbaren Programmabläufe ermöglichen.
• Lesbarkeit und Sicherheit sollen Absichten hinter Programmteilen sowie
mögliche Inkonsistenzen leicht erkennen lassen.
• Die Konzepte müssen verständlich bleiben und es muss klar sein, was
einfach machbar ist und was nicht geht.
Diese Aussagen widersprechen sich, auch wenn das auf den ersten Blick nicht
auffällt. Wenn wir uns sehr bemühen, können wir je zwei dieser Eigenschaften
19
1 Grundlagen und Zielsetzungen
so miteinander kombinieren, dass beide Eigenschaften recht gut erfüllt sind.
Die technischen Möglichkeiten sind heute so weit fortgeschritten, dass Flexibilität, Ausdruckskraft, Lesbarkeit und Sicherheit gleichzeitig recht gut erfüllt sein
können. Mehrere Jahrzehnte an Entwicklungsarbeit waren dafür nötig. Aber
sobald wir den dritten Punkt, die einfache Verständlichkeit dazu nehmen, wird
es schwierig: Die ersten beiden Punkte konnten nur dadurch unter einen Hut
gebracht werden, dass immer feiner verästelte Fallunterscheidungen getroffen
wurden, die außer wenigen Expert_innen kaum jemand mehr durchschaut.
Die althergebrachte dynamisch typisierte Programmierung bevorzugt den ersten gegenüber dem zweiten Punkt, während die althergebrachte statisch typisierte Programmierung den zweiten gegenüber dem ersten bevorzugt. Viele
jüngere Sprachen erzielen einen hohen Grad an Flexibilität und Ausdruckskraft
gleichzeitig mit Lesbarkeit und Sicherheit. Damit verbundene fortschrittliche
Beweistechniken gehen aber auf Kosten des dritten Punkts. Neue Sprachen
werden häufig mit verbesserter Einfachheit und Verständlichkeit beworben. Bei
Erfolg verschwindet diese Zielsetzung (vor allem, wenn Sicherheit ein Thema ist)
rasch aus dem Blickfeld, weil professionelle Entwickler_innen inhärent komplexe Aufgaben lösen müssen und dafür Unterstützung durch entsprechend komplexe Werkzeuge brauchen. Es ist auch erkennbar, dass statisch und dynamisch
typisierte Programmierung zwar schon seit Beginn der Informatik zusammen
existieren, aber gelegentlich Pendelbewegungen einmal hin zu eher statischer
und dann wieder hin zu eher dynamischer Typisierung stattfinden.
Ob sich eher ein dynamischer oder statischer Ansatz zur Lösung einer Aufgabe eignet, hängt stark von der Art der Aufgabe ab. Kaum jemand kommt
auf die Idee, ein sicherheitskritisches Betriebssystem in JavaScript oder Python
(zwei dynamisch typisierten Sprachen) zu entwickeln. Niemand wird ein nur
für den einmaligen Gebrauch bestimmtes Skript rasch in Rust (einer Sprache
mit komplexen statischen Prüfungen) hinschreiben wollen. Aber es gibt einen
großen Bereich an Anwendungen, für die sowohl statische als auch dynamische
Ansätze vertretbar sind. Dafür wird häufig nur der vertrautere Ansatz gewählt,
nicht notwendigerweise der beste. Die schwankende Anzahl an Personen, die
mit der Entwicklung in bestimmten Ansätzen vertraut sind, ist wahrscheinlich
der wichtigste Grund für die Pendelbewegungen. Technische Weiterentwicklungen können auch eine Ursache sein, aber meist nur in der Form, dass sich viele
Personen von den Entwicklungen ansprechen lassen und sie ausprobieren wollen.
Aus dem eben Gesagten könnte geschlossen werden, dass ein Programmierparadigma oder -stil vor allem eine Modeerscheinung auf einer nur schwer technisch begründbaren Basis darstellt – hinsichtlich vieler einander widersprechender Aspekte, nicht nur wenn es um statisch versus dynamisch typisierte Programmierung geht. Als seriöse Techniker_innen lehnen wir Modeerscheinungen
ab und verlassen uns lieber auf technische Hintergründe. Wahrscheinlich sind es
jedoch gerade die kaum begründbaren Modeerscheinungen, die als Triebfedern
technische Entwicklungen voranbringen. Auch Programmierparadigmen ändern
sich mit der Zeit und werden besser. Wer heute auf die Anfänge z. B. der objektorientierten Programmierung zurückblickt, wird kaum noch Ähnlichkeiten
zur damaligen Denkweise erkennen. Es gibt also einen starken evolutionären
20
1.2 Evolution und feine Programmstrukturen
Fortschritt, obwohl es sich um das gleiche Paradigma handelt. Viele Programmierer_innen haben die Entwicklung vom Beginn bis heute mitgemacht, immer
wieder etwas Neues ausprobiert, vieles davon wieder verworfen, aber einiges ist
hängen geblieben und hat das Paradigma nachhaltig verändert. Modeerscheinungen haben die Blickwinkel dabei immer wieder neu ausgerichtet und dazu
geführt, dass das Paradigma stabiler geworden ist und heute der kritischen
Betrachtung aus sehr vielen verschiedenen Blickwinkeln standhält.
Es stellt sich die Frage, wie sich ein Programmierparadigma verändern kann
und nicht einfach ein neues, anderes Paradigma entsteht. Das hat mit den Menschen zu tun, die ein Paradigma weiterentwickeln. Jeder Mensch wird einen
eigenen Stil innerhalb des Paradigmas finden, der sich von den Stilen der anderen unterscheidet. Jeder Mensch wird diesen Stil auch ständig weiterentwickeln.
Wenn viele Menschen ihre Stile in eine ähnliche Richtung weiterentwickeln, entwickelt sich auch das Paradigma ohne starke Brüche weiter. Menschen arbeiten
nicht isoliert voneinander, sondern passen ihre Stile einander an. Die Notwendigkeit der Zusammenarbeit ist eine sehr starke Kraft, die der Aufspaltung eines
Paradigmas entgegenwirkt und gleichzeitig evolutionäre Änderungen zulässt.
Ohne technisch zwingende Ursache könnte sich eine Aufspaltung in mehrere
Paradigmen nur ergeben, wenn Menschen sich zu isolierten Gruppen zusammenfinden würden, die nicht an gemeinsamen Projekten arbeiten. Das passiert
nur äußerst selten und ist im globalisierten Umfeld bei einem stark verbreiteten
Paradigma, insbesondere im Kern der Programmierung praktisch unmöglich.
Auch in der persönlichen Entwicklung des Programmierstils eines Menschen
sind typische Muster erkennbar. Nehmen wir als Beispiel die Namensgebung.
Zu Beginn steht eine Unsicherheit, weil einerseits das praktische Analogon zur
α-Konversion (Namen sind beliebig austauschbar) verinnerlicht wird, andererseits aber klar ist, dass es bestimmte Erwartungen gibt. Wir suchen nach starren, immer gültigen Regeln für Benennungen. Bald werden Namen lang und
als natürlichsprachige Texte lesbar. Danach entsteht manchmal eine gewisse
„Trotzphase“, in der lange Namen und starre persönliche Regeln vorherrschen,
gleichzeitig aber vorgegebene Regeln gebrochen werden, um eine persönliche
Note hineinzubringen. Schön langsam entsteht ein angepasster Stil, der Unterschiede zwischen den zu benennenden Einheiten macht und dort, wo Suggestion
beim Lesen des Programms nötig ist, auf beschreibende Namen setzt, an anderen Stellen aber kurze, einheitliche Namen bevorzugt. Das setzt ein gutes
Verständnis dafür voraus, was durch die Syntax oder Struktur des Programms
offensichtlich ist und wo zusätzliche Suggestion durch Namen hilft. Gleichzeitig wird verstärkt auf Verwechslungsgefahren aufgepasst, also ähnlich klingende
Namen ebenso gemieden wie solche, wo etwa 0 (Ziffer) mit O (Buchstabe) verwechselt werden könnte. Weit fortgeschrittene Personen achten darauf, dass
Bedeutungen aus der Programmstruktur ablesbar sind. Namen können kürzer
sein, folgen aber einem an das Projekt angepassten Schema. Regeln sind nicht
mehr auf persönlicher Ebene wichtig, sondern projektbezogen. An einem Projekt arbeiten unterschiedlich weit fortgeschrittene Menschen. Programmiersprachen müssen so flexibel sein, dass sie mit allen Formen und Entwicklungsstufen
in der Namensgebung umgehen können.
21
1 Grundlagen und Zielsetzungen
In gewisser Weise spiegeln sich persönliche Entwicklungen von Menschen auch
in Programmiersprachen wider. Nehmen wir die Urform von Fortran. Namen
wurden so gewählt, wie es in der Mathematik üblich ist. Variablen beginnend
mit i bis n enthielten ganze Zahlen, alle anderen Gleitkommazahlen. Texte
mussten (wie bei Lochkarten) in einer bestimmten Spalte beginnen. Schon bald
wurde die Programmstruktur und Namensgebung (vor allem durch die Einführung von Typen) flexibler, gleichzeitig wurden aber auch einige Freiheiten
(wie etwa Leerzeichen in Namen) aufgegeben. Namen wurden länger und beschreibender. Vor allem in Cobol waren Namen, auch syntaktisch vorgegebene,
durchwegs lang und es wurde versucht, Programmtexte als gut lesbare englische Texte darzustellen. Damit sollte bewirkt werden, dass jeder Mensch, auch
ohne Programmiererfahrung, Programme richtig lesen kann. Es stellte sich bald
heraus, dass lange Programmtexte Entwicklungszeiten in die Höhe treiben, ohne Programme tatsächlich lesbarer zu machen. Als Gegenreaktion entstanden
Sprachen wie APL, die sehr kompakte Programmtexte ermöglichten, die rasch
schreibbar, aber kaum lesbar waren. Heute herrschen Sprachen vor, die sowohl kurze als auch lange Namen gut unterstützen, aber hinsichtlich sonstiger
syntaktischer Elemente zurückhaltend sind, etwa Textblöcke in wenige überschaubare Klammern einschließen oder nur über Einrückungen klar vom Rest
absetzen. Die Programmstruktur ist visuell leicht erkennbar, ohne Details lesen
zu müssen. Treibende Faktoren solcher Entwicklungen sind stets neu gewonnene
Erfahrungen vor dem Hintergrund widersprüchlicher Ziele.
1.2.2 Strukturierte Programmierung
Visuell leicht erkennbare Programmstrukturen sind durch strukturierte Programmierung möglich, einer großen Erfolgsgeschichte der Programmierung. Sie
bringt Struktur in die prozedurale Programmierung. Jedes Programm und jeder
Rumpf einer Prozedur ist nur aus drei einfachen Kontrollstrukturen aufgebaut:
• Sequenz (ein Schritt nach dem anderen)
• Auswahl (Verzweigung im Programm, z. B. if und switch)
• Wiederholung (Schleife oder Rekursion)
Heute erscheinen diese Kontrollstrukturen nicht nur in der prozeduralen Programmierung als so selbstverständlich, dass wir uns kaum mehr etwas anderes
vorstellen können. Nur durch die genaue Ausformung ergeben sich noch Unterschiede. Das mächtige und früher allgegenwärtige „Goto“, das einen Gegenpol
zur strukturierten Programmierung bildet, wird heute fast gar nicht mehr verwendet. Die Goto-Anweisung spezifiziert eine beinahe beliebige Stelle in einer
Prozedur oder im Programm, an der die Programmausführung fortgesetzt werden soll. Diese Anweisung bildet Sprungbefehle, auf die Mikroprozessoren auf
Hardware-Ebene fast immer aufbauen, recht direkt in die Programmiersprache
ab. Dennoch gilt Goto wegen hoher Fehleranfälligkeit heute als verpönt.
Zuerst wollen wir klären, was eine Prozedur überhaupt ist. Sie ähnelt einer
Funktion, abgesehen davon, dass die Konzentration auf Seiteneffekten liegt. Bei
22
1.2 Evolution und feine Programmstrukturen
einer Prozedur kommt es hauptsächlich auf Zustandsänderungen an, die in der
Prozedur durch Zuweisung neuer Werte an Variablen erreicht werden, nicht
auf zurückgegebene Ergebnisse. Im Gegensatz zu Funktionen sind Prozeduren
nie frei von Seiteneffekten. Eine Prozedur verändert Werte von Variablen, die
außerhalb der Prozedur definiert sind. Die Goto-Anweisung wird erst durch
nicht-lokale Effekte von Zuweisungen mächtig. Goto ist daher untrennbar mit
prozeduraler Programmierung, der Programmierung mit Prozeduren verknüpft.
Viele Kontrollstrukturen und Programmabläufe lassen sich grafisch darstellen. Dafür gibt es mehrere Ansätze. Beispielsweise ist der Programmfluss in
Form eines Graphen visualisierbar, oder in Form von Kästchen, die wie Klötze eines Baukastens neben- und untereinander angeordnet die Blockstruktur
des Programms widerspiegeln. Visualisierungen können dabei helfen, sich den
Programmablauf vorzustellen. Heute werden Programme nur noch selten tatsächlich grafisch dargestellt, weil wir uns beim Lesen des Programmtexts die
Struktur direkt, ohne Umweg über eine Visualisierung gut vorstellen können.
Strukturierte Programmierung sorgt dafür, dass Visualisierungen der Struktur
sehr einfach werden, so einfach, dass wir sie direkt im Code „erkennen“. In der
strukturierten Programmierung hat jede Kontrollstruktur nur je einen wohldefinierten Einstiegs- und Ausstiegspunkt. Diese Punkte lassen sich in jeder
denkbaren Kombination einfach miteinander verbinden und erleichtern so das
Verfolgen des Programmpfads, wie in folgenden Methoden in Java:
long factRec(int x) { long factLoop(int x) {
long f = x; long f = x;
if (x > 2) { while (--x > 1) {
f *= factRec(x - 1); f *= x;
} }
return f; return f;
} }
Einrückungen und Klammern machen den Kontrollfluss visuell sichtbar, moderne Entwicklungsumgebungen verwenden zusätzlich Farben und Symbole.
Programme mit Goto können, diszipliniert verwendet, zwar auch eine „sichtbare“ Struktur haben, diese spiegelt aber nicht den gesamten Kontrollfluss wider, weil Ein- und Ausstiege an vielen Punkten erfolgen können, was schwierig
zu visualisieren und „erkennen“ ist. Noch schwieriger ist es, alle möglichen Programmpfade gedanklich nachzuvollziehen. Einige Kontrollstrukturen in aktuellen Sprachen erfüllen die Prinzipien der strukturierten Programmierung nicht
vollständig und haben Eigenschaften, die eher mit Goto als der strukturierten
Programmierung assoziiert werden. Betrachten wir Beispiele dazu:
for (int i=1; i<10; i++) { for (int i=1; i<10; i++) {
System.out.println(i); if (i == j) {
if (i == j) { continue;
break; }
} System.out.println(i);
} }
23
1 Grundlagen und Zielsetzungen
Oberflächlich betrachtet scheint der Kontrollfluss hier visuell „sichtbar“ zu sein,
aber die Anweisungen break und continue fügen zusätzliche Ein- und Ausgänge hinzu, die nicht „sichtbar“ sind und möglicherweise in die Irre leiten. Es ist
bekannt, dass Schleifen mit break oder continue häufig fehlerhaft sind. Die
Ursache liegt wahrscheinlich darin begründet, dass die Verletzung der strukturierten Programmierung, obwohl nur lokal auf einen kleinen Programmteil
begrenzt, nicht in das gewohnte Konzept passt und (vor allem routinierte Programmier_innen) mögliche Pfade im Programmablauf übersehen lässt. Das
allgemeine Goto ist fehleranfälliger als die in ihrer Wirkung eingeschränkten
Goto-Varianten break und continue, weil sich viel mehr Programmpfade ergeben können, die leicht übersehen werden. Problematisch an allen Varianten ist
nicht die eigentlich sehr einfache Semantik der Befehle, sondern die Tatsache,
dass Denkmuster, die wir uns durch die strukturierte Programmierung angewöhnt haben, durchbrochen sind. Ähnliche Effekte ergeben sich im Zusammenhang mit Ausnahmebehandlungen. Weniger gravierend sind die Auswirkungen
von irgendwo in Programmtexten vorkommenden return-Anweisungen:
long factRec2(int x) {
if (x <= 2) {
return (long)x; // zusätzlicher Ausgang
}
return x * factRec2(x - 1);
}
Solche return-Anweisungen stellen zwar zusätzliche Ausgänge dar (und verletzen damit Prinzipien der strukturierten Programmierung), aber es entstehen
keine zusätzlichen Eingänge, wodurch die Anzahl der zu betrachtenden Programmpfade überschaubar bleibt.
Damit kommen wir auf den Kern der strukturierten Programmierung: Sequenz, Auswahl und Wiederholung mit je einem Ein- und Ausgang bilden die
essenziellen Denkmuster, über die wir jedes Programm aufbauen können. Mehr
als diese wenigen Denkmuster müssen wir uns auf der Ebene der feinen Programmstrukturen nicht verinnerlichen. Diese Denkmuster sind einfach miteinander kombinierbar und skalieren sehr gut bis auf die höchsten von uns handhabbaren Ebenen der Komplexität. Strukturierte Programmierung gibt uns die
Einfachheit, die große Komplexität erst handhabbar macht.
Einfachheit ist auch eine wesentliche Eigenschaft des λ-Kalküls. Bei der strukturierten Programmierung geht es um Einfachheit auf einer anderen, abstrakteren Ebene. Der λ-Kalkül beinhaltet zwar einfache Konzepte, über die sich alles aufbauen lässt, aber die Umsetzung von Aufgabenstellungen in Programme
kann sehr fordernd sein. Die strukturierte Programmierung gibt uns zusätzlich
als Denkmuster noch klare Hinweise darauf, wie eine Aufgabenstellung in ein
Programm umsetzbar ist. Wir müssen in der Aufgabe nur geeignete Sequenzen, Fallunterscheidungen und Wiederholungen finden, schon ergibt sich der
Kontrollfluss fast von alleine.
Es stellt sich die Frage, warum es noch Anweisungen wie break und continue
gibt, obwohl sie nicht mit strukturierter Programmierung vereinbar sind. Solche
24
1.2 Evolution und feine Programmstrukturen
Anweisungen erhöhen weder die Ausdruckskraft, noch tragen sie (wesentlich)
zur Effizienz bei. Die Antwort liegt vermutlich im Wunsch nach Kontinuität
und Freiheit. Viele Programmierer_innen kennen diese Konzepte von früher
und wollen die Freiheit haben, sie einzusetzen. Weil der Wunsch danach groß
genug ist, kommen Sprachentwickler_innen ihm entgegen. Die Frage nach dem
tieferen Sinn hat hier keinen Platz. Nur mit der reinen Goto-Anweisung waren
die Erfahrungen so eindeutig negativ, dass kaum jemand sie haben will.
Die strukturierte Programmierung löst nicht alle Probleme. Neben dem Kontrollfluss müssen wir auch den Datenfluss im Auge behalten, der von der strukturierten (und auch der prozeduralen) Programmierung gänzlich ignoriert wird.
In Abschnitt 1.5 werden wir uns näher damit beschäftigen.
Wir sind implizit davon ausgegangen, dass die strukturierte Programmierung
eng mit der prozeduralen Programmierung verknüpft ist. Tatsächlich spielt die
strukturierte Programmierung fast überall eine bedeutende Rolle, auch in der
objektorientierten, funktionalen und parallelen Programmierung, wahrscheinlich sogar noch stärker als in der prozeduralen. Neuere Programmierstile tun
sich leichter damit, Überbleibsel aus vergangener Zeit zu entsorgen. Eine absolut reine strukturierte Programmierung ist dennoch kaum zu erreichen, weil
wir auf Konzepte wie Ausnahmebehandlung nicht verzichten möchten, die sich
von Natur aus nicht der strukturierten Programmierung unterordnen lassen.
1.2.3 Programmteile als Daten
Funktionen, Prozeduren, Methoden und ähnliche Konzepte gibt es in fast allen
Paradigmen. Dennoch spielen Funktionen in funktionalen Sprachen eine wichtigere Rolle, die unter anderem dadurch sichtbar wird, dass Funktionen wie Daten verwendet werden. Wir können Funktionen zur Laufzeit erzeugen (solche
Funktionen werden heute häufig als „Lambdas“ oder „Lambda-Ausdrücke“ bezeichnet, obwohl es sich eher um eingeschränkte Formen von λ-Abstraktionen
aus dem λ-Kalkül handelt), in Variablen ablegen, als Argumente an andere
Funktionen übergeben und als Ergebnisse von Funktionen zurückbekommen.
Methoden in Java sind dagegen nicht (oder nur sehr eingeschränkt, jedenfalls
nicht vollwertig) als Daten verwendbar. Wir können sie zwar aufrufen, aber
nicht an Variablen zuweisen oder als Argumente übergeben.1 Objekt-ähnliche
Konzepte gibt es ebenso in vielen Paradigmen, z. B. in Form von Modulen
(siehe Abschnitt 1.3), aber im Wesentlichen nur in der objektorientierten Programmierung sind Objekte wie alle anderen Daten zur Laufzeit erzeug- und
verwendbar. Anscheinend bekommt so manches Sprachkonzept erst durch die
gleiche Behandlung wie alle anderen Daten jene überragende Bedeutung, die
notwendig ist, um ein Paradigma darauf aufzubauen.
In Abschnitt 1.1 haben wir gesehen, dass der λ-Kalkül seine Mächtigkeit dadurch bekommt, dass alle λ-Ausdrücke, auch Funktionen, an Funktionen übergeben und von Funktionen zurückgegeben werden können, wodurch Funktionen
1Das gilt auch in neueren Java-Versionen, in denen z. B. Math::max scheinbar als Methode an
Variablen zugewiesen werden kann. Tatsächlich wird nicht die Methode selbst zugewiesen,
sondern ein Objekt, in dem die Methode definiert ist.
25
1 Grundlagen und Zielsetzungen
als Daten behandelt werden. Primitiv-rekursiven Funktionen fehlt diese Möglichkeit, weshalb sie für sich (ohne zusätzlichen Fixpunkt-Operator µ) nicht so
mächtig sind. Zumindest Funktionen verleiht die Verwendbarkeit als Daten also
tatsächlich formal nachweisbar große Mächtigkeit.
Aus solchen Überlegungen könnte geschlossen werden, dass die Verwendbarkeit von Programmteilen als Daten das Wesentliche in der Programmierung
wäre. Leider sind die Zusammenhänge komplizierter, wie wir am Vergleich
des λ-Kalküls mit primitiv-rekursiven Funktionen erkennen können. Primitivrekursive Funktionen sind das, was der Name suggeriert: Funktionen (oder z. B.
auch Methoden in Java), die sich gegenseitig aufrufen und Zahlen (oder andere elementare Objekte) als Parameter übergeben und Ergebnisse zurückgeben.
Beim Programmieren bevorzugen wir diese Funktionen wegen ihrer Einfachheit.
Der gesamte Kontrollfluss ist nach den Prinzipien der strukturierten Programmierung vollständig visualisierbar. Viele Aufgaben sind anhand der Denkmuster
hinter der strukturierten Programmierung ohne große intellektuelle Anstrengung lösbar. Aber nicht alle Aufgaben sind so lösbar, weil primitiv-rekursive
Funktionen nicht die Mächtigkeit des λ-Kalküls haben. Würden wir Funktionen zu Daten machen, hätten wir zwar das Problem mit der Vollständigkeit
auf einen Schlag beseitigt, aber Aufgaben, die mit primitiv-rekursiven Funktionen alleine nicht lösbar sind, könnten wir nicht durch strukturierte Programmierung lösen. Das liegt daran, dass die Parameterübergabe zum Datenfluss,
nicht zum Kontrollfluss zählt und damit von der strukturierten Programmierung nicht erfasst wird. Anders ausgedrückt: Die Mächtigkeit von Funktionen
als Daten steht im Gegensatz zur Einfachheit in der Verwendung. Glücklicherweise gibt es auch andere Wege, um Vollständigkeit zu erreichen. Wir können
in Übereinstimmung mit der strukturierten Programmierung einen FixpunktOperator hinzufügen, bzw. einfacher ausgedrückt, essentielle Kontrollstrukturen
einbauen. Wenn wir schon dabei sind, können wir ganz in Übereinstimmung
mit der strukturierten Programmierung auch weitere Kontrollstrukturen wie
einen if_then_else-Ausdruck und case-Ausdruck (wie die if- bzw. switchAnweisung in Java, aber als Ausdruck, nicht als Anweisung) einführen, um die
Bequemlichkeit beim Programmieren weiter zu erhöhen.
Das heißt, um die Mächtigkeit der Turing-Maschine zu erreichen, sind keine
Programmteile als Daten nötig; in vielen Bereichen des üblichen Programmieralltags verzichten wir gerne auf sie. In manchen Fällen sind Programmteile
als Daten dennoch sehr wertvoll: Sie ermöglichen es, innerhalb einer Sprache
selbst neue Konstrukte mit der Ausdruckskraft von Kontrollstrukturen einzuführen. Auch wenn wir das nur selten brauchen, ist es vorteilhaft, die Möglichkeit dazu zu haben. Es ist damit nicht nötig, alle Kontrollstrukturen schon im
Sprachdesign vorzusehen, weil if_then_else-Ausdrücke, case-Ausdrücke und
so weiter auch später jederzeit hinzugefügt werden können, abgesehen von speziell an die jeweiligen Kontrollstrukturen angepasster Syntax. Die Entwicklung
solcher Kontrollstrukturen kann aufwändig sein, aber wenn wir sie einmal haben, sind sie einfach verständlich. Es ist nicht schwer, dafür zu sorgen, dass
diese Kontrollstrukturen die Prinzipien der strukturierten Programmierung berücksichtigen. In Abschnitt 2.3.2 werden wir uns mit einem Programmierstil
26
1.2 Evolution und feine Programmstrukturen
beschäftigen, in dem vorwiegend Kontrollstrukturen zum Einsatz kommen, die
auf die Verwendung von Funktionen als Daten aufbauen und entweder vorgefertigt aus umfangreichen Bibliotheken stammen oder selbst geschrieben werden.
Eine Voraussetzung dafür, dass das gut funktioniert, ist ein passendes Umfeld;
das gesamte System muss darauf ausgelegt sein.
Ein weiterer Grund spricht gegen die Einführung vieler als Daten verwendbarer Programmteile: Die uneingeschränkte Verwendbarkeit als Daten führt häufig
zu einer großen Zahl an Sonderfällen, die in der Implementierung einer Programmiersprache berücksichtigt werden müssen. Das lässt den Implementierungsaufwand steigen und die Effizienz der Programme sinken. Außerdem verlagert die
Verwendung von Programmteilen als Daten einige Tätigkeiten, die sonst ein
Compiler statisch erledigen könnte, hin zur Laufzeit, wodurch einige Typprüfungen durch den Compiler nicht mehr machbar sind; darunter leidet auch die
Sicherheit. Es will gut überlegt sein, was als Daten verwendbar sein soll und
was nicht. Einfache Argumentationsketten sind als Entscheidungsgrundlagen
ungeeignet. Nur mit viel Expertenwissen in mehreren Bereichen wie der Theorie der Programmierung, Typtheorie und Compilertechnologie können fundierte
Entscheidungen getroffen werden. Viele Zusammenhänge haben sich als praktische Erfahrungen über mehrere Jahrzehnte langsam herauskristallisiert. Die
Entscheidung dafür, einige Programmteile als Daten verwendbar zu machen,
führt aufgrund inhärenter Zusammenhänge nicht selten automatisch dazu, dass
andere Programmteile nicht sinnvoll als Daten verwendbar sind. Unterschiedliche Entscheidungen führen damit zu unterschiedlichen Programmierstilen. Die
Verwendung von Funktionen als Daten führt zu funktionalen Stilen, die von Objekten als Daten zu objektorientierten. Natürlich wurden zahllose Versuche unternommen, diese beiden Paradigmen zu vereinen. Bis zu einem gewissen Grad
gelingt das ganz gut, sodass in der gleichen Sprache sowohl funktional als auch
objektorientiert programmiert werden kann. Aber eine vollständige Vereinigung
ist wegen inhärenter Widersprüche nicht möglich. Funktionale Programmteile
können nicht gleichzeitig objektorientiert sein. Beispielsweise sind Objekte in
Java ganz selbstverständlich als Daten verwendbar und Lambdas können seit
kurzem als Daten betrachtet werden, die für vollwertige Funktionen stehen.
Es sind jedoch nur dafür vorgesehene Funktionen (solche, hinter denen funktionale Abstraktionen stecken) sinnvoll als Daten verwendbar, nicht beliebige
Methoden. Dass das objektorientierte und funktionale Paradigma nicht zu einem Paradigma zusammenwachsen, hat prinzipielle Ursachen; es geht nicht um
den Willen zur Zusammenarbeit.
Wie Lambdas in Java zeigen, ist es gar nicht notwendig, dass Funktionen als
Daten verwendbar sind. Das Gleiche lässt sich durch Objekte erreichen, die in
Variablen abgelegt, als Parameter übergeben oder als Ergebnisse zurückgegeben werden können: Als Typ eines solchen Objekts wird ein Interface mit nur
einer Methode verwendet. Das Objekt muss diese Methode implementieren und
über das Interface ist die Methode (und sonst nichts) aufrufbar. Wir müssen
zwar um ein Eck herum denken, aber dieser Mechanismus funktioniert in jeder
objektorientierten Sprache. Leider ist dieser Mechanismus nur für Methoden
geeignet, die auf objektorientierte Formen der Abstraktion verzichten.
27
1 Grundlagen und Zielsetzungen
Wir können einen Schritt weiter gehen und uns fragen, was eine Methode
(oder Funktion) von einem Objekt unterscheidet. Eine für viele von uns wahrscheinlich provokant klingende, aber bei entsprechender Interpretation richtige
Antwort ist: Ein Objekt entspricht der Ausführung einer Methode. Einige eher
experimentelle Programmiersprachen, aber auch JavaScript machen das deutlich: Eine Ausführungsinstanz einer Methode, also der Activation-Record2
, der
neben Verwaltungsinformation die Parameter und lokalen Variablen der gerade ausgeführten Methode enthält, kann als Objekt betrachtet werden. Die
lokalen Variablen und Parameter in der Ausführungsinstanz entsprechen Objektvariablen, wenn von einer anderen Methode aus darauf zugegriffen wird (in
Java nicht möglich). Ein Methodenaufruf (ebenso wie ein Konstruktoraufruf)
ist also gleichzeitig eine Objekterzeugung und umgekehrt. Solche Überlegungen
lassen den Unterschied zwischen verschiedenen Arten als Daten verwendbarer
Programmteile noch weiter schwinden.
1.3 Programmorganisation
Bei der Programmierung im Groben geht es um die Zerlegung großer Programme in überschaubare Einheiten. Nicht einzelne Variablen, Typen, Funktionen,
etc. stehen im Blickpunkt, sondern größere Gruppen davon, sowie Beziehungen
zwischen diesen Gruppen. Das wesentliche Ziel ist die Modularisierung von Programmen (auch Faktorisierung genannt). Sie soll so erfolgen, dass größtmögliche
Flexibilität und Wartbarkeit über einen langen Zeitraum erzielt wird. Durch Parametrisierung und Ersetzbarkeit der durch die Modularisierung entstehenden
Einheiten bekommen wir die für die langfristige Wartung nötige Flexibilität.
1.3.1 Modularisierung
Durch Modularisierung bringen wir größere Strukturen in Programme. Wir zerlegen das Programm in einzelne Modularisierungseinheiten, die nur lose voneinander abhängen und daher relativ leicht austauschbar sind. Verschiedene
Formen von Modularisierungseinheiten sind unterscheidbar:
Modul: Darunter verstehen wir eine Übersetzungseinheit, also die Einheit, die
ein Compiler in einem Stück bearbeitet, z. B. in Java ein Interface oder eine
Klasse. Ein Modul enthält vor allem Deklarationen bzw. Definitionen von zusammengehörenden Variablen, Typen, Prozeduren, Funktionen, Methoden und
Ähnlichem. Getrennt voneinander übersetzte Module werden von einem Binder (Linker) oder zur Laufzeit zum ausführbaren Programm verbunden. Wenn
2Alle Daten in einem Programm liegen entweder auf einem Stack (von wo sie am Lebensende
leicht durch „pop“ entfernt werden können) oder am Heap (mit eher langlebigen Daten,
die am Lebensende durch eine vergleichsweise aufwändige Speicherbereinigung entfernt
werden). Die Ausführungsinstanz wird auch Stack-Frame genannt, weil sie meist am Stack
liegt. Wenn jedoch nicht zwischen Objekt und Ausführungsinstanz unterschieden wird,
liegt die Ausführungsinstanz am Heap, sodass Stack-Frame kein passender Name ist.
28
1.3 Programmorganisation
einzelne Module geändert werden, sind nur diese Module (sowie möglicherweise
von ihnen abhängige Module, siehe unten) neu zu übersetzen, nicht alle Module. Diese Vorgehensweise beschleunigt die Übersetzung großer Programme wesentlich. Einzelne Module sind in wenigen Sekunden oder Minuten übersetzt,
während die Übersetzung aller Module Stunden und Tage dauern kann.
Beim Programmieren zeigt sich ein weiterer Vorteil: Module lassen sich relativ
unabhängig voneinander entwickeln, sodass mehrere Leute oder mehrere Teams
gleichzeitig an unterschiedlichen Stellen eines Programms arbeiten können, ohne
sich gegenseitig zu stark zu behindern.
Unter der Schnittstelle eines Moduls verstehen wir zusammengefasste Information über Inhalte des Moduls, die auch in anderen Modulen verwendbar sind.
Nicht nur Java-Interfaces sind Schnittstellen. Klar definierte Schnittstellen sind
überall hilfreich. Einerseits braucht der Compiler Schnittstelleninformation, um
Inhalte anderer Module verwenden zu können, andererseits ist diese Information auch beim Programmieren wichtig, um Abhängigkeiten zwischen Modulen
besser zu verstehen. Meist wird nur ein kleiner Teil des Modulinhalts in anderen
Modulen benötigt. Schnittstellen unterscheiden klar zwischen Modulinhalten,
die für andere Module zugreifbar sind, und solchen, die nur innerhalb des Moduls gebraucht werden. Erstere werden exportiert, letztere sind privat. Private
Modulinhalte sind von Vorteil: Sie können vom Compiler im Rahmen der Programmiersprachsemantik beliebig optimiert, umgeformt oder sogar weggelassen
werden, während für exportierte Inhalte eine Zugriffsmöglichkeit von außen bestehen muss, die gewisse Regeln einhält. Änderungen privater Modulinhalte
wirken sich hinsichtlich der Konsistenz nicht auf andere Module aus. Änderungen exportierter Inhalte machen hingegen oft entsprechende Änderungen in
anderen Modulen nötig, die diese Inhalte verwenden. Zumindest müssen Module, die geänderte Inhalte verwenden, neu übersetzt werden. Um Abhängigkeiten
deutlicher zu machen, wird in Modulen häufig auch angegeben, welche Inhalte
anderer Module verwendet werden. Diese Inhalte werden importiert.
Der explizite Import ermöglicht getrennte Namensräume. Innerhalb eines
Moduls sind nur die Namen der in diesem Modul deklarierten bzw. definierten sowie importierten Inhalte sichtbar und nur diese Namen müssen eindeutig
sein. Der gleiche Name kann in einem anderen Modul (also in einem anderen
Namensraum) eine andere Bedeutung haben. Beim Programmieren sind Namen in anderen Modulen ignorierbar. Allerdings kann es vorkommen, dass aus
unterschiedlichen Modulen unterschiedliche Inhalte des gleichen Namens zu importieren sind. Solche Namenskonflikte sind durch Umbenennung während des
Importierens oder durch Qualifikation des Namens (das ist das Voranstellen des
Modulnamens vor den importierten Namen) auflösbar.
Module im ursprünglichen Sinn können nicht zyklisch voneinander abhängen.
Wenn ein Modul B Inhalte eines Moduls A importiert, kann A keine Inhalte
von B importieren. Das hat mit der getrennten Übersetzung zu tun: Modul A
muss vor B übersetzt werden, damit der Compiler während der Übersetzung
von B bereits auf die übersetzten Inhalte von A zugreifen kann. Würde A
auch Inhalte von B importieren, könnten A und B nur gemeinsam übersetzt
werden, was der Definition und Idee von Modulen widerspricht. Manchmal ist
29
1 Grundlagen und Zielsetzungen
die gemeinsame Übersetzung voneinander zyklisch abhängiger Module dennoch
erlaubt. Wesentliche Vorteile ergeben sich aber nur bei getrennter Übersetzung.
Zyklen in den Abhängigkeiten lassen sich durch Aufspaltung in je zwei getrennte Module auflösen, wobei eines Schnittstelleninformation und das andere
die Implementierung enthält. Beispielsweise sind in Java Interfaces und Klassen,
welche die Interfaces implementieren, getrennte Übersetzungseinheiten. Schnittstelleninformationen hängen in der Regel nicht zyklisch voneinander ab. Dagegen hängen Implementierungen häufig gegenseitig von Schnittstellen anderer
Module ab. Die Trennung ermöglicht getrennte Übersetzungen: Zuerst werden
die Schnittstellen getrennt voneinander übersetzt. Da Implementierungen nicht
direkt auf andere Implementierungen, sondern auf Schnittstellen zugreifen, sind
danach die Implementierungen getrennt voneinander übersetzbar.
Die in Java 9 hinzugefügten sogenannten Java-Module (Dateien, deren Inhalte mit module beginnen) sind selbst keine Module im hier beschriebenen Sinn,
sondern verdeutlichen Schnittstellenspezifikationen von Klassen und Interfaces
aus Modulsicht und klären damit Abhängigkeiten zwischen Modulen.
Objekt. Anders als Module sind Objekte keine Übersetzungseinheiten und
werden erst zur Laufzeit erzeugt. Daher gibt es keine derart starken Einschränkungen hinsichtlich zyklischer Abhängigkeiten wie bei Modulen. Abgesehen davon haben Objekte eine ähnliche Zielsetzung wie Module: Sie kapseln Variablen
und Methoden zu logischen Einheiten (Kapselung) und schützen private Inhalte
vor Zugriffen von außen (Data-Hiding). Wie Module stellen sie Namensräume
dar und sorgen dafür, dass sich Änderungen privater Teile nicht auf andere
Objekte auswirken. Kapselung und Data-Hiding zusammen nennen wir Datenabstraktion. Dieses Wort deutet an, wozu wir Objekte einsetzen: Wir betrachten
ein Objekt als rein abstrakte Einheit, die ein in unserer Vorstellung existierendes
„Etwas“ auf der Ebene der Software realisiert. Zwar ist eine solche Abstraktion
auch mit Modulen erreichbar, aber durch die Vermischung mit dem zusätzlichen
Verwendungszweck als Übersetzungseinheit nur sehr eingeschränkt.
Anders als Module sind Objekte immer als Daten verwendbar. Zu den wichtigsten Eigenschaften von Objekten zählen Identität, Zustand und Verhalten.
Im Prinzip haben auch Module diese Eigenschaften. Jedoch ist die Identität eines Moduls mit dessen eindeutigem Namen und Verhalten gekoppelt, wodurch
unterschiedliche Module immer unterschiedliches Verhalten aufweisen. Objekte haben keinen eindeutigen Namen und es kann mehrere Objekte mit dem
gleichen Verhalten geben. Erst dadurch gewinnen Begriffe wie Identität und
Gleichheit Bedeutung: Zwei durch verschiedene Variablen referenzierte Objekte
sind identisch, wenn es sich um ein und dasselbe Objekt handelt. Zwei Objekte
sind gleich, wenn sie den gleichen Zustand und das gleiche Verhalten haben,
auch wenn sie nicht identisch sind. Dann ist ein Objekt eine Kopie des anderen.
Es gibt eine breite Palette an Möglichkeiten zur Festlegung der Details. So
ist Data-Hiding in jeder objektorientierten Sprache etwas anders realisiert und
neue Objekte werden auf ganz unterschiedliche Weise erzeugt und initialisiert.
Beispielsweise entstehen neue Objekte in der Programmiersprache Self [33] nur
30
1.3 Programmorganisation
durch Kopieren bereits bestehender Objekte und es sind keine Klassen und
ähnliche Konzepte nötig. Meist verwenden wir aber Klassen und Konstruktoren
für die Initialisierung.
Klasse. Eine Klasse wird häufig als Schablone für die Erzeugung neuer Objekte beschrieben. Sie gibt die Variablen und Methoden des neuen Objekts
vor und spezifiziert ihre wichtigsten Eigenschaften, jedoch nicht die Werte der
Variablen. Alle Objekte der gleichen Klasse haben gleiches Verhalten. Objekte
unterschiedlicher Klassen verhalten sich unterschiedlich, sodass nur Objekte der
gleichen Klasse gleich oder identisch sein können. Der Begriff Klasse kommt von
der Klassifizierung anhand des Verhaltens. In diesem Sinn betrachten wir auch
Java-Interfaces als Klassen.
Meist ist es möglich, Klassen von anderen Klassen abzuleiten und dabei Methoden zu erben. Auf eine bestimmte Art angewandt ergeben sich durch Klassenableitungen auf vielfältige Weise strukturierbare Klassifizierungen von Objekten, in denen ein einzelnes Objekt gleichzeitig mehrere Typen haben kann.
Zusammen mit abgeleiteten Klassen sind auch abstrakte Klassen sinnvoll, von
denen zwar andere Klassen ableitbar, aber keine Objekte erzeugbar sind. Interfaces wie in Java sind einfach nur eine Spezialform von abstrakten Klassen.
Zwecks Datenabstraktion sollten Objekte zwischen exportierten und privaten
Inhalten unterscheiden. Da diese Unterscheidung für alle Objekte der gleichen
Klasse gleich ist, wird das meist auf der Klassenebene spezifiziert. Allerdings
sind die Kriterien häufig aufgelockert. Beispielsweise können auch andere Objekte der gleichen Klasse auf private Inhalte zugreifen, und es gibt mehrere
Stufen der Sichtbarkeit. Dadurch, dass neue Objekte nur über Klassen (oder
durch Kopieren) erzeugt werden, sind trotz größerer Flexibilität die Ziele des
Data-Hiding dennoch erreichbar.
Wie in Java ist eine Klasse oft auch ein Modul und damit eine Übersetzungseinheit. Statische Variablen und Methoden sowie Konstruktoren entsprechen
Modulinhalten und zyklische Abhängigkeiten zwischen Klassen sind verboten.
Durch Ableitung von (abstrakten) Klassen oder Interfaces lassen sich zyklische
Abhängigkeiten für nicht-statische Methoden immer auflösen. Statische Methoden sind in Java nicht in getrennten Schnittstellen beschreibbar, sodass diese
Technik dafür nicht nutzbar ist.
Komponente. Eine Komponente ist ein eigenständiges Stück Software, das
in ein Programm eingebunden wird. Für sich alleine ist eine Komponente nicht
lauffähig, da sie die Existenz anderer Komponenten voraussetzt und deren Dienste in Anspruch nimmt.
Komponenten ähneln Modulen: Beides sind Übersetzungseinheiten und Namensräume, die Datenkapselung und Data-Hiding unterstützen. Komponenten
sind flexibler: Während ein Modul Inhalte ganz bestimmter, namentlich genannter anderer Module importiert, importiert eine Komponente Inhalte von
zur Übersetzungszeit nicht genau bekannten anderen Komponenten. Erst beim
Einbinden in ein Programm werden diese anderen Komponenten bekannt. So31
1 Grundlagen und Zielsetzungen
wohl bei Modulen als auch Komponenten ist offen, wo exportierte Inhalte verwendet werden, aber bei Komponenten ist zusätzlich offen, von wo importierte
Inhalte kommen. Letzteres verringert die Abhängigkeit der Komponenten voneinander. Deswegen gibt es bei der getrennten Übersetzung kein Problem mit
zyklischen Abhängigkeiten.
Das Einbinden von Komponenten in Programme nennen wir Deployment. Es
ist aufwändiger als das Einbinden von Modulen, da auch die Komponenten festgelegt werden müssen, von denen etwas importiert wird. Oft werden zuerst die
einzubindenden Komponenten zum Programm hinzugefügt und erst in einem
zweiten Schritt festgelegt, von wo importiert wird. Diese Vorgehensweise ermöglicht die Einführung zyklischer Abhängigkeiten zwischen Komponenten, so
wie zyklische Abhängigkeiten zwischen Objekten erst nach der Objekterzeugung
entstehen können. Das Deployment kann statisch vor der Programmausführung
oder dynamisch zur Laufzeit erfolgen.
Namensraum. Jede oben angesprochene Modularisierungseinheit bildet einen
eigenen Namensraum und kann damit Namenskonflikte abfedern. Das gilt jedoch nicht für globale Namen, die außerhalb dieser Modularisierungseinheiten
stehen, etwa für die Namen von Modulen, Klassen und Komponenten. Wir müssen auch globale Namen verwalten. Häufig werden Modularisierungseinheiten
in andere Modularisierungseinheiten gepackt, z. B. innere Klassen in äußere.
Dies ist zwar für die Datenabstraktion sinnvoll, aber nicht für die Verwaltung
globaler Namen, da das Ineinanderpacken die getrennte Übersetzung behindert.
Manche Sprachen bieten keine Unterstützung für die globale Namensverwaltung. Beim Anwenden von Werkzeugen, z. B. Compilern, sind alle Dateien anzuführen, die benötigte Modularisierungseinheiten enthalten. Dieser vor allem
aus C bekannte Ansatz ist flexibel, aber recht unsicher.
Etwas fortgeschrittener sind Modularisierungseinheiten, die wir Namensräume nennen. Sie fassen mehrere Modularisierungseinheiten zu einer Einheit zusammen, ohne die getrennte Übersetzbarkeit zu stören. Meist entstehen dabei hierarchische Strukturen, die Verzeichnisstrukturen ähneln und manchmal
tatsächlich auf Verzeichnisse abgebildet werden, z. B. Pakete in Java. Namen
werden dadurch komplexer. Beispielsweise bezeichnet a.b.C die Klasse C im
Namensraum b welcher im Namensraum a steht.
Trotz der Verwendung von Namensräumen sind solche Namen immer relativ zu einer Basis und daher nur in einer eingeschränkten Sichtweise global. In
letzter Zeit verwenden wir vermehrt tatsächlich global eindeutige Namen zur
Adressierung von Modularisierungseinheiten, vor allem für öffentlich sichtbare.
Meist werden diese Einheiten wie Webseiten durch ihre URI-Adressen bezeichnet. Solche Adressen wurden ja extra dafür geschaffen, Ressourcen in einem
globalen Umfeld eindeutig zu bezeichnen.
Die Art einer Modularisierungseinheit hängt nicht von der Größe ab. Von
jeder Art gibt es kleine und große Exemplare. Auch bei der Schachtelung von
Modularisierungseinheiten sind viele Varianten denkbar. So können etwa Module in Komponenten enthalten sein, aber auch Komponenten in Modulen.
32
1.3 Programmorganisation
1.3.2 Parametrisierung
Ein bekanntes Schlüsselkonzept zur Steigerung der Flexibilität von Modularisierungseinheiten ist deren Parametrisierung. Darunter verstehen wir im weitesten Sinn, dass in den Modularisierungseinheiten belassene Lücken erst später
befüllt werden. Ein Beispiel dafür haben wir schon betrachtet: Aus einem Modul wird eine Komponente, wenn zunächst offen bleibt, von welchen anderen
Komponenten etwas importiert wird; erst beim Zusammensetzen des Systems
werden diese Komponenten bestimmt. Im Allgemeinen bleiben beliebige Teile
offen, und das Befüllen der Lücken erfolgt zu unterschiedlichen Zeitpunkten aus
verschiedenen Quellen.
Befüllen zur Laufzeit. Am einfachsten ist das Befüllen der Lücken zur Laufzeit, wenn gewöhnliche Daten (elementare Werte oder als Daten verwendbare Programmteile) einzufüllen sind. In diesem Fall werden die Lücken durch
einfache Variablen dargestellt. Beim Befüllen werden ihnen Werte zugewiesen.
Allerdings befinden sich die Variablen üblicherweise nicht an der Stelle im Programm, an der die zuzuweisenden Werte bekannt sind. Die Werte können auf
unterschiedliche Weise zu den Variablen gebracht werden, unter anderem so:
Konstruktor: Beim Erzeugen eines Objekts wird ein Konstruktor ausgeführt,
der die Objektvariablen initialisiert. Der Konstruktor hat Parameter. An
der Stelle, an der die Objekterzeugung veranlasst wird, werden Werte als
Argumente an den Konstruktor übergeben, die zur Initialisierung verwendet werden. Das ist die häufigste und einfachste Form der Parametrisierung, nicht nur in objektorientierten Sprachen.
Initialisierungsmethode: In einigen Fällen sind Konstruktoren nicht verwendbar, beispielsweise wenn Objekte durch Kopieren erzeugt werden oder zwei
zu erzeugende Objekte voneinander abhängen; wir können ja das später
erzeugte Objekt nicht an den Konstruktor des zuerst erzeugten Objekts
übergeben. Solche Probleme sind durch Methoden lösbar, die unabhängig von der Objekterzeugung zur Initialisierung eines bereits bestehenden
Objekts aufgerufen werden. Objekte werden also in einem ersten Schritt
erzeugt und in einem zweiten initialisiert, bevor sie verwendbar sind. Diese
Technik funktioniert nur in imperativen Paradigmen.
Zentrale Ablage: Eine weitere Möglichkeit besteht darin, Werte an zentralen
Stellen (etwa in globalen Variablen oder als Konstanten) abzulegen, von
wo sie bei der Objekterzeugung oder erst bei der Verwendung abgeholt
werden. In letzterer Variante ist diese Technik auch für statische Modularisierungseinheiten verwendbar, die bereits zur Übersetzungszeit feststehen.
Zum Abholen der Werte wird direkt auf die Variablen oder Konstanten
zugegriffen oder es werden Methoden verwendet. Klassen können Konstanten z. B. von einem Interface erben, um Werte „abzuholen“.
Von diesen Techniken sind unzählige Verfeinerungen vorstellbar, die gerade in
der objektorientierten Programmierung häufig ausgereizt werden.
33
1 Grundlagen und Zielsetzungen
Diese Techniken eignen sich zur Dependency-Injection (Einbringen von Abhängigkeiten). Dabei wird die Verantwortung für das Erzeugen und Initialisieren
von Objekten an eine zentrale Stelle (z. B. eine Klasse) übertragen, von der aus
die Abhängigkeiten zwischen den Objekten überblickbar und steuerbar sind.
Generizität. Unter Generizität verstehen wir eine Form der Parametrisierung,
bei der Lücken zumindest konzeptuell bereits zur Übersetzungszeit befüllt werden. Daher können alle Arten von Modularisierungseinheiten außer Objekten
generisch sein (also Lücken enthalten, die mittels Generizität befüllt werden),
aber beispielsweise auch Funktionen und Ähnliches. Die Lücken werden zunächst durch generische Parameter bezeichnet. In allen Lücken, die mit demselben befüllt werden sollen, steht auch derselbe generische Parameter. Später, aber noch vor der Programmausführung werden die generischen Parameter
durch das Einzufüllende ersetzt. Bevorzugt stehen generische Parameter nicht
für gewöhnliche Werte, sondern für Konzepte, die keine Daten sind. Häufig sind
das Typen. In diesem Fall nennen wir generische Parameter auch Typparameter. Generizität ist also vorwiegend für solche Fälle gedacht, wo das Befüllen
der Lücken zur Laufzeit nicht funktioniert.
Grundsätzlich ist Generizität einfach und z. B. in Ada und C++ schon lange
erprobt. In der Praxis ergeben sich aber Schwierigkeiten. So muss der Compiler
mehrere Varianten des Codes verwalten, den generischen Quellcode sowie eine
oder mehrere durch Füllen der Lücken generierte Variante(n). Fehlermeldungen,
die sich auf generierten Code beziehen, sind für Programmierer_innen kaum
verständlich auszudrücken. Oft sind Einschränkungen auf generischen Parametern nötig, weil das dafür Einzufüllende bestimmte Bedingungen erfüllen muss.
Da es sich nicht um Daten handelt, lassen sich derartige Einschränkungen nur
schwer ausdrücken. Besonders schwierig wird es, wenn für mehrere Vorkommen
desselben generischen Parameters unterschiedliche Einschränkungen gelten.
Annotationen. Annotationen sind optionale Parameter, die an unterschiedlichste Sprachkonstrukte anheftbar sind. Ein Beispiel dafür ist die Annotation
überschriebener Methoden in Java mittels @Override. Annotationen werden
von Werkzeugen verwendet oder einfach ignoriert. So gibt ein Compiler, der
@Override versteht, in manchen Situationen Warnungen aus, während andere
Compiler und Werkzeuge, die die Annotation nicht kennen, diese unberücksichtigt lassen. Diese Form der Annotationen wirkt sich statisch, also zur Übersetzungszeit aus. Annotationen sind häufig auch dynamisch, also zur Laufzeit
abfragbar. Über spezielle Funktionen oder Ähnliches lässt sich erfragen, mit
welchen Annotationen ein Sprachkonstrukt versehen ist. Alles funktioniert so,
als ob keine Annotationen vorhanden wären, solange die Annotationen nicht
explizit abgefragt und entsprechende Aktionen gesetzt werden. Annotationen
ähneln also Kommentaren, die aber bei Bedarf auch zur Steuerung des Programmablaufs herangezogen werden können.
Wie Generizität eignen sich Annotationen nur für statisch bekannte Informationen. Die Lücken, die durch Annotationen befüllt werden, sind im Gegensatz
34
1.3 Programmorganisation
zur Generizität nirgends im Programm festgelegt. Daher ist die Art und Weise,
wie die mitgegebenen Informationen zu verwenden sind, ebenso unterschiedlich
wie die Anwendungsgebiete. In der Praxis werden Annotationen oft in Situationen eingesetzt, wo Informationen nicht nur von lokaler Bedeutung sind, sondern
auch System-Werkzeuge (wie einen Compiler oder das Betriebssystem) steuern
oder zumindest beeinflussen. Über Annotationen werden auch häufig Spracherweiterungen eingeführt. In diesem Fall ist das Programm nicht sinnvoll, wenn
Annotationen nicht verstanden werden, aber die Erweiterungen sind mit relativ
kleinem Aufwand durchführbar, ohne vorgegebene Standards zu verletzen.
Aspektorientierte Programmierung. Auch bei der aspektorientierten Programmierung ist in der Regel keine Spezifikation von Lücken im Programm nötig. Stattdessen werden zu einem bestehenden Programm von außen sogenannte
Aspekte hinzugefügt. Ein Aspekt spezifiziert eine Menge von Punkten im Programm, etwa alle Stellen, an denen bestimmte Methoden aufgerufen werden,
sowie das, was an diesen Stellen passieren soll, etwa vor oder nach dem Aufruf
bestimmten zusätzlichen Code ausführen oder den Aufruf durch einen anderen
ersetzen. Ein Aspect-Weaver genanntes Werkzeug angewandt auf das Programm
und die Aspekte modifiziert das Programm entsprechend der Aspekte. Meist
geschieht dies vor der Übersetzung des Programms, manche Aspect-Weaver erledigen diese Aufgabe erst zur Laufzeit. Beispielsweise ist über Aspekte recht
einfach erreichbar, dass bestimmte Aktionen im Programm zuverlässig in einer
Log-Datei protokolliert werden, ohne den Quellcode des Programms dafür ändern zu müssen. Die Aspekte lassen sich vor der Übersetzung des Programms
leicht austauschen oder weglassen, sodass auf einfache Weise ganz unterschiedliches Programmverhalten erreicht wird. Über Aspekte wird etwa die Generierung
bestimmter Debug-Information veranlasst und später wieder weggenommen.
Bestimmte Aufgaben lassen sich durch Aspekte überzeugend rasch und einfach lösen. Für andere Aufgaben sind Aspekte kaum geeignet. Ein Problem
besteht darin, dass die Bestimmung der betroffenen Punkte im Programm Wissen über Implementierungsdetails voraussetzt. Wenn sich solche Details ändern,
müssen auch die Aspekte angepasst werden.
Parametrisierung steigert zwar die Flexibilität von Modularisierungseinheiten, aber ein Problem bleibt bei allen Formen der Parametrisierung bestehen:
Die Änderung einer Modularisierungseinheit macht mit hoher Wahrscheinlichkeit auch Änderungen an allen Stellen nötig, an denen diese Modularisierungseinheit verwendet wird. Konkret: Wenn die Lücken sich ändern, dann muss sich
auch das ändern, was zum Befüllen der Lücken verwendet wird. Solche notwendigen Änderungen behindern die Wartung gewaltig. Vor allem müssen für die
Änderungen alle Stellen bekannt sein, an denen eine Modularisierungseinheit
verwendet wird. Bei Modularisierungseinheiten, die in vielen unterschiedlichen
Programmen über die ganze Welt verstreut zum Einsatz kommen, ist das so
gut wie unmöglich. Nachträgliche Änderungen der Lücken in solchen Modularisierungseinheiten sind dadurch praktisch kaum durchführbar.
35
1 Grundlagen und Zielsetzungen
1.3.3 Ersetzbarkeit
Eine Möglichkeit zur praxistauglichen Änderung von Modularisierungseinheiten
verspricht der Einsatz von Ersetzbarkeit statt oder zusätzlich zur Parametrisierung: Eine Modularisierungseinheit A ist durch eine andere Modularisierungseinheit B ersetzbar, wenn ein Austausch von A durch B keine Änderungen an
Stellen nach sich zieht, an denen A (nach dessen Ersetzung B) verwendet wird.
Leider ist es recht kompliziert, im Detail festzustellen, unter welchen Bedingungen A durch B ersetzbar ist. Diese Bedingungen hängen nicht nur von A
und B selbst ab, sondern auch davon, was, von außen betrachtet, von A und
B erwartet wird. Daher ist Ersetzbarkeit nur für Modularisierungseinheiten anwendbar, die alle erlaubten Betrachtungsweisen von außen klar festlegen. Das
geht Hand in Hand mit klar definierten Schnittstellen. Ersetzbarkeit zwischen
A und B ist dann gegeben, wenn die Schnittstelle von B das Gleiche beschreibt
wie die von A. Jedoch kann die Schnittstelle von B mehr Details festlegen als
die von A, also etwas festlegen, was in A noch offen ist. Entsprechende Schnittstellen sind auf verschiedene Weise spezifizierbar:
Signatur: In der einfachsten Form spezifiziert eine Schnittstelle nur, welche Inhalte der Modularisierungseinheit von außen zugreifbar sind. Diese Inhalte
werden über ihre Namen und gegebenenfalls die Typen von Parametern
und Ergebnissen beschrieben. Die Bedeutung der Inhalte bleibt offen. Wir
nennen eine solche Schnittstelle Signatur der Modularisierungseinheit. In
Kapitel 3 werden wir sehen, dass Ersetzbarkeit für Signaturen einfach und
klar definiert ist und auch von einem Compiler überprüft werden kann.
Im Wesentlichen muss B alles enthalten und von außen zugreifbar machen, was auch in A von außen zugreifbar ist, kann aber mehr enthalten
als A. Wenn wir uns hinsichtlich der Ersetzbarkeit jedoch nur auf Signaturen verlassen, kommt es leicht zu Irrtümern. Ein Inhalt von B könnte
eine ganz andere Bedeutung haben als der gleichnamige Inhalt von A. Es
passiert etwas Unerwartetes, wenn statt des Inhalts von A der entsprechende Inhalt von B verwendet wird. Dennoch verlassen wir uns eher auf
Signaturen, als ganz auf Ersetzbarkeit zu verzichten.
Abstraktion realer Welt: Schnittstellen werden neben Signaturen auch durch
Namen und informelle Texte beschrieben, welche die Modularisierungseinheiten charakterisieren. Diese Schnittstellen entsprechen abstrakten Sichtweisen von Objekten aus der realen Welt. Aufgrund von Alltagserfahrungen können wir recht gut abschätzen, ob eine solche Abstraktion als Ersatz
für eine andere angesehen werden kann. Beispielsweise ist ein Auto genauso wie ein Fahrrad ein Fahrzeug; wir können ein Fahrzeug durch ein Auto
oder Fahrrad ersetzen, aber ein Auto ist kein Fahrrad und das Fahrrad
nicht durch ein Auto ersetzbar. In der objektorientierten Programmierung
spielen solche Abstraktionen eine wichtige Rolle: Ersetzbarkeit haben wir
nur, wenn sowohl die Signaturen als auch die Abstraktionen passen, sodass Irrtümer unwahrscheinlich sind. Jedoch beruht dieser Ansatz auf
Intuition und kann in die Irre führen.
36
1.3 Programmorganisation
Zusicherungen: Um Fehler auszuschließen, ist eine genaue Beschreibung der
erlaubten Erwartungen an eine Modularisierungseinheit nötig. Diese Beschreibung bezieht sich auf die Verwendungsmöglichkeiten aller nach außen sichtbaren Inhalte. In der objektorientierten Programmierung hat sich
für solche Beschreibungen der Begriff Design-by-Contract etabliert. Dabei entspricht die Schnittstelle einem Vertrag zwischen einer Modularisierungseinheit (als Server) und ihren Verwendern (Clients). Der Vertrag legt
in Zusicherungen fest, was sich der Server von den Clients erwarten kann
(das sind Vorbedingungen), was sich die Clients vom Server erwarten können (Nachbedingungen), welche Eigenschaften in konsistenten Programmzuständen immer erfüllt sind (Invarianten), wie sich Zustände ändern und
in welchen Aufruf-Reihenfolgen Clients mit dem Server interagieren können (History-Constraints). Theoretisch sind über diese Arten von Zusicherungen die erlaubten Erwartungen beliebig genau beschreibbar. Es ist
auch klar geregelt, wie sich Zusicherungen aus unterschiedlichen Schnittstellen zueinander verhalten müssen, damit Ersetzbarkeit gegeben ist. In
der Praxis ergeben sich jedoch Probleme. Häufig sind Zusicherungen nur
informell und nicht präzise. Vor allem komplexere Vorbedingungen stehen
nicht selten in Konflikt zu Data-Hiding, weil sie von Programmzuständen
abhängen, die eigentlich nach außen nicht sichtbar werden sollten. Meist
wird die Einhaltung von Zusicherungen, wenn überhaupt, erst zur Laufzeit überprüft; dann ist es dafür eigentlich schon zu spät.
Überprüfbare Protokolle: In jüngerer Zeit wurden Techniken entwickelt, die
formale Beschreibungen erlaubter Erwartungen auf eine Weise ermöglichen, dass bereits der Compiler deren Konsistenz überprüfen kann. Genau genommen spezifizieren solche Schnittstellen Kommunikationsprotokolle zwischen Modularisierungseinheiten. Die Protokolle unterscheiden
sich darin, ob nur die Beziehung zwischen einem Client und Server geregelt wird, oder zwischen mehreren Einheiten gleichzeitig. Natürlich können die Protokolle auch auf ganz unterschiedliche Weise ausgedrückt sein.
Es gibt nicht nur einen sinnvollen Ansatz, sondern viele verschiedene,
die sich in ihren Eigenschaften grundsätzlich voneinander unterscheiden.
Generell dürfen die Protokolle nicht beliebig komplex sein, da die Konsistenz sonst nicht mehr entscheidbar ist. Für praktische Anwendungen
würde die Mächtigkeit jedenfalls ausreichen. Allerdings sind alle solchen
Ansätze noch recht neu und weit von einer praktischen Realisierung in einer etablierten Programmiersprache entfernt. Ob die Erwartungen erfüllt
werden, kann erst die fernere Zukunft zeigen.
In der objektorientierten Programmierung steht die Ersetzbarkeit ganz zentral im Mittelpunkt. Programme werden so gestaltet, dass jeder Programmteil
möglichst problemlos durch einen anderen Programmteil ersetzbar ist. Einerseits ist auf Ersetzbarkeit von Objekten innerhalb eines Programms zu achten.
Programmteile sind vielfältig einsetzbar, wenn sie zwar Objekte einer bestimmten Art erwarten, aber trotzdem auf allen Objekten operieren können, durch die
die erwarteten Objekte ersetzbar sind. Dies ermöglicht einfache Erweiterungen
37
1 Grundlagen und Zielsetzungen
des Programms, ohne dabei schon existierenden Code ständig anpassen zu müssen. Andererseits bietet die Ersetzbarkeit (nicht nur in der objektorientierten
Programmierung) eine Grundlage für die Erzeugung neuer Programmversionen,
die mit Ihrer Umgebung trotz Erweiterungen kompatibel bleiben.
Als Basis für Schnittstellenbeschreibungen verwenden wir in der objektorientierten Programmierung Abstraktionen der realen Welt, in jüngerer Zeit meist
gepaart mit Zusicherungen. Abstraktionen werden häufig über Klassen realisiert. Ersetzbarkeit wird nur dann als gegeben angesehen, wenn es auch eine
Klassenableitung gibt. Klassenableitungen bekommen dadurch eine doppelte
Bedeutung: Sie sind Grundlage sowohl für Vererbung als auch Ersetzbarkeit.
Obwohl Vererbung aufgrund der Definition nichts mit Ersetzbarkeit zu tun
hat, werden diese Begriffe als zusammengehörig betrachtet. Dennoch ist es in
der praktischen Programmierung notwendig, Vererbung klar von Ersetzbarkeit
zu unterscheiden, da andernfalls unvermeidliche Zielkonflikte zu versteckten
schweren Fehlern führen.
In anderen Programmierparadigmen ist Ersetzbarkeit zur Erzeugung neuer
Programmversionen genauso wichtig, aber meist fehlen Sprachmechanismen,
um Ersetzbarkeit sicherzustellen. Im besten Fall gibt es noch eine Unterstützung
zur Überprüfung von Signaturen.
1.4 Abstraktion
In der Programmierung spielen Abstraktionen eine sehr wichtige Rolle. Allerdings wird dieser Begriff für viele Dinge verwendet, die zwar irgendetwas miteinander zu tun haben, sich aber doch voneinander unterscheiden. Wir haben
in Abschnitt 1.1.2 erfahren, dass λ-Abstraktion ein anderer Begriff für Funktion ist, also jede Funktion, Prozedur, Methode und Ähnliches eine Abstraktion
darstellt. In Abschnitt 1.1.3 haben wir Abstraktion als einen Erfolgsfaktor für
Programmierparadigmen beschrieben, wobei es vorwiegend um die Abstraktion
über Details der Hardware und des Betriebssystems geht, mit Portabilität als
Ziel. In Abschnitt 1.3.3 haben wir Abstraktion über die reale Welt als ein Konzept zur Beschreibung von Schnittstellen eingeführt, das intuitive Beziehungen
zwischen unterschiedlichen Einheiten herstellt. Im Folgenden vertiefen wir die
Sichtweise von Abstraktion als Konzept, das intuitive Vorstellungen in die ansonsten unpersönliche, formale Welt der Programmierung bringt. Gut gewählte
Abstraktionen können die Komplexität einer Aufgabe erheblich reduzieren.
1.4.1 Prinzip der Abstraktion
Beim Programmierenlernen wird uns immer wieder das Abstraktionsprinzip3
gepredigt: Duplikate von Programmtexten sind zu vermeiden, stattdessen sind
Funktionen, Prozeduren, Methoden und Ähnliches einzusetzen, welche die an
mehreren Stellen benötigten Programmtexte nur einmal enthalten, diese sind
3Abstraktionsprinzip ist auch ein Begriff aus dem Wirtschaftsrecht, der jedoch nichts mit
dem hier verwendeten Prinzip zu tun hat.
38
1.4 Abstraktion
an den entsprechenden Stellen aufzurufen. Diese Form der Abstraktion hängt
eindeutig mit der λ-Abstraktion zusammen. Natürlich ist es vorteilhaft, wenn
nur ein Programmtext statt vieler Duplikate des gleichen Programmtexts gewartet werden muss. Der Begriff „Abstraktion“ deutet an, dass damit intuitive
Vorstellungen in die Programmierung gebracht werden: Die Funktionen, Prozeduren, Methoden, etc. haben Namen und sollten von Kommentaren begleitet
sein, welche die Absichten dahinter klar machen. Zwar existieren Abstraktionen
auch ohne Namen und Beschreibungen, sind dann aber nicht leicht greifbar.
In gewisser Weise wird mit dem Abstraktionsprinzip das Pferd von hinten
aufgezäumt: Es sollte nicht so sein, dass zuerst Duplikate eingeführt werden,
die danach durch Abstraktion beseitigt werden. Besser wäre es, gleich abstrakt
zu denken, wodurch Duplikate von vorne herein vermieden werden und uns gar
nicht in den Sinn kommt, dass es um die Vermeidung von Duplikaten geht.
Unter Duplikaten sind diesbezüglich auch nicht nur genau gleiche Programmtexte zu verstehen, sondern solche mit gleicher beabsichtigter Wirkung. Es geht
also um die Absicht dahinter, die einem Werkzeug (z. B. dem Compiler) nicht
bekannt ist. Betrachten wir dazu einige Beispiele:
// swap a[i] and a[j]; i != j // swap a[i] and a[j]; i != j
void swap(int[] a,int i,int j){ void swap(int[] a,int i,int j){
int h = a[i]; a[i] ^= a[j];
a[i] = a[j]; a[j] ^= a[i];
a[j] = h; a[i] ^= a[j];
} }
Die beiden swap-Varianten enthalten unterschiedliche Programmtexte, aber die
Absicht dahinter dürfte gleich sein – links die übliche Vertauschung mit einer
Hilfsvariable, rechts eine Vertauschung mit einem bitweise auf ganze Zahlen angewandten XOR. Diese beiden Methoden bewirken nicht das Gleiche, weil die
rechte Variante nur funktioniert, wenn i und j voneinander verschieden sind.
Trotzdem eignen sich die beiden Methoden für den im Kommentar beschriebenen Einsatzzweck, stellen also die gleiche Abstraktion dar. Wir sehen auch, dass
Kommentare eine wesentliche Rolle spielen, wenn sie Absichten ausdrücken, die
von der Semantik einer Programmiersprache nicht erfasst werden.
Umgekehrt kann es auch so sein, dass unterschiedliche Abstraktionen durch
den gleichen Programmtext ausdrückbar sind:
// x smaller y if result < 0
// x equals y if result == 0
// x larger y if result > 0 // difference between x and y
int compare(int x, int y) { int subtract(int x, int y) {
return x - y; return x - y;
} }
Wir wollen compare nicht als äquivalent zu subtract betrachten, da hinter
diesen Methoden unterschiedliche Absichten stecken. Es hat sich bewährt, unterschiedliche Dinge auch bei zufällig gleichem Programmtext als unterschiedlich zu betrachten, weil damit gerechnet werden muss, dass sich Abstraktionen
39
1 Grundlagen und Zielsetzungen
im Laufe der Zeit weiterentwickeln, bei unterschiedlichen Absichten auf unterschiedliche Weise. Vielleicht stellt es sich irgendwann als günstig heraus, Ergebniswerte von compare auf -1, 0 und 1 zu beschränken, was die Absichten
unverändert lässt, aber zu anderem Programmtext als in subtract führt.
Gute Abstraktionen beruhen immer auf bestimmten Absichten und wirken
damit auch auf einer intuitiven Ebene. Eine restriktive Verfolgung des Abstraktionsprinzips zur Vermeidung von Duplikaten ist dabei nicht immer hilfreich.
Stattdessen wäre es sinnvoll, sich eine abstrakte Denkweise anzueignen, also
von vorne herein in Abstraktionen, nicht in konkreten Programmtexten zu denken. Natürlich müssen wir konkrete Programmtexte lesen können, sollten dabei
aber stets auch versuchen, die Abstraktionen zu sehen, die beim Schreiben der
Programmtexte maßgebend waren.
Eine Frage, die sich immer wieder stellt, ist die nach der richtigen Granularität, also Größe der Abstraktionseinheiten (Funktionen, Prozeduren, Methoden,
etc.). Abstraktionen existieren auf allen Granularitätsstufen, vom Einzeiler bis
zu sehr vielen Zeilen. Eine (außer aus einem pragmatischen Gesichtspunkt)
durch nichts belegbare Regel besagt, dass eine Einheit auf einer Bildschirmseite Platz haben soll. Genau genommen ist diese Antwort wenig sinnvoll, weil es
nicht um die Anzahl der Zeilen geht, sondern um die Komplexität der abstrakten
Vorstellung, der Absicht hinter dieser Einheit; es gibt sehr komplexe Einzeiler
genauso wie lange und trotzdem simple Programmteile. Es ist auch nicht sinnvoll zu fragen, ab welcher Komplexität Abstraktionen eingesetzt werden sollen
und bis zu welcher Komplexität die elementaren, durch die Programmiersprache vorgegebenen Operationen reichen, weil jede noch so elementare Operation
schon von vorne herein eine Abstraktion ist. Zumindest abstrahieren elementare
Operationen von der darunter liegenden Hardware und verstecken dabei unnötige Details. Sinnvoller ist eher die Frage danach, ob allgemeine Abstraktionen
auf der Sprachebene reichen, oder ob spezifisch auf den Anwendungsbereich
zugeschnittene Abstraktionen eingeführt werden sollen. Da meist umfangreiche
Bibliotheken zur Verfügung stehen, hängt die Antwort darauf kaum von der
Komplexität ab, sondern davon, wie gut bereits vorhandene allgemeine Abstraktionen das abdecken, was gebraucht wird. Wir versuchen, Abstraktionen
so gut es geht auf einer allgemeinen Ebene zu halten, müssen aber auch den
Mut haben, auf spezifisch zugeschnittene Abstraktionen auszuweichen, wenn
sich Lösungsansätze auf allgemeiner Ebene als zu umständlich erweisen.
Es wird heute als selbstverständlich empfunden, dass Programmiersprachen
von Details der Hardware und des Betriebssystems abstrahieren. In einigen Bereichen stehen jedoch keine passenden Abstraktionen zur Verfügung. Besonders
fällt das dort auf, wo Eigenschaften bestimmter, wenig standardisierter Hardware, etwa einer GPU, eingesetzt werden sollen. Dafür benötigen wir spezielle
Abstraktionen, die sowohl von Hardware-Details als auch Details des Anwendungsbereichs und der eingesetzten Algorithmen abhängen. Wir haben gelernt,
vorgefertigte Abstraktionen einzusetzen und diese miteinander zu kombinieren. Wenn vorgefertigte Abstraktionen fehlen, brauchen wir neue, ungewohnte
Denkansätze und viel Wissen über die Hardware. Sobald wir eine brauchbare Lösung gefunden haben, kommt die nächste Generation an Hardware, die
40
1.4 Abstraktion
wieder neue Lösungsansätze verlangt; trotz aller Mühe ist unsere Lösung nicht
ausreichend portabel.
An solchen Beispielen lässt sich erahnen, wie die heute üblichen allgemeinen
Abstraktionen entstanden sind. Anfangs mussten Programme immer wieder an
sich ständig ändernde Hardware angepasst werden. Über die Jahre änderte sich
zweierlei: Einerseits wurde Hardware einheitlicher und Änderungen aus Sicht
der Programmierung immer weniger gravierend, auch weil Hardware besser an
die Bedürfnisse der Software angepasst wurde. Andererseits wurden durch Herumprobieren Abstraktionen gefunden, die viele Bereiche der Programmierung
recht gut abdecken, aber nicht alle (z. B. GPU-Programmierung). Die Existenz
mehrerer Programmierparadigmen und vieler -stile nebeneinander verdeutlicht,
dass es nicht eine ideale Menge an Abstraktionen gibt, sondern viele unterschiedliche Varianten, die sich evolutionär weiterentwickeln. Die Mengen solcher Abstraktionen sind fragile Gebilde, die nur durch die Notwendigkeit der
Zusammenarbeit vieler Menschen eine ausreichende Stabilität finden.
Heute noch innovative Techniken werden, wenn sie allgemein gebraucht werden, zusammen mit spezieller Hardware sehr wahrscheinlich auch bald einen
Weg in übliche Hardwareausstattungen und Programmbibliotheken finden und
dann nicht mehr als außergewöhnlich gelten. Allerdings werden sich auch immer wieder neue innovative Techniken entwickeln, für die erst mühsam passende
Abstraktionen gefunden werden müssen. Das braucht alles viel Zeit.
„Abstraktion“ ist ein vielschichtiger Begriff. Fassen wir zusammen, was Abstraktion auf der Ebene der Programmierung im Feinen bedeuten kann:
• Eine Funktion, Prozedur oder Methode fasst Programmteile zu einer Einheit zusammen, sodass diese Programmteile nicht mehrfach geschrieben
werden müssen. Der Name der Einheit wird zur Referenzierung der Einheit verwendet und ist eventuell suggestiv, hat darüber hinaus aber keine
Bedeutung. Er ist austauschbar. Jede semantisch wirksame Änderung der
Einheit ändert auch das dahinter stehende abstrakte Verständnis. Es handelt sich um eine λ-Abstraktion im ureigensten Sinn.
• Wenn wir eine Variable verwenden, die eine Funktion, Prozedur oder Methode enthält, bleibt uns der dahinter stehende Programmcode ohne zusätzliche Information gänzlich verborgen. Im abstrakten Verständnis gehen wir davon aus, dass die Variable für beliebigen, austauschbaren Programmtext steht. Nur die Signatur ist uns gegebenenfalls bekannt. Da die
Signatur die Struktur eines möglichen Aufrufs bestimmt, können wir von
struktureller Abstraktion sprechen.
• Wie bei der λ-Abstraktion fasst eine Funktion, Prozedur oder Methode Programmteile zu einer Einheit zusammen, aber deren Name und Signatur wird zusammen mit Kommentaren zur Beschreibung der Einheit
als für Menschen verständliche Spezifikation betrachtet. Jede inhaltliche
Änderung des Namens, der Signatur oder eines Kommentars ändert das
dahinter stehende abstrakte Verständnis, auch wenn die beschriebenen
Programmteile unverändert bleiben. Eine Änderung der Programmteile
41
1 Grundlagen und Zielsetzungen
lässt das abstrakte Verständnis unberührt, solange die geänderte Semantik inhaltlich noch immer das widerspiegelt, was im Namen, in der Signatur und in den Kommentaren ausgedrückt wird. Aufgrund der Bedeutung
des Namens können wir von einer nominalen Abstraktion sprechen. Wir
haben es mit der gleichen Form nominaler Abstraktion zu tun, wenn eine
Variable eine entsprechende Funktion oder Prozedur enthält, da nur Name, Signatur und Beschreibung der Variablen die abstrakte Vorstellung
bestimmt, nicht der beschriebene Programmtext.
• In jeder Sprache ist ein Gefüge von Basisbegriffen nötig, die zusammengenommen von der Hardware und dem Betriebssystem abstrahieren. Hinter
jedem dieser Begriffe steckt eine Basisabstraktion. Auch vorgegebene Kontrollstrukturen und elementare Anweisungen sind Basisabstraktionen. Da
das abstrakte Verständnis auf von der Sprache vorgegebenen Namen und
syntaktischen Elementen beruht, ähnelt jede Basisabstraktion einer nominalen Abstraktion. Allerdings lässt die Sprachdefinition keine Änderungen
von Basisabstraktionen zu – der wesentliche Unterschied zu nominalen
Abstraktionen. Name und abstraktes Verständnis sind untrennbar miteinander verbunden. Wenn wir eine Basisabstraktion in einer Variablen
ablegen, verlieren wir den Zusammenhang zwischen Name und abstraktem Verständnis, wodurch die Variable nur mehr als entweder strukturelle
oder nominale Abstraktion gesehen werden kann.
Wir können zahlreiche weitere Arten von Abstraktionen unterscheiden, etwa
zwischen selbst entwickelten und in einer Bibliothek gefundenen vorgefertigten Abstraktionen. Jede Bedeutung von „Abstraktion“ hat in einem bestimmten Kontext seine Berechtigung, keine ist besser oder richtiger als eine andere.
Wann immer dieser Begriff verwendet wird, müssen wir darauf achten, welche
Bedeutung im Fokus steht. Häufig schwingen mehrere Bedeutungen mit.
1.4.2 Datenabstraktion
Datenabstraktion verlagert die Abstraktion von der Ebene der feinen Programmstrukturen auf die Ebene der Programmierung im Groben. Modularisierungseinheiten werden als Abstraktionseinheiten betrachtet, die häufig Konzepte der realen Welt simulieren und von ihnen abstrahieren. Modularisierungseinheiten bilden auch die Grundbausteine in der Programmorganisation, die
wir zu ganzen Programmen zusammenfügen. Datenabstraktion beruht auf der
Kombination folgender zwei Konzepte (wie in Abschnitt 1.3.1 angerissen):
Datenkapselung: Eine Modularisierungseinheit fasst eine Menge von Funktionen, Prozeduren oder Methoden und eine Menge von Variablen zu einer
untrennbaren Einheit zusammen. Diese Teile hängen stark voneinander
ab: Die Bedeutungen der Variablen sind nur diesen Funktionen, Prozeduren oder Methoden bekannt, wären ohne sie also sinnlos. Die Funktionen,
Prozeduren oder Methoden verwenden die Daten in den Variablen gemeinsam, würden ohne die Variablen also nicht funktionieren und müssen
42
1.4 Abstraktion
bei der Verwendung der Variablen koordiniert vorgehen. Die abstrakte
Sichtweise kommt aus dem abstrakten Verständnis jedes einzelnen Teils
sowie dem oft von der realen Welt motivierten Zusammenspiel der Teile.
Data-Hiding: Hier geht es um die Trennung der Innenansicht von der Außenansicht einer Modularisierungseinheit. Innerhalb der Modularisierungseinheit sind alle Teile einander bekannt und unbeschränkt verwendbar.
Von außerhalb sind nur die von der Modularisierungseinheit exportierten Teile sichtbar, wobei Variablen im Normalfall privat (also von außen
nicht sichtbar) bleiben, weil ihre Bedeutungen ja nur innerhalb bekannt
sind. Die Betrachtung von außen steht damit notwendigerweise auf einer
höheren Abstraktionsstufe als die Betrachtung von innen.
Die nach außen sichtbaren Inhalte bestimmen die Verwendbarkeit der Modularisierungseinheit. Private Inhalte bleiben bei der Verwendung unbekannt und
die gesamte Modularisierungseinheit daher auf gewisse Weise abstrakt. Wir
sprechen von einer „undurchsichtigen Schachtel“ (black box) oder häufiger „semitransparenten Schachtel“ (grey box), weil manches von außen sichtbar ist.
Natürlich stellen auch die Funktionen, Prozeduren und Methoden Abstraktionen dar, meist im Sinn von nominalen Abstraktionen (in allen Paradigmen, vor
allem dem objektorientierten), manchmal als λ-Abstraktionen (in der funktionalen Programmierung wenn Funktionen als Spezifikationen gesehen werden –
denotationale Semantik), selten als strukturelle Abstraktionen (etwa abstrakte
Methoden in „funktionalen Interfaces“ in Java). Implizit steht hinter jeder Modularisierungseinheit ein nicht formal festgelegtes, also abstraktes Konzept, das
im Idealfall (vor allem in der objektorientierten Programmierung, aber nicht
nur dort) eine Analogie in der realen Welt hat, um besser verständlich zu sein.
Private und nach außen sichtbare Inhalte zusammen müssen diesem Konzept
entsprechen. Solange das Konzept erhalten bleibt, sind private Inhalte problemlos änderbar, ohne dabei die Verwendbarkeit der Modularisierungseinheit zu beeinträchtigen. Üblicherweise sind auch Modularisierungseinheiten selbst, nicht
nur ihre Inhalte, durch Kommentare beschrieben, um die Absichten dahinter
(also die Abstraktionen) klar darzulegen.
Ein abstrakter Datentyp ist im Wesentlichen eine Schnittstelle einer Modularisierungseinheit4 und entspricht damit genau der abstrakten Sichtweise. Wie in
Abschnitt 1.3.3 beschrieben, können wir die Signatur der Modularisierungseinheit als Schnittstelle betrachten. In diesem Fall sprechen wir von einem strukturellen Typ, weil er nur von den Namen, Parametertypen und Ergebnistypen
4
In einer ursprünglichen Definition entspricht ein abstrakter Datentyp einer freien Algebra
(siehe Abschnitt 1.1) und besteht damit neben der Signatur auch aus einer Menge an
algebraischen Gesetzen (Axiomen), die Elemente der Signatur miteinander in Beziehung
setzen. Diese ursprüngliche Definition wird in reinen Formen der funktionalen Programmierung verwendet, wo man auch von algebraischen Datentypen spricht. Das passt, weil
Modularisierungseinheiten in der rein funktionalen Programmierung keine destruktiv änderbaren Variablen enthalten. Zur Beschreibung von durch Zuweisungen änderbaren Programmzuständen sind Axiome weniger gut geeignet. Aus pragmatischen Gründen wurden
die Axiome daher einfach weggelassen (für strukturelle Typen) oder durch abstraktere
Formen zur Festlegung der Beziehungen ersetzt (für nominale Typen).
43
1 Grundlagen und Zielsetzungen
der nach außen sichtbaren Inhalte abhängt – quasi von der nach außen sichtbaren Struktur. Alle Funktionen, Prozeduren und Methoden in einem strukturellen Typ werden als strukturelle Abstraktionen gesehen. Wenn unterschiedliche
Modularisierungseinheiten die gleiche Signatur haben, dann haben sie auch den
gleichen strukturellen Typ. Genau diese Eigenschaft lässt strukturelle Typen als
abstrakte Datentypen in der Praxis wenig sinnvoll erscheinen, weil dies bedeuten
würde, dass hinter allen Modularisierungseinheiten mit gleicher Signatur auch
die gleiche Abstraktion steckt. Das trifft im Allgemeinen nicht zu. In einigen
speziellen Einsatzgebieten sind aber genau solche Abstraktionen sinnvoll; eine
Abstraktion entspricht dabei genau der Signatur ohne Annahme zusätzlicher Eigenschaften, es zählt nur das Vorhandensein öffentlich sichtbarer Funktionen,
Prozeduren und Methoden, ohne Rücksicht darauf, was sie machen.
Meist gehen wir davon aus, dass neben Signaturen abstrakte Vorstellungen zu
den Schnittstellen gehören. Das ergibt die übliche Bedeutung eines abstrakten
Datentyps. Für die praktische Verwendung ist es notwendig, dass ein solcher abstrakter Datentyp mit einem eindeutigen Namen bezeichnet wird (etwa Stack
oder Queue), es handelt sich also um einen nominalen Typ. Zwei abstrakte Datentypen mit unterschiedlichen Namen werden als verschieden betrachtet, auch
wenn sie die gleiche Signatur haben. Erst dadurch können wir diese abstrakten Datentypen mit voneinander verschiedenen Abstraktionen in Verbindung
bringen. Der Name steht stellvertretend für alle Absichten und Vorstellungen,
die damit verbunden sind, egal ob über Kommentare beschrieben oder aus der
Bedeutung von Namen und einer Analogie zur realen Welt intuitiv erfasst. Auf
solche Techniken zur Spezifikation aufbauend sind Funktionen, Prozeduren und
Methoden in einem nominalen Typ meist nominale Abstraktionen.
In rein funktionalen Sprachen werden bestimmte abstrakte Datentypen als
algebraische Datentypen spezifiziert, wobei alle Beziehungen zwischen Daten
und Funktionen klar festgelegt sind. In diesem Fall werden eindeutige Namen
verwendet, wodurch es sich um nominale Typen handelt, aber die Programmstruktur legt wesentlich mehr als nur Signaturen fest. Zusätzliche Beschreibungen sowie Bedeutungen von Namen und Analogien zur realen Welt sind weniger
wichtig. Funktionen werden manchmal als λ-Abstraktionen gesehen. Das heißt
nicht, dass es in funktionalen Sprachen keine strukturellen Typen gibt. Etwa in
ML spielen „Structure“ genannte strukturelle Typen eine bedeutende Rolle.
Abstrakte Datentypen können beliebig genau und auf unterschiedlichste Arten beschrieben sein. In vielen Fällen verwenden wir Zusicherungen entsprechend Design-by-Contract zur Beschreibung, siehe Abschnitt 1.3.3. Das ändert
nichts daran, dass wir es mit nominalen Typen zu tun haben. Nominale Typen
implizieren, dass es nur genau eine Stelle geben kann, an der ein Typ eingeführt
wird; an dieser Stelle wird der Name mit der Modularisierungseinheit in Verbindung gebracht. Das ist auch die Stelle, an der die Zusicherungen zu finden
sind. Strukturelle Typen mit Zusicherungen ergeben auch deswegen wenig Sinn,
weil einander entsprechende Typen an mehreren Stellen stehen würden und die
Zuordnung von Zusicherungen zu Typen im Allgemeinen mehrdeutig wäre.
Theoretisch lässt sich auch mit strukturellen Typen so arbeiten, als ob es sich
um nominale Typen handeln würde: Jede Modularisierungseinheit bekommt zu44
1.4 Abstraktion
sätzliche, eigentlich nicht verwendete Inhalte, deren Namen als Teil der Signatur das abstrakte Konzept dahinter beschreiben (etwa eine Methode namens
iBelongToStack oder iBelongToQueue). Damit werden gleiche Signaturen für
unterschiedliche Konzepte ausgeschlossen. Wegen dieser theoretischen Möglichkeit betrachten wir in der Theorie fast nur strukturelle Typen, arbeiten in der
Praxis aber dennoch fast ausschließlich mit nominalen Typen.
Heute ist es in allen etablierten Programmierparadigmen üblich, vorgefertigte
Funktionen, Prozeduren und Modularisierungseinheiten nur über abstrakte Datentypen bereitzustellen, die man häufig Bibliotheken nennt. Die Zuordnung der
Inhalte zu Bibliotheken sorgt nicht nur für eine schönere Gliederung, sondern
verbessert auch die Verständlichkeit der Abstraktionen. Alleine schon die Zugehörigkeit zu einer bestimmten Bibliothek verrät einiges über dahinter stehende
Absichten. Ein der Bibliothek innewohnender logischer Aufbau und dessen Beschreibung trägt noch wesentlich mehr zum Verständnis bei. Auch elementare
Typen wie int können als abstrakte Datentypen betrachtet werden. Als Basisabstraktionen abstrahieren diese Typen etwa von der Repräsentation in der
Hardware. Operationen darauf, etwa die ganzzahlige Addition, werden als mit
dem Typ (int) in Zusammenhang stehend angesehen, so als ob die Addition in
der Bibliothek int auffindbar wäre.
1.4.3 Abstraktionshierarchien
Gerade in der objektorientierten Programmierung, aber nicht nur dort, spielen
ganze Hierarchien an Abstraktionen eine große Rolle. Ein und dieselbe Modularisierungseinheit kann gleichzeitig auf mehreren Abstraktionsebenen betrachtet
werden. Das heißt, mehrere voneinander verschiedene abstrakte Datentypen, die
Abstraktionen des gleichen Konzepts mit unterschiedlichem Detailiertheitsgrad
darstellen, sind gleichzeitig unterschiedliche Schnittstellen der gleichen Modularisierungseinheit. Ein abstrakter Datentyp kann auch gleichzeitig Schnittstelle
mehrerer unterschiedlicher Modularisierungseinheiten sein. Daraus können sich
recht komplexe Beziehungsstrukturen ergeben. Diese Strukturen lassen sich zum
Teil formal in Programmen abbilden.
Abstraktionen sind häufig vage, nicht genau beschriebene oder beschreibbare Vorstellungen von irgendwelchen Konzepten. Weil Menschen gut darin sind,
mit solchen vagen Vorstellungen zu arbeiten, ist das prinzipiell als Vorteil zu
sehen; vage Vorstellungen können mit der Zeit konkreter werden und sich in ihrer Bedeutung wandeln, ohne die Vorstellungswelt zusammenbrechen zu lassen.
Allerdings führen zu vage Vorstellungsgebilde dazu, dass Abstraktionshierarchien nicht eindeutig sind, unterschiedlich verstanden werden oder im Laufe der
Zeit schwinden. Zumindest sind sie einer formalen Analyse kaum zugänglich
und daher in Programmen kaum abbildbar. Der einzige Ausweg besteht darin,
Beziehungen zwischen Abstraktionen klarer zu definieren, aus vagen Vorstellungen konkretere zu machen. Dabei passiert jedoch etwas Entscheidendes: Es
reicht nicht mehr, einfach nur mit Vorstellungen zu arbeiten, sondern wir brauchen präzise Definitionen, die zwar so unbestimmt wie möglich bleiben, aber
in jenen Bereichen, die die Struktur betreffen, keinen Interpretationsspielraum
45
1 Grundlagen und Zielsetzungen
lassen. Einfache Beziehungen zwischen Vorstellungen werden zu komplizierten
Gebilden. Mehr noch: Es gibt nicht nur eine sinnvolle Art, Präzision in die Vorstellungswelt zu bringen, sondern zahlreiche, die sich teilweise widersprechen
und zu unterschiedlichen Strukturen führen. Einige davon werden in der Programmierung tatsächlich eingesetzt, manchmal sogar gleichzeitig. Wir unterscheiden zumindest folgende Arten von Beziehungen zwischen Abstraktionen:
Beziehungen in der realen Welt: Dabei bleiben die Vorstellungen vage, werden aber durch Begriffe aus der realen Welt möglichst klar umrissen. Beziehungen zwischen Abstraktionen ergeben sich einfach daraus, ob die Begriffe auch in der realen Welt auf natürliche Weise in einer entsprechenden
Beziehung stehen. Vor allem die „is-a“-Beziehung steht im Mittelpunkt.
Da z. B. in der realen Welt ein Auto genau so wie ein Fahrrad ein Fahrzeug ist (is-a), wird ein abstrakter Datentyp zur Darstellung eines Autos
als eine speziellere, kompatible Variante eines abstrakten Datentyps zur
Darstellung eines Fahrzeugs betrachtet. Entsprechend ist auch ein Fahrrad eine Spezialisierung von Fahrzeug, aber Auto und Fahrrad stehen
in keiner solchen Beziehung zueinander. Beziehungen aus der realen Welt
werden vor allem in der objektorientierten Modellierung zum Entwurf von
Systemen eingesetzt, in der Programmierung selbst aber nur zusammen
mit zusätzlichen Einschränkungen.
Untertypbeziehungen: Das sind die in der objektorientierten Programmierung
essenziellen Beziehungen, die Ersetzbarkeit ausdrücken. B ist Untertyp
von A, wenn jede Instanz von B verwendet werden kann, wo eine Instanz von A erwartet wird, wobei A und B abstrakte Datentypen sind.
Häufig werden während der Programmentwicklung Beziehungen in der
realen Welt zu Untertypbeziehungen weiterentwickelt. Es kann durchaus
sein, dass ein Auto und ein Fahrrad Untertypen eines Fahrzeugs sind, es
hängt aber von vielen Details ab, ob das tatsächlich so ist. Neben den
Beziehungen in der realen Welt (nötig um den weiterhin vage bleibenden Vorstellungen zu entsprechen) gelten viele weitere Einschränkungen
auf den Inhalten der Modularisierungseinheiten, um Ersetzbarkeit zu ermöglichen. Eine der Einschränkungen bezieht sich z. B. darauf, dass ein
Parameter einer Methode im Untertyp B nicht spezieller sein darf als der
entsprechende Parameter im Obertyp A, um Typsicherheit zu gewährleisten; daraus folgt, dass keine binären Funktionen, also Funktionen, die
jeweils mindestens zwei Argumente des gleichen Typs nehmen (etwa um
Werte des gleichen Typs miteinander zu vergleichen), über mehrere Ebenen hinweg typsicher darstellbar sind. Wie in Abschnitt 1.3.3 argumentiert, ist Ersetzbarkeit in der Programmierung extrem wichtig, aber nicht
ohne bedeutsame Kompromisse erreichbar.
Untertypbeziehungen höherer Ordnung: Dieser Begriff ist zwar aus einer formalen Definition verständlich, aber irreführend, weil entsprechende Beziehungen keine Ersetzbarkeit garantieren und daher keine Untertypbeziehungen sind. Der einzige Unterschied zu Untertypbeziehungen besteht
46
1.5 Daten und Datenfluss
darin, dass binäre Funktionen unterstützt werden. Solche Beziehungen
sind vor allem im Zusammenhang mit einigen Varianten der Generizität
von Bedeutung, weil damit Einschränkungen der Werte, die generische Parameter ersetzen, spezifiziert werden können, ohne auf binäre Funktionen
verzichten zu müssen. Sie kommen in einigen objektorientierten Sprachen
(C++) ebenso zum Einsatz wie in funktionalen Sprachen (Haskell).
Vererbungsbeziehungen: Vererbung ist ein in der objektorientierten Programmierung historisch gewachsener Begriff. Es geht darum, Programmtexte
aus einer Oberklasse direkt in eine Unterklasse zu übernehmen, was so
erfolgen soll, dass die Unterklasse eine Spezialisierung der Oberklasse ist
und Beziehungen aus der realen Welt übernimmt. Damit wird, im Gegensatz zu ursprünglichen Erwartungen, leider keine Ersetzbarkeit garantiert.
Obwohl in der objektorientierten Programmierung nach wie vor viel von
Vererbung gesprochen wird, ist es nicht mehr zeitgemäß, die Programmstruktur auf Vererbung aufzubauen.
Simulation: Der Begriff Simulation ist überladen. Einerseits verstehen wir darunter das Erstellen eines virtuellen Abbilds eines Konzepts aus der realen
Welt in Software, wobei das virtuelle Abbild natürlich nicht die volle Komplexität der realen Welt übernehmen kann, sondern darüber abstrahieren
muss. Andererseits wird damit auch eine Beziehung zwischen formalen
Systemen, etwa Sprachen bezeichnet, die die relative Ausdrucksstärke vergleicht. Ein System simuliert ein anderes System, wenn das eine System
jedes Konstrukt des anderen Systems abbilden kann; das eine System ist
in einem gewissen Sinn so mächtig wie das andere. Wenn zwei formale
Systeme sich gegenseitig simulieren, sprechen wir von Bisimulation.
1.5 Daten und Datenfluss
Während es mit der strukturierten Programmierung eine breite Akzeptanz für
einen relativ einheitlichen Umgang mit dem Kontrollfluss in einem Programm
gibt, gehen unterschiedliche Programmierparadigmen mit dem Datenfluss unterschiedlich um. Wir untersuchen zunächst, wie Daten fließen und wie der
Datenfluss mit dem Kontrollfluss interagiert. Abhängigkeiten zwischen Daten
können als ein mächtiges Instrument genutzt werden, erweisen sich aber auch als
gefährlich, wenn sie Programme sehr rasch undurchschaubar kompliziert werden
lassen. Wir betrachten mehrere Vorgehensweisen, um damit zurechtzukommen.
1.5.1 Kommunikation über Variablen
Variablen in einem Programm lassen sich in folgende Kategorien einteilen:
Lokale Variablen: Sie werden in einer Funktion, Prozedur oder Methode deklariert und sind nur dort sichtbar und zugreifbar. Bei jedem Aufruf wird
neuer Speicherplatz für die lokalen Variablen angelegt, bei der Rückkehr
47
1 Grundlagen und Zielsetzungen
wird der Speicherplatz wieder freigegeben. Jede Ausführung verwendet
daher unterschiedliche Instanzen lokaler Variablen.
Parameter: Sie werden innerhalb einer Funktion, Prozedur oder Methode (im
Kopf) deklariert und wie lokale Variablen verwendet. Im Unterschied zu
lokalen Variablen stellen Sie eine Verbindung zum Aufrufer her, der beim
Aufruf Argumente (auch aktuelle Parameter genannt) übergibt; Parameter (zur klaren Unterscheidung von aktuellen Parametern auch formale
Parameter genannt) sind interne Bezeichner für entsprechende Argumente. Java unterstützt nur Eingangsparameter, aber im Allgemeinen werden
folgende Parameterarten unterschieden:
Eingangsparameter: Daten fließen auf oberster Ebene nur vom Aufrufer
zum Aufgerufenen. Als Argumente werden beliebige Werte übergeben. Falls neue Werte an Parameter zugewiesen werden, gehen diese
bei der Rückkehr verloren.
Durchgangsparameter: Daten fließen auch auf oberster Ebene in beide
Richtungen. Als Argumente werden initialisierte Variablen übergeben, deren Inhalte über Parameter sichtbar sind. Falls neue Werte
an Parameter zugewiesen werden, ändern sich auch die Werte in den
Argumenten. Bei einer Realisierung, in der Parameter Referenzen auf
Argumente enthalten, sprechen wir von Referenzparametern. Durchgangsparameter können aber auch so realisiert sein, dass Argumente
beim Aufruf und bei der Rückkehr kopiert werden.
Ausgangsparameter: Daten fließen nur vom Aufgerufenen zum Aufrufer.
Als Argumente werden Variablen übergeben. An die entsprechenden
Parameter müssen Werte zugewiesen werden, die nach der Rückkehr
in den Argumenten stehen.
Variablen in Modularisierungseinheiten: Das sind Variablen, die zu einer Modularisierungseinheit gehören, aber keine lokalen Variablen oder Parameter sind. Je nach Art der Modularisierungseinheit sprechen wir von
Objektvariablen (gehören zu Objekten, auch wenn sie in einer Klasse deklariert sind), Klassenvariablen und so weiter. Speicherplatz für solche
Variablen wird bei statischen Modularisierungseinheiten statisch (vom
Compiler) reserviert, bei Objekten zum Zeitpunkt der Objekterzeugung.
Der Speicher bleibt logisch gesehen bis zum Programmende reserviert,
kann aber, sobald keine Zugriffe mehr möglich sind, schon früher durch
Speicherbereinigung freigegeben werden.
Globale Variablen: Im engeren Sinn sind das Variablen auf globaler Ebene,
also solche, die außerhalb von Modularisierungseinheiten deklariert sind
und statisch (vom Compiler) verwaltet werden. In einem weiteren Sinn
werden auch Variablen, die zu statischen Modularisierungseinheiten gehören, als global bezeichnet, aber nicht Objektvariablen.
Variablen, genauer die über Variablennamen angesprochenen Speicherbereiche, enthalten ihre Werte entweder direkt, oder Zeiger (bzw. Referenzen) auf
48
1.5 Daten und Datenfluss
Speicherbereiche, in denen die Werte (oder wieder nur Zeiger) stehen. Wenn wir
einen Zeiger dereferenzieren, erhalten wir das, worauf gezeigt wird. Das Dereferenzieren erfolgt explizit durch Anwendung eines dafür vorgesehenen Operators
oder implizit, also automatisch beim Zugriff auf eine Variable, die eine Referenz
enthält. Bei automatischer Dereferenzierung sprechen wir eher von Referenzen,
bei expliziter eher von Zeigern, aber ganz einheitlich ist die Terminologie nicht.
Häufig enthalten zwei verschiedene Variablen Zeiger auf dieselben Daten.
Die beiden Variablen sind dann Aliase voneinander, also im Wesentlichen unterschiedliche Namen für eine Sache; die Inhalte der beiden Variablen sind identisch. In einer Programmiersprache mit Zeigern (schließt Referenzen ein), also
eigentlich in jeder Sprache, entstehen Aliase unvermeidlich, beispielsweise bei
der Parameterübergabe: Wird eine Variable mit einem Zeiger übergeben, sind
das Argument und der Parameter Aliase voneinander. Das spart Kopieraufwand, weil statt einer großen Datenmenge nur ein kleiner Zeiger übergeben wird.
Nebenbei ergeben sich zusätzliche semantische Möglichkeiten, indem Veränderungen der durch den Parameter referenzierten Daten auch bei Eingangsparametern über das Argument sichtbar werden. Bei der Parameterübergabe fließen
Daten auf übersichtliche Weise entlang Linien, die durch den Kontrollfluss vorgegeben sind. Aber auch ganz unterschiedliche Arten von Variablen in ganz
unterschiedlichen Programmteilen können Aliase voneinander sein. Die Änderung der referenzierten Daten über eine der beiden Variablen wird sofort über
die andere Variable sichtbar.
Über Variableninhalte können Programmteile miteinander kommunizieren:
void sort(int[] a) {
boolean done;
do { done = true;
for (int i = 1; i < a.length; i++)
if (a[i - 1] > a[i]) {
swap(a, i - 1, i);
done = false;
}
} while (!done);
}
In sort werden über die Variable done Informationen über erfolgte Vertauschungen gesammelt und am Ende der do-Schleife einmal überprüft. Das heißt,
done dient als Kommunikationskanal zwischen Ausführungen der if-Anweisung
und der Abbruchbedingung, wobei done eine lokale Variable ist. Im folgenden
Beispiel geschieht Ähnliches, aber gut versteckt und nicht lokal:
boolean print(int[] a) {
for (int i : a) System.out.println(i);
return !System.out.checkError();
}
In System.out gibt es eine private Variable, die Informationen darüber enthält,
ob Ein-/Ausgabefehler aufgetreten sind; checkError() prüft auf Fehler. Hier
49
1 Grundlagen und Zielsetzungen
wird Information unabhängig von der Aufrufstruktur über viele Aufrufe hinweg
weitergegeben und nur einmal am Ende abgefragt. In diesen Beispielen sind für
die Kommunikation über Variablen keine Aliase nötig.
Aliase erweitern die Möglichkeiten zur Kommunikation zwischen Programmteilen nicht nur, sie können sie auch sehr gut verschleiern. In obigem printBeispiel ist noch erkennbar, dass die Kommunikation über System.out als zentrale Stelle läuft. In folgender Abwandlung ist das nicht mehr erkennbar:
boolean print(int[] a, PrintStream p, PrintStream q) {
for (int i : a) p.println(i);
return !q.checkError();
}
Das ist so, obwohl ein Aufruf print(a, System.out, System.out) nichts an
der Ausführung ändert. Es ist sehr schwer und aufwändig, schon vor der Laufzeit
zuverlässige Informationen über Alias-Beziehungen zu erhalten, also im Beispiel
festzustellen, ob p und q immer oder niemals identisch sind. Zur Laufzeit können
wir solche Information durch if(p==q)... leicht erhalten.
Das Hauptproblem an der Kommunikation zwischen Variablen besteht darin, dass Kommunikationsstrukturen die über die Kontrollstrukturen aufgebaute
Programmstruktur unterlaufen, damit auch die bewusst gemachten Einschränkungen durch die strukturierte Programmierung. Die Verschleierung durch Aliase kann auf mehrerlei Weise gesehen werden: Aus einer Sicht verstärkt sie
die Problematik und ist daher bestmöglich einzuschränken (eine herkömmliche,
eher intuitive Vorgehensweise). Aus einer anderen Sicht ist sie nicht die Ursache,
sondern nur ein Verstärkungsfaktor und die Ursache sollte gelöst werden (referentielle Transparenz in der funktionalen Programmierung). Aus einer dritten
Sicht sind Programme so zu gestalten, dass stets maximale Verschleierung angenommen wird, also darauf verzichtet wird, den genauen Ablauf zu verstehen
und bei Bedarf zur Laufzeit Abfragen und Programmverzweigungen einzusetzen
(eine professionelle Vorgehensweise in der objektorientierten Programmierung).
Jede dieser Sichtweisen ist gerechtfertigt.
Die Verwendung von Programmteilen als Daten erschwert das Verständnis
von Programmen zusätzlich: Werden Funktionen oder Modularisierungseinheiten als Parameter übergeben (das ist eine Kurzform für „als Argumente an
Parameter übergeben“), dann werden implizit auch Kontrollstrukturen übergeben, die über den Parameter beliebig benutzbar sind. Wie wir schon gesehen haben, ist das ein sinnvolles und sehr mächtiges Instrument. Ein Nachteil
besteht jedoch darin, dass der zur Ausführung gebrachte (dynamische) Kontrollfluss nicht mehr vollständig mit dem im Programmtext direkt ersichtlichen
(statischen) Kontrollfluss übereinstimmt. Um den Programmablauf auf einer
niedrigen Ebene im Detail zu verstehen, müssten wir den Datenfluss, also die
Weitergabe der Kontrollstrukturen als Daten in allen Einzelheiten nachvollziehen. Da alle Arten von Variablen indirekt auch Kontrollstrukturen enthalten
können, ist das äußerst schwierig oder sogar unmöglich. Die übliche Vorgehensweise besteht darin, gar nicht zu versuchen, den Programmablauf auf dieser
Detailebene zu verstehen. Die Lösung liegt in der Abstraktion: Wir verknüpfen
50
1.5 Daten und Datenfluss
eine abstrakte Vorstellung mit einer Variablen, ohne den Inhalt der Variablen
zu betrachten. Wir versuchen, das Programm nur anhand der abstrakten Vorstellung zu verstehen, sodass jeder mögliche bzw. erwartete oder erlaubte Variableninhalt zum gleichen Programmverständnis führt. Diese Vorgehensweise
hat sich in mehreren unterschiedlichen Programmierparadigmen (vor allem in
der objektorientierten und funktionalen Programmierung) als sehr erfolgreich
erwiesen. Allerdings handelt es sich um eine komplexe, professionelle Vorgehensweise, die viel Programmiererfahrung und -disziplin voraussetzt. Das bewusste Zulassen eines gewissen Kontrollverlusts, bzw. das gezielte Ersetzen der
Kontrolle durch intuitive Vorstellungen, war vielleicht einer der wichtigsten Paradigmenwechsel in der Geschichte der Programmierung, der die Entwicklung
einiger hochkomplexer heutiger Systeme erst ermöglicht hat. Ein bedeutender
Teil dieses Skriptums wird sich mit Techniken beschäftigen, die zeigen, wie wir
mit abstrakten Vorstellungen umzugehen haben, um flexibel genug zu bleiben
und trotzdem die Kontrolle nicht ganz zu verlieren.
Programmteile als Daten in Variablen, die auf irgendeine Weise zur Ausführung gebracht werden können, sind ein potenzielles Sicherheitsrisiko. Wenn
wir Programmtexte ausführen, die wir nicht kennen oder kontrollieren können,
könnte Unerwartetes oder Gefährliches passieren. Mehrere typische Techniken
zum Eindringen in geschützte Systeme beruhen darauf. Wir müssen also besondere Sorgfalt walten lassen. Aliase verstärken diese Gefahr. Über ein Alias
in einem weniger gut geschützten Programmbereich könnte gefährlicher Code
untergejubelt werden, der durch die Kommunikation über Variablen in einem
geschützten Bereich automatisch sichtbar wird und zur Ausführung kommt.
Ein vielfältiger Einsatz von Programmteilen als Daten lässt die Bedeutung
von Kontrollstrukturen schwinden. Das bringt uns zur Frage, ob es überhaupt
noch zeitgemäß ist, den Programmablauf auf Kontrollstrukturen zu stützen,
oder ob wir den Programmablauf ganz auf den Fluss von Daten aufbauen
können. Lazy-Evaluation zeigt uns, dass Letzteres nicht nur machbar, sondern
in einigen Bereichen sogar sehr sinnvoll ist. Herkömmliche Programmabläufe,
manchmal als Eager-Evaluation bezeichnet, wenden Operationen wie im Kontrollfluss beschrieben sofort (also so früh wie möglich) auf Operanden an, z. B.
wird 2+5 sofort zu 7 evaluiert. Bei Lazy-Evaluation erfolgen die eigentlichen Berechnungen dagegen so spät wie möglich, 2+5 bleibt unverändert, solange das
Ergebnis nicht verwendet wird; erst wenn die Auswertung unumgänglich ist,
wird sie durchgeführt, etwa in System.out.print(2+5) unmittelbar vor der
Ausgabe. Das heißt, die Programmabarbeitung entsprechend des Kontrollflusses baut nur ein Netz von Abhängigkeiten zwischen Operationen, Werten und
Variablen auf, ohne die Operationen auszuführen. Erst wenn eine abschließende
Operation das Ergebnis benötigt, wird das Netz der Abhängigkeiten so weit reduziert, bis das Ergebnis vorliegt, jedoch nicht weiter. Der Begriff Reduktion verdeutlicht, dass es um Reduktionen wie im λ-Kalkül geht: Für Lazy-Evaluation
wählen wir in jedem Reduktionsschritt einen am weitesten außen liegenden
reduzierbaren λ-Ausdruck, für Eager-Evaluation, einen am weitesten innen liegenden. Auf den ersten Blick mag die Vorgehensweise bei Lazy-Evaluation als
umständlich erscheinen, die eigentlichen Berechnungen werden ja nur hinaus51
1 Grundlagen und Zielsetzungen
gezögert. Aber genau das ist einer der Vorteile: Berechnungen werden vielleicht
so lange hinausgezögert, bis ihre Ergebnisse (und damit auch die Berechnungen) gar nicht mehr nötig sind. In der Praxis zeigt sich, dass viele berechnete
Teilergebnisse nie verwendet werden und sich der Zusatzaufwand daher möglicherweise lohnt. Wichtiger ist, dass Lazy-Evaluation neue Programmierstile
ermöglicht. Beispielsweise ist es ganz einfach, mit potenziell unendlich langen
Listen zu arbeiten, weil ohnehin nur die gebrauchten Listenelemente tatsächlich
berechnet werden, der Rest wird nicht berechnet. Einige Programmiersprachen
wie Haskell beruhen ganz auf Lazy-Evaluation, viele anderen Sprachen, einschließlich Java, verwenden Lazy-Evaluation nur in bestimmten Bereichen.
Eine Schwierigkeit besteht in der unterschiedlich langen Lebenszeit von Variablen. Würden wir einen Zeiger in einer globalen Variable auf den Inhalt einer
lokalen Variable zeigen lassen, würde dieser Zeiger ins Leere gehen, sobald die
lokale Variable nicht mehr existiert. Einige Programmiersprachen überlassen
es ganz den Programmierer_innen, mit dieser Problematik umzugehen (z. B.
C und C++). Aktuellere Sprachen sorgen in der Regel dafür, dass nur von
kurzlebigen Variablen auf längerlebige gezeigt werden kann, nicht umgekehrt.
Das hat jedoch einen Preis: Viel mehr Daten als unbedingt nötig werden in potenziell langlebigen Variablen in Modularisierungseinheiten, insbesondere Objekten abgelegt, nicht in lokalen Variablen, wodurch ein verstärkter Bedarf an
Speicherbereinigung entsteht.
1.5.2 Wiedererlangung der Kontrolle
Kommen wir zurück auf den Kern des Problems: Der Datenfluss unterläuft Programmstrukturen, die durch den Kontrollfluss vorgegeben sind. Aliase und die
Verwendung von Kontrollstrukturen als Daten verstärken den Effekt, sodass es
insgesamt sehr schwierig wird, den Überblick zu behalten; die Kontrolle geht
verloren. Wir haben zwar schon einige Lösungsansätze angedeutet, müssen diese aber noch genauer betrachten und vor allem mit konkreten Zielsetzungen
in Verbindung bringen. Das gelingt am besten, indem wir die entsprechenden
Lösungsansätze in unterschiedlichen Paradigmen einander gegenüberstellen.
Prozedurale Programmierung. Das Ziel besteht darin, die Programmstruktur möglichst klar durch den Kontrollfluss abzubilden. Es wird versucht, unvermeidliche Störfaktoren klein zu halten. Auch wenn die Ursprünge in die Zeit
von „Goto“ zurückgehen, steht die strukturierte Programmierung auf der Basis von Kontrollstrukturen, die ihre Wirkung hauptsächlich über Seiteneffekte
erzielen, zentral im Mittelpunkt. Globale Variablen sind aus Effizienzgründen
zugelassen und spielen praktisch gesehen eine wichtige Rolle, gleichzeitig wird
vor ausufernden Verwendungen globaler Variablen gewarnt, weil ihr störerischer
Einfluss auf die Programmstruktur bekannt ist. Auch vor gefährlichen Verwendungen von Aliasen wird gewarnt, obwohl ihr Auftreten kaum zu verhindern ist.
Modularisierungseinheiten sind, falls vorhanden, bewusst stark eingeschränkt,
jedenfalls sind sie nicht als Daten verwendbar. Auch Prozeduren werden meist
nicht als Daten angesehen, jedenfalls spielen sie als Daten keine große Rolle.
52
1.5 Daten und Datenfluss
Manchmal können Prozeduren als Eingangsparameter an Prozeduren übergeben werden, aber in die umgekehrte Richtung geht das eher nicht, was auch
durch die beschränkte Lebenszeit lokaler Variablen begründet ist.
Funktionale Programmierung. Anders als in der prozeduralen Programmierung stehen Funktionen, die als Daten verwendbar sind, zentral im Mittelpunkt.
Von Anfang an war klar, dass dies einen auf Seiteneffekten beruhenden Kontrollfluss untergräbt, weshalb auf entsprechende Kontrollstrukturen verzichtet
wurde. Das Ziel ist, stattdessen Kontrollstrukturen aus der Verwendung von
Funktionen als Daten aufzubauen. Strukturierte Programmierung ist nicht von
der Sprache vorgegeben, aber viele Programme halten sich dennoch an die entsprechenden Prinzipien. Ursprünglich waren Seiteneffekte erlaubt (etwa in Lisp
und Scheme), sie galten aber bald als unerwünscht, weil sich Aliase zu bedeutenden Störfaktoren entwickelten. Inzwischen hat sich referentielle Transparenz
als Ziel neuerer funktionaler Sprachen durchgesetzt, was bedeutet, dass nicht
zwischen dem Original und einer Kopie unterschieden wird, das heißt, kein Unterschied zwischen gleichen und identischen Dingen gemacht wird. Das wird
durch die gänzliche Vermeidung von Seiteneffekten erreicht (außer für die Einund Ausgabe, aber auch dafür gibt es trickreiche Lösungen). Damit verliert
der Umgang mit Aliasen den Schrecken und Modularisierungseinheiten werden
problemlos einsetzbar. Der offensichtliche Nachteil, dass mangels Seiteneffekten
keine Kommunikation über Variablen außerhalb der durch Funktionsaufrufe
gegebenen Programmstruktur möglich ist, wird als Vorteil gesehen. Paradoxerweise hat sich durch Verzicht auf vorgegebene Kontrollstrukturen und Seiteneffekte etwas ergeben, was die ursprünglichen Ziele hinter Kontrollstrukturen
sehr gut erreicht. Beim praktischen Programmieren müssen wir uns jedoch daran gewöhnen, dass die Einschränkungen restriktiv und nicht umgehbar sind,
sowie daran, dass neben λ-Abstraktion und nominaler Abstraktion auch strukturelle Abstraktion eingesetzt wird (also viele Variablen für – abgesehen von
Typinformation – unbekannte ausführbare Funktionen stehen).
Objektorientierte Programmierung. Die objektorientierte Programmierung
folgt der prozeduralen Herangehensweise, stellt aber Objekte als Daten in den
Mittelpunkt. Zunächst ergeben sich alle angesprochenen Schwierigkeiten, die
durch ein Bündel an Maßnahmen in den Griff gebracht werden:
• Jedes Objekt (allgemeiner jede Modularisierungseinheit) wird als nominaler abstrakter Datentyp betrachtet und ist damit auf einer abstrakten
Ebene, ohne Kenntnis von Implementierungsdetails verständlich.
• Variablen enthalten ausschließlich (Referenzen auf) Objekte, wobei aus
pragmatischen Gründen manche Objekte, etwa Zahlen, speziell behandelt werden. Damit ist jeder Variableninhalt als Instanz eines nominalen
abstrakten Datentyps verwendbar.
• Auf Variablen in Modularisierungseinheiten wird von außen nicht zugegriffen, innerhalb schon. Das ermöglicht die örtlich eingegrenzte einfache
53
1 Grundlagen und Zielsetzungen
Kommunikation über Variablen, aber auch die von konkreten Daten unabhängige abstrakte Sichtweise von außen (als abstrakter Datentyp). Aliase
ermöglichen dennoch die uneingeschränkte Kommunikation über Variablen, ohne Rücksicht auf Grenzen von Modularisierungseinheiten.
• Einzelne Methoden sind entsprechend der strukturierten Programmierung
über prozedurale Kontrollstrukturen aufgebaut. Neben lokalen Variablen
und Parametern wird auch großzügig auf Variablen der eigenen Modularisierungseinheit zugegriffen; das gilt nicht als verpönt.
• Methoden aus anderen Objekten werden über dynamisches Binden aufgerufen, wodurch nicht von vorne herein bekannt ist, welche Methoden
zur Ausführung kommen. Damit ist ein statisches Verständnis der genauen Methodenausführungen auf Detailebene ausgeschlossen. Es wird ein
Verständnis auf einer abstrakten Ebene erzwungen.
• Mit Aliasen wird offensiv umgegangen, sie gelten nicht als Problem. Es
wird streng zwischen dem Original und seiner Kopie unterschieden, auch
Gleichheit und Identität werden klar voneinander unterschieden.
Zusammengefasst: Innerhalb einer Modularisierungseinheit gibt es große Freiheit mit allen damit verbundenen Gefahren, über Grenzen von Modularisierungseinheiten hinweg besteht die größtmögliche Abgrenzung. Aliase bleiben
auf globaler Ebene deutlich sichtbar. Ihre Auswirkungen werden dadurch reduziert, dass alle Variableninhalte als abstrakte Objekte mit klar definierter
Identität betrachtet werden. Beim praktischen Programmieren steht der Umgang mit nominalen Abstraktionen ganz zentral im Mittelpunkt.
Häufig wird von paradigmenübergreifenden Sprachen und Systemen gesprochen. Dahinter steckt das Ziel, die Grenzen zwischen den Paradigmen zu überwinden. Wenn wir allerdings sehen, wie starke Unterschiede zwischen den Paradigmen sich schon bei vergleichsweise elementaren Aspekten ergeben, können
wir erahnen, wie schwierig die Überwindung der Grenzen sein muss.
1.5.3 Verteilung der Daten
Unterschiedliche Herangehensweisen bewirken unterschiedliche Datenstrukturen und Verteilungen der Daten im Speicher. Nehmen wir als Beispiel die Daten
zu einer Lehrveranstaltung, die Studierenden Beurteilungen einzelner Lehrveranstaltungsteile zuordnet. Die prozedurale Programmierung versucht Aliase zu
vermeiden und führt daher häufig, aber nicht immer zu Strukturen, wo jedes
Datenelement auf nur eine einheitliche Weise auffindbar ist. Das Beispiel könnte
etwa über assoziative Datenstrukturen gelöst sein, je eine für Studierendendaten
(Namen, Mailadressen, Studienkennzahlen, etc.) und jeden einzeln beurteilten
Lehrveranstaltungsteil, die jeweils Matrikelnummern als Schlüssel verwenden.
In der funktionalen Programmierung ist es schwierig, einzelne Daten in großen
Strukturen zu ändern, weil dabei die ganzen Strukturen neu aufgebaut werden
54
1.5 Daten und Datenfluss
müssen. Das führt häufig zu einer Strukturierung, in der stabile (sich kaum
ändernde) Daten von solchen getrennt werden, die ständigen Änderungen unterliegen. Gleichzeitig sollen zu viele voneinander getrennte Datenstrukturen
vermieden werden, weil Funktionen dafür jeweils einen eigenen Parameter benötigen. Beispielsweise könnte es eine stabile Datenstruktur mit Studierendendaten (wie im prozeduralen Fall) und eine davon getrennte, sich ständig erweiternde Datenstruktur mit Beurteilungen geben, aber keine Trennung zwischen
Lehrveranstaltungsteilen. Häufig geänderte Beurteilungsdaten können auf stabile Studierendendaten verweisen, umgekehrt ergäben sich Effizienzprobleme.
Die objektorientierte Programmierung fasst häufig ganz unterschiedliche Daten zu einer Einheit zusammen, um Variablen zur Kommunikation zu nutzen.
Im Beispiel würden alle Daten zu einem Studierenden in einem einzelnen Objekt
gekapselt, einschließlich der Beurteilungsdaten.
Während es in der prozeduralen, funktionalen und objektorientierten Programmierung vorwiegend um den Überblick über den Programmablauf geht
und sich die Verteilungen der Daten eher zufällig ergeben, gehen einige andere
Paradigmen von bestimmten vorgegebenen Datenverteilungen aus.
Verteilte Programmierung. Dabei wird von einer Verteilung der Daten über
mehrere oder viele Rechner ausgegangen, wobei gleiche Daten redundant auf
mehreren Rechnern vorhanden sein können. Gründe für die Verteilung sind vielfältig. Häufig ist die Datenmenge und die Anzahl der Zugriffe zu groß, um sie
auf einem Rechner handhaben zu können, manchmal ist eine Datenauslagerung zwecks Datenschutz oder aus wirtschaftlichen oder rechtlichen Gründen
erforderlich. In jedem Fall müssen der Ort der Daten und der Ort der Berechnungen zusammengebracht werden. Das lässt sich bewerkstelligen, indem
Rohdaten, oder wenn Berechnungen am Ort der Daten erfolgen, erst Ergebnisse
weitergegeben werden. Es ergibt sich eine gewaltige Zahl an Möglichkeiten und
Optimierungen zur Verringerung des Datenverkehrs, wenn Berechnungen nicht
nur lokal, sondern über ein Netzwerk verteilt stattfinden. Das Ziel der verteilten
Programmierung besteht im Finden einer guten Strategie für die Verteilung der
Daten und Berechnungen. In diesem Kontext werden Überlegungen hinter der
prozeduralen, funktionalen und objektorientierten Programmierung als engstirnig betrachtet, weil die Problematik auf anderen Gebieten liegt; ein Verständnis
des Kontrollflusses ist vergleichsweise einfach. Dennoch gibt es Querverbindungen, weil etwa eine Strukturierung von Daten wie in der objektorientierten Programmierung den Austausch von (im Beispiel) Daten zu einzelnen Studierenden
unterstützt, während eine Strukturierung wie in der funktionalen Programmierung dabei hilft, mehrere Kopien der Daten auf einfache Weise zu verwalten.
Parallele Programmierung. Das Ziel ist die Erreichung kurzer Laufzeiten.
Meist ist eine große Menge eher homogener (gleichartiger) Daten in kurzer Zeit
zu verarbeiten, wobei häufig auch spezielle Hardware zum Einsatz kommt. Daten müssen so strukturiert sein, wie es der Hardwareeinsatz verlangt. Meist ist
es notwendig, die Daten in mehrere Bereiche zu untergliedern, die unabhän55
1 Grundlagen und Zielsetzungen
gig voneinander auf unterschiedlichen Recheneinheiten verarbeitbar sind. Eine
Datenstrukturierung wie in der objektorientierten Programmierung ist dabei
hinderlich. Im Beispiel sollen etwa Daten über Beurteilungen unterschiedlicher
Lehrveranstaltungsteile unabhängig voneinander verarbeitbar sein, was aber
nicht geht, wenn Studierendendaten zu Objekten zusammengefügt sind. Umgekehrt entspricht eine für die parallele Programmierung optimierte Datenstruktur nur selten auch den Prinzipien der objektorientierten Programmierung. Datenstrukturierungen wie in der funktionalen Programmierung sind ebenso nicht
von vorne herein ideal, aber eine im Hinblick auf die Parallelverarbeitung optimierte Datenstruktur lässt sich meist auch ganz gut mit der funktionalen Programmierung kombinieren. Oft setzt die parallele Programmierung nach wie
vor auf der prozeduralen Programmierung auf, weil einige der Zielsetzungen
übereinstimmen, etwa die bestmögliche Vermeidung von Aliasen.
Nebenläufige Programmierung. Es soll auf Ereignisse reagiert werden, die
zu unvorhersehbaren Zeitpunkten auftreten. Meist bestehen gleichzeitig viele,
logisch in sich geschlossene Handlungsstränge, die jedoch nicht in einem Stück
ausführbar sind, weil auf das Eintreffen angeforderter Daten gewartet werden
muss. Nehmen wir als Beispiel einen Web-Server, der gleichzeitig viele offene
Verbindungen bearbeitet, aber für jede Verbindung den Großteil der Zeit mit
dem Warten auf das Eintreffen ausgefüllter Formulare verbringt. Die Verbindungen sind logisch voneinander getrennt, z. B. wegen unterschiedlicher Zugangsdaten und Zugriffsrechte. Am einfachsten lässt sich das durch nebenläufige Programmierung handhaben, indem jeder Handlungsstrang etwa als Java-Thread
(Sprachkonstrukt für Handlungsstränge) abgebildet wird. Der englische Begriff
concurrent programming drückt nicht so klar aus, worum es geht. Es können
ähnliche Techniken wie in der Parallelprogrammierung eingesetzt werden, aber
die Zielsetzung ist eine andere, weil eine kurze Laufzeit nur eine untergeordnete Rolle spielt; die meiste Zeit wird ja mit Warten verbracht. Wichtiger ist es,
viele Handlungsstränge handhaben zu können. Gleichzeitige Zugriffe auf gleiche
Daten durch unterschiedliche Handlungsstränge sind nur mittels Synchronisation verhinderbar. Die Daten müssen so strukturiert sein, dass unterschiedliche
Handlungsstränge möglichst auf unterschiedliche Daten zugreifen. Die ideale
Strukturierung hängt von Details ab. Manchmal ist eine Strukturierung wie in
der objektorientierten Programmierung gut geeignet, in vielen Fällen nicht.
Echtzeitprogrammierung. Dabei geht es darum, dass auf Ereignisse von außen innerhalb einer vorgegebenen, meist kurzen Maximalzeit reagiert werden
muss. Häufig sind Garantien für die Einhaltung der Maximalzeit nötig. Diese
Forderung macht die Echtzeitprogrammierung sehr speziell und unterscheidet
sie von allen anderen Programmierparadigmen und -stilen. Daten müssen so
vorliegen, dass Zugriffe darauf in einer vorgegebenen Maximalzeit abgeschlossen
sind. Auf Zufall beruhende Datenstrukturen wie etwa Hashtabellen sind dafür
kaum geeignet, ebenso wenig wie Datenstrukturierungen, die üblicherweise in
der funktionalen und objektorientierten Programmierung entstehen.
56
1.6 Typisierung
Metaprogrammierung. Dabei werden Programme selbst als Daten angesehen. Je nach Ausformung werden Programme zur Laufzeit erstellt, verändert
oder nur gelesen und Programmtexte so ausgeführt, wie es im ursprünglichen
Programm nicht vorgesehen war. Alle Bestandteile eines Programms müssen
in Form von Daten vorliegen. Metaprogrammierung ist mit allen Paradigmen
kombinierbar (außer vielleicht Echtzeitprogrammierung), kann dabei aber einen
beträchtlichen Einfluss auf die Strukturierung der Daten haben.
1.6 Typisierung
Die Bedeutung von Typen in Programmiersprachen nimmt stetig zu, obwohl
Typen Einschränkungen darstellen: Wir können nicht beliebige Werte, Ausdrücke, etc. verwenden, sondern nur solche, die vorgegebenen Typen entsprechen. Als Gegenleistung erhalten wir bessere Planbarkeit – weil wir früher wissen, womit wir es zu tun haben – und bestimmte Formen der Abstraktion.
Beides verbessert die Lesbarkeit und Zuverlässigkeit von Programmen.
1.6.1 Typkonsistenz, Verständlichkeit und Planbarkeit
Berechnungsmodelle kennen nur je eine Art von Werten, z. B. nur Zahlen, Symbole oder Terme. In Programmiersprachen unterscheiden wir mehrere Arten
von Werten, z. B. ganze Zahlen, Gleitkommazahlen und Zeichenketten. Typen
helfen bei der Klassifizierung der Werte. Viele Operationen sind nur für Werte
bestimmter Typen definiert. Beispielsweise sind nur ganze Zahlen oder Gleitkommazahlen miteinander multiplizierbar, aber keine Zeichenketten. Wenn die
Typen der Operanden mit der Operation zusammenpassen, sind die Typen konsistent. Andernfalls tritt ein Typfehler auf. Ohne Typen kommen wir nur aus,
wenn alle Operationen auf alle Operanden anwendbar sind, was praktisch nie
der Fall ist. Daher gibt es in jeder Programmiersprache Typen.
Typprüfungen. Es gibt erhebliche Unterschiede zwischen Sprachen hinsichtlich des Umgangs mit Typen. Nur wenige hardwarenahe Sprachen prüfen Typkonsistenz gar nicht, etwa Assembler oder Forth; bei einem Typfehler werden
Bit-Muster falsch interpretiert. Einige Sprachen prüfen Typkonsistenz in vielen,
aber nicht allen Fällen; etwa bleiben in C Arraygrenzen und Typumwandlungen
unüberprüft. Die meisten Sprachen prüfen Typkonsistenz vollständiger.
Sprachen unterscheiden sich hinsichtlich des Zeitpunkts, an dem Typen bekannt sind und überprüft werden. In dynamisch typisierten Sprachen ergeben
sich Typen erst zur Laufzeit und werden bei der Anwendung einer Operation
dynamisch überprüft. In statisch typisierten Sprachen sind genaue Typen aller Ausdrücke bereits dem Compiler bekannt, der sie für statische Prüfungen
nutzt. Bei statischer Prüfung kann zur Laufzeit kein Typfehler auftreten. Bei
dynamischer Prüfung müssen wir immer mit Typfehlern rechnen. Allerdings
ist die statisch verfügbare Information begrenzt. Das heißt, für manche Aspekte (etwa Arraygrenzen) ist dynamische Prüfung nötig, oder die Flexibilität ist
57
1 Grundlagen und Zielsetzungen
so eingeschränkt, dass statische Prüfung ausreicht. Dank ausgefeilter statischer
Typanalysen gibt es heute hinreichend flexible Sprachen ausschließlich auf Basis statischer Typprüfungen, z. B. Haskell. Viele Sprachen bevorzugen dennoch
einen Mix aus dynamischer und statischer Prüfung. Zwischen dynamisch und
statisch typisierten Sprachen liegen stark typisierte Sprachen, in denen der Compiler statisch Typsicherheit garantiert, obwohl nicht alle Typinformation zur
Compilezeit vorliegt.5 Objektorientierte Sprachen wie Java sind häufig, bis auf
Aspekte wie Arraygrenzen, stark typisiert; deklarierte Typen reichen für statische Typprüfungen, obwohl viel Typinformation durch dynamisches Binden
erst zur Laufzeit bekannt wird.
Der Hauptgrund für statische Typprüfungen scheint die verbesserte Zuverlässigkeit der Programme zu sein. Das stimmt zum Teil, aber nicht auf direkte
Weise. Typkonsistenz bedeutet ja nicht Fehlerfreiheit, sondern nur die Abwesenheit bestimmter, eher leicht auffindbarer Fehler. Manchmal hört man die
Meinung, dynamisch typisierte Sprachen seien sogar sicherer, weil Programme
besser getestet und dabei neben Typfehlern auch andere Fehler entdeckt würden. Es bleibt offen, ob dahinter ein wahrer Kern steckt. Viel wichtiger ist die
Tatsache, dass explizit hingeschriebene Typen, die es in dynamisch typisierten
Sprachen meist nicht gibt, das Lesen und Verstehen der Programme erleichtern.
Die statische Prüfung sorgt dafür, dass die Information in den Typen zuverlässig
ist – die einzig mögliche Form zuverlässiger Kommentare. Besseres Verstehen
erhöht die Zuverlässigkeit der Programme, die Typprüfungen selbst aber kaum.
Auch in statisch typisierten Sprachen müssen nicht alle Typen hingeschrieben
sein. Viele Typen kann ein Compiler aus der Programmstruktur herleiten; wir
sprechen von Typinferenz. Beispielsweise müssen wir in Haskell keinen Typ hinschreiben, obwohl der Compiler statisch Typkonsistenz garantiert. Zur besseren
Lesbarkeit sollen wir Typen dennoch hinschreiben. Bis zu einem gewissen Grad
erhöht Typinferenz die Verständlichkeit, obwohl keine Typen dabei stehen. Das
hat mit reduzierter Flexibilität zu tun: Es sind keine so komplexen Programmstrukturen verwendbar, dass der Compiler bei der Prüfung der Typkonsistenz
überfordert wäre. Einfachere Programmstrukturen sind nicht nur für Compiler,
sondern auch für Menschen einfacher verständlich.
Typen und Entscheidungsprozesse. Bedeutende Entscheidungen hinsichtlich
Programmstruktur und Programmablauf müssen schon früh getroffen werden.
Hier ist eine Einteilung von Entscheidungszeitpunkten:
• Einiges ist bereits in der Sprachdefinition oder in der Sprachimplementierung festgelegt. Beispielsweise ist festgelegt, dass Werte von int ganze
Zahlen in einer 32-Bit-Zweierkomplementdarstellung sind und vordefinierte Operationen darauf nicht auf Über- bzw. Unterläufe prüfen. Programme können daran nichts ändern.
5Der Begriff stark typisiert ist schon lange in der hier genannten Bedeutung etabliert. Allerdings haben Programmiersprachentwickler mehrfach versucht, ihn für sich zu reklamieren
und umzudefinieren. Daher hat er heute in einigen Kreisen eine etwas andere Bedeutung.
58
1.6 Typisierung
• Zum Zeitpunkt der Erstellung von Übersetzungseinheiten werden die meisten wichtigen Entscheidungen getroffen, auf die wir beim Programmieren
Einfluss haben. Hierzu braucht es viel Flexibilität.
• Manche wichtige Entscheidungen werden durch Parametrisierung erst bei
der Einbindung vorhandener Module, Klassen oder Komponenten getroffen. Dies geht jedoch nur, wenn die eingebundenen Modularisierungseinheiten dafür vorgesehen sind.
• Vom Compiler getroffene Entscheidungen sind eher von untergeordneter
Bedeutung und betreffen meist nur Optimierungen. Alles Wichtige ist
bereits im Programmcode festgelegt oder liegt erst zur Laufzeit vor.
• Zur Laufzeit ist die Initialisierungsphase von der eigentlichen Programmausführung zu unterscheiden. Erstere dient auch der Einbindung von Modularisierungseinheiten und der Parametrisierung, wo dies erst zur Laufzeit möglich ist. Zur Laufzeit getroffene Entscheidungen folgen einem fixen, im Programm festgelegten Schema.
Typen verknüpfen die zu unterschiedlichen Zeitpunkten vorliegenden Informationen miteinander. Vor allem statisch geprüfte Typen helfen dabei, einmal
getroffene Entscheidungen über den gesamten folgenden Zeitraum konsistent zu
halten. Einige Beispiele in Java sollen dies verdeutlichen:
• Angenommen, bei der Erstellung einer Klasse bekommt eine Variable den
Typ int. Der Compiler reserviert so viel Speicherplatz, wie in der Sprachdefinition für int (32 Bit) vorgesehen ist. Zur Laufzeit wird ebenso wie
bei der Programmerstellung fix davon ausgegangen, dass die Variable eine
Zahl im entsprechenden Wertebereich enthält.
• Ist der Typ der Variablen ein Typparameter, reserviert der Compiler den
für eine Referenz nötigen Speicherplatz. Bei der Programmerstellung und
zur Laufzeit wird innerhalb der Klasse, in der die Variable deklariert ist,
von einer Referenz unbekannten Typs ausgegangen. Wird der Typparameter durch Integer ersetzt (int darf in Java keine Typparameter ersetzen),
kann davon ausgegangen werden, dass die Variable ein Objekt dieses Typs
referenziert. Vor dem Ablegen einer Zahl vom Typ int in der Variablen
wird ein Integer-Objekt erzeugt und beim Auslesen wieder die int-Zahl
extrahiert. Code dafür erzeugt der Compiler automatisch.
• Hat die Variable einen allgemeinen Typ wie Object, reserviert der Compiler Platz für eine Referenz. Zur Laufzeit muss bei Verwendung der Variablen in der Regel eine Fallunterscheidung getroffen werden, da unterschiedliche Arten von Werten unterschiedliche Vorgehensweisen verlangen. Diese Fallunterscheidungen stehen im Programmcode innerhalb einer
Klasse oder verteilt auf mehrere Klassen.
Je früher Entscheidungen getroffen werden, desto weniger ist zur Laufzeit zu
tun und desto weniger Programmtext für Fallunterscheidungen ist nötig. Ohne
59
1 Grundlagen und Zielsetzungen
statisch geprüfte Typen wäre mehrfacher Aufwand nötig: Sogar wenn wir wissen, dass die Variable eine ganze Zahl enthält, muss der Compiler eine beliebige
Referenz annehmen und Code für eine dynamische Typprüfung erzeugen.
Frühe Entscheidungen haben einen weiteren Vorteil: Typfehler und andere damit zusammenhängende Fehler zeigen sich früher, wo ihre Auswirkungen
noch nicht so schwerwiegend sind. Vom Compiler entdeckte Fehler sind meist
einfach zu beseitigen. Auf Fehler, die zur Laufzeit in einer Initialisierungsphase erkannt werden, kann effektiver reagiert werden als auf Fehler, die während
der eigentlichen Programmausführung auftreten. Auch in dynamisch typisierten
Sprachen können wir Typfehler schon in der Initialisierungsphase zu erkennen
versuchen. Dazu gibt es die Möglichkeit, den Typ eines Werts abzufragen.
Planbarkeit. Frühe Entscheidungen erleichtern die Planung weiterer Schritte.
Ist eine Variable vom Typ int, ist klar, welche Werte die Variable enthalten
kann. Statt auf Spekulationen bauen wir auf Wissen auf. Um uns auf einen
Typ festzulegen, müssen wir voraussehen (also planen), wie bestimmte Programmteile im fertigen Programm verwendet werden. Wir werden zuerst jene
Typen festlegen, bei denen kaum Zweifel an der künftigen Verwendung bestehen. Frühe Entscheidungen sind daher oft stabil. Unsichere Entscheidungen
werden eher nach hinten verschoben und zu einem Zeitpunkt getroffen, an dem
bereits viele andere damit zusammenhängende Entscheidungen getroffen wurden und der Entscheidungsspielraum entsprechend kleiner ist. Probleme können
sich durch notwendige Programmänderungen ergeben: Wenn in Typen dokumentierte Entscheidungen revidiert werden müssen, sind alle davon abhängigen
Programmteile anzupassen. Bei statischer Typprüfung hilft der Compiler, die
betroffenen Programmstellen zu finden; an diesen Stellen werden Typen nicht
mehr konsistent verwendet und müssen geändert werden.
Wahrscheinlich sind Verbesserungen der Lesbarkeit und Verständlichkeit sowie der Unterstützung früher Entscheidungen zusammen mit der Planbarkeit
die wichtigsten Gründe für die Verwendung statisch geprüfter Typen. Mit Abstrichen können wir Ähnliches auch in dynamischen Sprachen erreichen. Häufig
finden wir Variablennamen, welche die Art der enthaltenen Werte beschreiben.6 Damit wird nicht nur bessere Lesbarkeit bezweckt, sondern es werden
auch getroffene Entscheidungen festgehalten. Bei Einhaltung entsprechender
Konventionen funktioniert das gut. Dazu braucht es viel Programmierdisziplin.
Gelegentlich geht die Disziplin genau dann verloren, wenn es darauf ankommt –
dann, wenn in Variablennamen dokumentierte Entscheidungen revidiert werden
müssen. Es ist schwer, alle von einer frühen Entscheidung abhängigen Stellen
zu finden. Oft bleiben aus Bequemlichkeit bestehende Namen erhalten, obwohl
die durch die Namen suggerierten Informationen nicht mehr stimmen.
6Dabei geht jedoch viel von der Flexibilität dynamisch typisierter Sprachen verloren, weil
gerade die dynamische Typisierung nicht genutzt wird. Das gleiche Programm hätte auch
in einer stark oder statisch typisierten Sprache geschrieben werden können.
60
1.6 Typisierung
1.6.2 Nominale und strukturelle Typen
Wie wir in Abschnitt 1.4.2 gesehen haben, ermöglichen nominale Typen einige
Formen der Abstraktion, die mit strukturellen Typen nicht möglich sind. Der
einzige Unterschied zwischen nominalen und strukturellen Typen besteht darin,
dass nominale Typen in expliziten Typdefinitionen eingeführt werden und dabei
Namen bekommen. Hinsichtlich der Struktur, also des inneren Aufbaus, gibt es
keinen Unterschied. Die Einführung von Namen alleine reicht schon aus, um zu
bewirken, dass alle nominalen Typen vom Compiler als voneinander verschieden
angesehen werden, auch wenn sie die gleiche Struktur haben. Dazu kommt der
praktische Vorteil, dass die Definition eines Typs nur an einer einzigen Stelle
im Programm erfolgt und alle Informationen dazu an dieser Stelle gesammelt
aufzufinden sind.
Allerdings haben nominale Typen nicht nur Vorteile gegenüber strukturellen
Typen, sondern auch Nachteile. Strukturelle Typen sind vor allem einfacher
und flexibler verwendbar, weil keine speziellen Typdefinitionen nötig sind. Außerdem verringern sie die Gefahr von Namenskonflikten. Der einzige Nachteil
struktureller Typen liegt in Schwierigkeiten im Umgang mit Abstraktionen. Wir
wollen uns vor Augen führen, was das für einige Aspekte praktisch bedeutet.
Untertypen. Untertypbeziehungen werden in der objektorientierten Programmierung durch das Ersetzbarkeitsprinzip [8, 25] definiert:
Ein Typ U ist Untertyp eines Typs T wenn jedes Objekt von U
überall verwendbar ist, wo ein Objekt von T erwartet wird.
Ohne Ersetzbarkeit (siehe Abschnitt 1.3.3) gibt es also keine Untertypen.
Wie wir in Kapitel 3 sehen werden, sind Untertypbeziehungen durch das Ersetzbarkeitsprinzip für strukturelle Typen ganz eindeutig definiert. Die Theorie
lässt keinen Spielraum: Sind zwei strukturelle Typen gegeben, kann ein Compiler durch Anwendung fixer Regeln automatisch ermitteln, ob ein Typ Untertyp
des anderen ist [2]. Untertypbeziehungen auf strukturellen Typen sind damit
implizit; wir müssen nicht angeben, dass U Untertyp von T ist.
Für nominale Typen reichen einfache Regeln nicht aus. Abstrakte und daher
den Regeln nicht zugängliche Konzepte lassen sich nicht automatisch vergleichen. In der Praxis müssen wir beim Programmieren explizit hinschreiben, welcher Typ Untertyp von welchem anderen ist. Meist verwenden wir abgeleitete
Klassen: Wird eine Klasse U von einer Klasse T abgeleitet, so nehmen wir an,
dass U Untertyp von T ist. Dazu müssen auch die entsprechenden strukturellen
Typen, also die Signaturen von U und T, in einer durch die Regeln überprüfbaren Untertypbeziehung stehen. U soll nur dann von T abgeleitet werden,
wenn das Konzept hinter T durch das Konzept hinter U vollständig ersetzbar
ist, sodass statt eines Objekts von T stets auch ein Objekt von U verwendbar
ist. Beim Programmieren liegt die Einhaltung dieser Bedingung zur Gänze in
unserer Verantwortung. Kein Compiler oder anderes Werkzeug kann sie uns abnehmen. Fälschlich angenommene Untertypbeziehungen zählen zu den folgenschwersten Fehlern in der objektorientierten Programmierung. Die Einführung
61
1 Grundlagen und Zielsetzungen
nominaler Typen ermöglicht nicht nur Abstraktion, sondern verpflichtet uns zu
einem bewussten Umgang damit, weil andernfalls die gesamte Vorstellungswelt
der objektorientierten Programmierung in sich zusammenbrechen würde.
Zusicherungen werden häufig in die Entscheidung von Untertypbeziehungen
einbezogen. Einige wenige Programmiersprachen wie Eiffel [28] integrieren Zusicherungen als Sprachkonstrukte. Allerdings zeigt die Erfahrung, dass Zusicherungen praktisch nie so präzise und vollständig sind, dass dadurch abstrakte
Konzepte bei der Entscheidung von Untertypbeziehungen außer Acht gelassen
werden könnten. Das heißt, die Einbeziehung von Zusicherungen ändert nichts
daran, dass Untertypbeziehungen explizit hingeschrieben und damit die Verantwortung für die Kompatibilität der Konzepte übernommen werden muss.
Daher kennen die meisten Programmiersprachen keine Zusicherungen, die zum
geprüften Teil der Typen gehören.7 Trotzdem empfiehlt es sich, beim Programmieren die wichtigsten Zusicherungen zumindest in Form von Kommentaren
hinzuschreiben und dadurch die gröbsten Fehler bei Untertypbeziehungen zu
verhindern. Kommentare werden damit zu einem Teil der Typen, wodurch Änderungen der Kommentare ähnliche Auswirkungen wie Änderungen der Typen
(konkreter, der öffentlich sichtbaren Schnittstellen) nach sich ziehen. Natürlich
versteht ein Compiler Inhalte von Kommentaren genau so wenig wie die von
dynamisch geprüften Zusicherungen. Explizit angegebene Untertypbeziehungen
sind für den Compiler aber prüfbar und falsche Abstraktionen ergeben echte
Fehler im Programmablauf.
Obwohl die Regeln zur Entscheidung von Untertypbeziehungen struktureller
Typen einfach sind, implizieren sie eine bedeutende Einschränkung, die auch
für nominale Typen gilt: Typen von Funktions- bzw. Methoden-Parametern
(als Eingangsparameter) dürfen in Untertypen nicht stärker werden. Enthält ein
Obertyp T z. B. eine Methode int compareTo(T x) (die x mit this vergleicht),
kann sie im Untertyp U nicht durch int compareTo(U x) überschrieben sein.
Derartiges wird in der Praxis häufig benötigt, nicht nur (allerdings häufig) für
binäre Methoden wie compareTo. Aber es gibt keine Möglichkeit, entsprechende
Typen statisch zu prüfen. Dynamische Prüfungen sind natürlich möglich.
Generizität. Während Subtyping den objektorientierten Sprachen vorbehalten ist, wird Generizität in Sprachen aller Paradigmen mit statischer Typprüfung verwendet. Da es darum geht, Typen als Parameter einzusetzen und diese
Parametrisierung schon zur Übersetzungszeit aufzulösen, muss Generizität sehr
tief in das Typsystem integriert sein. Für dynamisch typisierte Sprachen ist
Generizität aus diesem Grund nicht sinnvoll.
7Für viele Sprachen, auch für Java, gibt es Bibliotheken, die Funktionalität zur dynamischen
Prüfung von Zusicherungen anbieten. Praktisch werden solche Bibliotheken kaum eingesetzt, weil dynamische Prüfungen von Zusicherungen mit erheblichen Nachteilen verbunden sind. Gründe dafür sind zum Teil sehr verzwickt und nicht in wenigen Zeilen erklärbar.
Ein Problem liegt darin, dass zur Laufzeit für Prüfungen verfügbare Information häufig
nicht statisch antizipierbar ist, wodurch dynamisch geprüfte Zusicherungen im Gegensatz
zu abstrakten Vorstellungen oft nur wenig zum statischen Programmverständnis beitragen
und keinesfalls das explizite Hinschreiben von Untertypbeziehungen ersetzen können.
62
1.6 Typisierung
Einfache Generizität ist leicht zu verstehen und auch vom Compiler leicht
handzuhaben. Die Komplexität steigt jedoch rasch an, wenn Einschränkungen auf Typparametern zu berücksichtigt sind, das heißt, wenn Typparameter
nur durch Typen mit bestimmten Eigenschaften ersetzbar sein sollen. Im Wesentlichen gibt es zwei etwa gleichwertige formale Ansätze dafür: F-gebundene
Generizität [7] nutzt Untertypbeziehungen zur Beschreibung von Einschränkungen und wird z. B. in Java eingesetzt. Higher-Order-Subtyping [1] (siehe
Abschnitt 1.4.3) ist dafür ebenso geeignet und unterstützt binäre Methoden direkt. Dieser Ansatz wird auf unterschiedliche Weise beispielsweise in C++, aber
auch in der funktionalen Sprache Haskell verwendet. Beide Ansätze können gut
mit Fällen umgehen, mit denen Untertypen nicht oder nur schwer zurechtkommen. Deswegen unterstützen die meisten stark typisierten objektorientierten
Sprachen Generizität. Jedoch ist Generizität hinsichtlich der Wartung prinzipiell kein vollwertiger Ersatz für Ersetzbarkeit.
Zur Beschreibung von Einschränkungen auf Typparametern wären strukturelle Typen sinnvoll, da es dabei nicht um Abstraktionen geht, sondern um
das Vorhandensein von Funktionen oder Methoden mit bestimmten Signaturen. Allerdings wäre es für viele Menschen kaum nachvollziehbar, wenn Namen
manchmal eine Rolle spielen und in anderen Fällen nicht, sodass z. B. in Java
auch für solche Einschränkungen nominale Typen mit nominalem Subtyping
zum Einsatz kommen. Dahinter stecken pragmatische Überlegungen. Es soll
vermieden werden, mehrere einander ähnliche Hierarchien nebeneinander anzubieten, selbst wenn mehrere Hierarchien echte Vorteile hätten. Higher-OrderSubtyping basierend auf strukturellen Typen in C++ hat sich durchgesetzt, weil
entsprechende Typhierarchien implizit sind, also nur vom Compiler, nicht von
Menschen gesehen werden. Versuche, diese Hierarchien für Menschen sichtbar
zu machen, sind bislang wegen fehlender Akzeptanz großteils gescheitert. Dagegen ist Higher-Order-Subtyping in Haskell ohne Probleme explizit sichtbar,
weil dort mangels Untertypbeziehungen keine Verwechslungsgefahr besteht.
1.6.3 Gestaltungsspielraum
Zahlreiche Entscheidungen im Entwurf einer Sprache spiegeln sich in den Typen
wider. Der Gestaltungsspielraum scheint endlos. Aber die Theorie zeigt klare
Grenzen auf. Beispielsweise haben wir schon angedeutet, dass es im Zusammenhang mit Untertypen eine bedeutende Einschränkung gibt. Im Folgenden
betrachten wir kurz einige weitere Bereiche, teilweise mit für Uneingeweihte
überraschenden Möglichkeiten und Grenzen.
Rekursive Datenstrukturen. Werte vieler Typen wie Listen und Bäume sind
in ihrer Größe unbeschränkt. Diese Typen sind rekursiv. Nicht selten sprechen
wir von potentiell unendlichen Strukturen. Aber tatsächlich unendlich große
Strukturen sind im Computer niemals darstellbar. Würden wir versuchen, Werte rekursiver Typen als möglicherweise unendlich zu betrachten, würden wir
scheitern, weil Typprüfungen in eine Endlosschleife geraten.
63
1 Grundlagen und Zielsetzungen
Der einzig sinnvolle Weg zur Beschreibung von Werten nicht beschränkter
Größe führt über eine induktive Konstruktion: Wir beginnen mit einer endlichen
Menge M0, die nur einfache Werte enthält, z. B. M0 = {end} wenn wir nur
einen einzigen Wert end brauchen. Dann beschreiben wir, wie über endlich
viele Möglichkeiten aus einer Menge Mi die Menge Mi+1 (i ≥ 0) generiert wird,
wobei Mi+1 zumindest alle Elemente von Mi enthält, z. B.
Mi+1 = Mi ∪ {elem(n, x) | n ∈ Int; x ∈ Mi}.
Die meist unendliche Menge M =
S∞
i=0 Mi
, also die Vereinigung aller Mi
, enthält
dann alle beschriebenen Werte. Die als Beispiel konstruierte Menge enthält
die Elemente end, elem(3,end), elem(7,elem(3,end)) und so weiter, also
alle Listen ganzer Zahlen. Vor allem in neueren funktionalen Sprachen können
Typen tatsächlich auf diese Weise konstruiert werden. Der Typ IntList, dessen
Werte die Elemente der oben beschriebenen Menge sind, wird in Haskell so
definiert (wobei „|“ Alternativen trennt):
data IntList = end | elem(Int, IntList)
Aus der Syntax geht auf den ersten Blick hervor, dass IntList ein rekursiver
Typ ist, weil IntList sowohl links als auch in zumindest einer Alternative
rechts von = vorkommt. Rekursion beschreibt die Mengenkonstruktion nur in
leicht abgewandelter Form, so wie Mi
in Mi+1 verwendet wird.
Nicht alle unendlichen Mengen sind auf diese Weise konstruierbar: Wir können z. B. Listen konstruieren, in denen alle Listenelemente vom gleichen Typ
sind oder in denen sich die Typen der Listenelemente zyklisch wiederholen,
aber es ist unmöglich, Listen zu konstruieren, in denen alle Listenelemente
unterschiedliche Typen haben. Die Art der Konstruktion ist nicht willkürlich,
sondern unterscheidet handhabbare Strukturen von solchen, deren Typen nicht
in endlicher Zeit prüfbar sind. In der Praxis brauchen wir sicher nicht mehr, als
auf obige Weise konstruierbar ist.
Wir müssen klar zwischen M0 (nicht-rekursiv) und der Konstruktion aller
Mi mit i > 0 (rekursiv) unterscheiden, wobei M0 nicht leer sein darf. Letztere
Eigenschaft heißt Fundiertheit. In jeder Typdefinition muss es zumindest eine
nicht-rekursive Alternative (z. B. end in IntList) geben.
Viele Sprachen verwenden einen einfacheren Ansatz für nicht-elementare Typen: Die Menge M0 ist etwa in Java schon in der Sprachdefinition vorgegeben
und enthält nur den speziellen Wert null; Klassen beschreiben die Mi mit i > 0.
Da statt eines Objekts immer auch null verwendet werden kann, ist Fundiertheit immer gegeben. Der Nachteil dieser Vereinfachung liegt auf der Hand: Wir
müssen beim Programmieren stets damit rechnen, statt eines Objekts nur null
zu erhalten, auch wenn es gar nicht um rekursive Datenstrukturen geht.
Typinferenz. Die Techniken hinter der Typinferenz sind heute weitgehend
ausgereift. Für bestimmte Aufgaben wird Typinferenz sehr breit eingesetzt,
auch in Java (etwa für Typparameter im Zusammenhang mit Generizität). Ein
prinzipielles Problem ist bis heute jedoch ungelöst: Typinferenz funktioniert
64
1.6 Typisierung
nicht, wenn gleichzeitig, also an derselben Programmstelle, Ersetzbarkeit durch
Untertypen verwendet wird. Aus diesem Grund wird Typinferenz in neueren
funktionalen Sprachen, in denen keine Untertypen verwendet werden, in größtmöglichem Umfang eingesetzt, aber nur lokal und eingeschränkt in objektorientierten Sprachen.
Typinferenz erspart das Hinschreiben von Typen, ändert aber nichts an der
durch statische Typprüfungen reduzierten Flexibilität. Weggelassene Typangaben schwächen obendrein die Dokumentation (Typangaben als in Programmcode gegossene Kommentare). Aber gerade im lokalen Bereich vermeidet Typinferenz das mehrfache Hinschreiben des gleichen Typs an nahe beieinander
liegenden Stellen, ohne den Dokumentations-Effekt zu zerstören. Das kann die
Lesbarkeit sogar verbessern.
Propagieren von Eigenschaften. Statisch geprüfte Typen sind sehr gut dafür
geeignet, statische Information von einer Stelle im Programm an andere Stellen zu propagieren (= weiterzuverbreiten). Beispielsweise kann eine Funktion
nur aufgerufen werden, wenn der Typ des Arguments mit dem des Parameters
übereinstimmt; die Kompatibilität zwischen Operator und Operanden sicherzustellen ist ja eine wesentliche Aufgabe von Typen. Dabei wird Information
über das Argument an die aufgerufene Funktion propagiert. Entsprechendes gilt
auch für das Propagieren von Information von der aufgerufenen Funktion zur
Stelle des Aufrufs unter Verwendung des Ergebnistyps und bei der Zuweisung
eines Werts an eine Variable. Genau diese Art des Propagierens von Information funktioniert nicht nur für Typen im herkömmlichen Sinn, sondern für alle
statisch bekannten Eigenschaften. Wenn wir z. B. wissen, dass das Argument
ein Objekt ungleich null, eine Zahl zwischen 1 und 9 oder eine Primzahl ist,
dann gilt genau diese Eigenschaft innerhalb der Funktion auch für den Parameter. Erst in jüngster Zeit wird das Propagieren beliebiger solcher Information
in Programmiersprachen unterstützt.
Typen werden in der Regel als unveränderlich angesehen: Eine Variable, deren deklarierter Typ int ist, enthält immer eine entsprechende ganze Zahl,
auch nachdem der Variablen ein neuer Wert zugewiesen wurde. Auch beliebige
Information ist so handhabbar: Wenn der Typ eine Eigenschaft (wie ungleich
null) impliziert, dann muss bei jeder Zuweisung an die Variable statisch sichergestellt sein, dass der zugewiesene Wert diese Eigenschaft erfüllt. Das ist der
schwierige Teil. Es muss irgendwie festgelegt sein, welche Werte die gewünschten Eigenschaften besitzen. Beispielsweise sind neue Objekte genauso ungleich
null wie Variablen nach einer expliziten Prüfung – etwa x als Argument von
use in if(x!=null)use(x);. Unnötige dynamische Typprüfungen dieser Art
möchten wir vermeiden.
Manche sinnvolle Information ist nicht beliebig propagierbar. Beispielsweise
wäre es unsinnig, die Information „das ist die einzige Referenz auf das Objekt“
durch Zuweisung an mehrere Variablen zu propagieren, weil dies die Richtigkeit
der Information zerstören würde; danach gäbe es ja mehrere Referenzen. Um
mit solcher Information umzugehen, muss das Propagieren kontrolliert werden:
65
1 Grundlagen und Zielsetzungen
Die Information wird zwar weitergegeben, etwa vom Argument zum Parameter, aber nicht dupliziert; das Argument hätte die Information durch die Weitergabe an den Parameter verloren (wodurch das Argument nach dem Aufruf
nicht mehr sinnvoll verwendbar ist). Derartiges funktioniert gut, wenn Typen
nicht unveränderlich, sondern zustandsbehaftet sind; je nach Zustand des Typs
ist die Information vorhanden oder nicht. Solche Typen gibt es in experimentellen Sprachen. Diese Typen sind sehr mächtig, weil sie auch Eigenschaften
ausdrücken können, die für die Synchronisation benötigt werden.
Zusammenfassend gilt, dass mit statisch geprüften Typen heute sehr viel
möglich ist, was noch vor wenigen Jahren unmöglich schien. Umgekehrt kennen
wir aber auch unerwartete Einschränkungen, die es notwendig machen, manche
Aspekte erst zur Laufzeit zu prüfen.
66
2 Etablierte Denkmuster und
Werkzeugkisten
In diesem Kapitel nähern wir uns einigen etablierten Programmierparadigmen
auf herkömmliche Weise. Wir versuchen zu klären, welche Denkmuster dominieren, welche Sprachkonzepte in entscheidendem Ausmaß zum Einsatz kommen
und was hinter dem Erfolg entsprechender Programmierstile steckt.
2.1 Prozedurale Programmierung
Die prozedurale Programmierung ist das älteste Paradigma. Es wird auch heute
noch sehr häufig eingesetzt. Fast alle, die jemals mit Programmierung in Kontakt gekommen sind, sind zuerst mit prozeduraler Programmierung in Kontakt
gekommen. Entsprechende Denkmuster sind daher weit verbreitet und stehen
für viele als Synonym für „Programmieren“. Die Vertrautheit erschwert es, über
das Wesen der prozeduralen Programmierung nachzudenken, weil gefühlsmäßig
fast alles darunter fällt. Wir versuchen dennoch eine klare Abgrenzung.
2.1.1 Alles im Griff
Namensgebend in der prozeduralen Programmierung sind die Prozeduren, die
sich gegenseitig aufrufen und einen Programmfortschritt durch destruktive Zuweisungen, also Änderungen des Programmzustands über Seiteneffekte erzielen.
Programme sind daher nicht nur über eine Menge an Prozeduren beschrieben,
sondern auch durch (in der Regel globale) Daten bzw. Datenstrukturen, die es
erst ermöglichen, von einem Programmzustand zu sprechen. Obwohl Goto mit
der prozeduralen Programmierung kompatibel ist, werden heute vorwiegend
Prozeduren nach dem Prinzip der strukturierten Programmierung geschrieben.
Die Konzentration liegt auf der Programmierung im Feinen, Programmierung
im Groben beschränkt sich, wenn überhaupt thematisiert, auf Module zur getrennten Übersetzung, eventuell unterstützt durch Generizität zur Parametrisierung. Weder Objekte noch Prozeduren sind als Daten im engeren Sinn verwendbar, obwohl manchmal Prozeduren als Parameter erlaubt sind.
Aus dieser Zusammenfassung sind folgende typische Eigenschaften ableitbar:
• Durch Weglassen komplexer Sprachkonzepte (Prozeduren als Daten, viele
Formen von Modularisierungseinheiten und Parametrisierung, Ersetzbarkeit) ergibt sich eine überschaubare, anfängerfreundliche Menge sprachlicher Ausdrucksmittel, die sich vergleichsweise einfach und widerspruchsfrei miteinander kombinieren und jeden erdenklichen Programmablauf
67
2 Etablierte Denkmuster und Werkzeugkisten
darstellen lassen. Eine große Zahl an Programmieraufgaben scheint gut
lösbar zu sein, sobald Variablen, Parameterübergaben, Hintereinanderausführungen, Verzweigungen und Schleifen verstanden wurden. Wenige
zusätzliche Konzepte auf Seiten der Datenstrukturen wie Arrays, Records
und Zeiger vervollständigen das Bild.
• Der Programmablauf wird fast zur Gänze durch Kontrollstrukturen festgelegt, die Strukturierung der Daten durch wenige elementare Datenstrukturen wie Arrays, Records und Zeiger. Diese Eigenschaft gibt uns sehr viel
Kontrolle. Jedes Detail wird kontrolliert, nicht nur der Programmablauf,
sondern auch die genaue Strukturierung der Daten.
• Wegen der guten Kontrollmöglichkeiten ist die prozedurale Programmierung besonders gut für die hardwarenahe Programmierung geeignet. Auch
unübliche Hardware lässt sich meist vergleichsweise einfach ansprechen.
• Kontrolle wird nicht nur ermöglicht, sondern erzwungen. Wir müssen sogar in jenen Bereichen jedes feine Detail festlegen, wo es auf die Details
gar nicht ankommt. Diese Eigenschaft lässt auch Programme mittlerer
Größe und von mittlerem Schwierigkeitsgrad schon sehr komplex wirken,
weil der Überblick über die vielen Details rasch verloren geht.
• Abstraktion erfolgt nur über Prozeduren. Abstrakte Denkweisen spielen
deshalb eine eher untergeordnete Rolle. Obwohl nominale Abstraktion
sinnvoll einsetzbar ist, wird dennoch häufig nur λ-Abstraktion eingesetzt.
• Vollständige Kontrolle und niedrige Abstraktionsgrade sind die besten
Voraussetzungen für die Entwicklung von Programmen, die formal auf
Korrektheit überprüft werden können. Prozedurale Programmierung ist
gut mit formalen Korrektheitsbeweisen kombinierbar.
• Rekursive Prozeduraufrufe sind heute fast überall möglich, aber Rekursion wird meist nur dann angewandt, wenn äquivalente Schleifenkonstruktionen deutlich mehr Aufwand verursachen würden. Das wird meist mit
besserer Laufzeiteffizienz und besserer Kontrolle begründet, insbesondere
im Hinblick auf die Auslastung des Stacks für Prozeduraufrufe.
• Programme sind entweder nur dynamisch oder (bis auf Arrayzugriffe und
Ähnliches) nur statisch typisiert. Mischformen wie in der objektorientierten Programmierung kommen nicht zum Einsatz. Dynamisch typisierte
Programmierung herrscht vor allem in einem semiprofessionellen Bereich
vor, wo Programme eher einfach sind und die Anzahl sprachlicher Ausdrucksmittel auf ein absolutes Mindestmaß reduziert bleiben soll. Statische Typisierung herrscht dort vor, wo es auf Hardwarenähe, Laufzeiteffizienz und sehr gute Kontrollmöglichkeiten ankommt, die Einfachheit der
sprachlichen Ausdrucksmittel dagegen nicht so wichtig ist.
68
2.1 Prozedurale Programmierung
Zusammengefasst überzeugt die prozedurale Programmierung einerseits durch
einfach handhabbare Werkzeuge, andererseits durch gute Kontrollmöglichkeiten. Beides hat auch Nachteile: Die Notwendigkeit der Kontrolle und das Fehlen fortgeschrittener Werkzeuge für den Umgang mit Abstraktionen lässt den
Aufwand für das Erstellen und Warten umfangreicher, komplexer Software gewaltig ansteigen. Wer prozedural programmiert, möchte und muss jedes Detail
im Griff haben. Geht der Überblick verloren, entstehen unweigerlich Fehler, die
üblicherweise nicht durch sprachliche Mittel eingegrenzt oder verziehen werden.
Typische prozedurale Programme in Java verwenden statische Methoden
als Prozeduren, Klassen als Module, Klassenvariablen als globale Variablen
und Objekte von Klassen ohne Methoden als Records. Das sind im Großen
und Ganzen jene Programmiertechniken, von deren Verwendung in der JavaProgrammierung ständig abgeraten wird. Der Grund ist einfach: Zuerst lernen
wir meist die Grundkonzepte der prozeduralen Programmierung; sobald wir
sie beherrschen, sollen wir in Java weg von der prozeduralen hin zur objektorientierten Programmierung geführt werden. Im Kontext der prozeduralen
Programmierung verwenden wir jedoch genau diese in der objektorientierten
Programmierung verpönten Techniken:
import java.io.*;
import java.util.Scanner;
public class ProceduralCourse {
private final static int MAX = 1000;
private final static Student[] studs = new Student[MAX];
private final static int[] exercises = new int[MAX];
private final static int[] test1 = new int[MAX];
private final static int[] test2 = new int[MAX];
private static int used = 0;
private static void readStudents(InputStream stream) {
Scanner in = new Scanner(stream);
while (in.hasNextInt()) {
if (used >= MAX)
throw new IndexOutOfBoundsException("too much");
studs[used] = new Student();
studs[used].regNo = in.nextInt();
for (int i = 0; i < used; i++)
if (studs[i].regNo == studs[used].regNo)
throw new RuntimeException("double regNo");
studs[used].curr = in.nextInt();
studs[used].name = in.nextLine().strip();
used++;
}
}
69
2 Etablierte Denkmuster und Werkzeugkisten
private static void readP(int[] xs, InputStream stream) {
Scanner in = new Scanner(stream);
while (in.hasNextInt())
set(xs, in.nextInt(), in.nextInt());
}
private static void set(int[] xs, int r, int p) {
if (p <= 0 || p > 100)
throw new RuntimeException("inappropriate points");
for (int i = 0; i < used; i++)
if (studs[i].regNo == r) {
if (xs[i] != 0)
throw new RuntimeException("contradictory");
xs[i] = p;
return;
}
throw new RuntimeException("unknown regNo");
}
private static void printStatus() {
for (int i = 0; i < used; i++)
System.out.println(studs[i].regNo + ", "
+ studs[i].name + " (" + studs[i].curr + "): "
+ exercises[i] + "+"
+ test1[i] + "+" + test2[i] + " = "
+ (exercises[i] + test1[i] + test2[i]));
}
public static void main(String[] args) throws IOException {
if (args.length != 4)
throw new IllegalArgumentException("wrong params");
readStudents(new FileInputStream(args[0]));
readP(exercises, new FileInputStream(args[1]));
readP(test1, new FileInputStream(args[2]));
readP(test2, new FileInputStream(args[3]));
printStatus();
}
}
class Student {
public int regNo;
public int curr;
public String name;
}
Das Beispiel ist nicht rein prozedural, weil vordefinierte Klassen wie Scanner
und FileInputStream zum Einsatz kommen, die eine Verwendung im objektorientierten Stil verlangen. In Java ist es schwer, alle Objekte auszublenden.
Aber das Wesentliche kommt doch zum Ausdruck: Es wird viel mit globalen
70
2.1 Prozedurale Programmierung
Daten gearbeitet, obwohl das auch in der prozeduralen Programmierung eher
vermieden werden soll. Die Alternative wäre, Prozeduren mit vielen Parametern zu verwenden, was aber oft auf Kosten der Verständlichkeit geht. Klassen,
die als Module nur Klassenvariablen und statische Methoden enthalten, können durchaus über Data-Hiding Inhalte vor Zugriffen von außen schützen. Bei
Verwendung als Record wie für Student ist jedoch eine Objekterzeugung nötig
(das Objekt ist ein Record) und für Zugriffe von außen müssen Objektvariablen
public sein. Es wäre möglich, ein Objekt von Student als abstrakte Einheit zu
betrachten, darauf wird aber weitgehend verzichtet; die Variablen in Student
werden beinahe so verwendet, als ob sie unabhängig voneinander wären.
Das Beispiel ist klein gehalten, indem Plausibilitätsprüfungen eingelesener
Daten minimalistisch sind und Fehler nur über Ausnahmen gemeldet werden.
Genauere Prüfungen und informativere Fehlermeldungen würden die Komplexität des Programms ansteigen lassen. Die Einfachheit ist also relativ zu sehen.
2.1.2 Totgesagte leben länger
Wer die objektorientierte oder funktionale Programmierung gewohnt ist, empfindet die prozedurale Programmierung vielleicht als längst überholt. Dabei
wird übersehen, dass die prozedurale Programmierung zwar nicht überall, aber
in bestimmten Bereichen durchaus vorteilhaft ist und ganz bewusst eingesetzt
wird. Wir wollen einige dieser Bereiche kurz ansprechen.
Hardwarenahe Programmierung. Wenn es notwendig ist, spezielle Hardware anzusprechen, ist Kontrolle über den Programmablauf und die Position im
Speicher, an der Daten abgelegt werden, unerlässlich. Diese Forderungen lassen
sich in der prozeduralen Programmierung am besten erfüllen. Höhere Formen
der Abstraktion wären nur hinderlich. Allerdings kommen hauptsächlich Programmiersprachen zum Einsatz, die für die hardwarenahe Programmierung ausgelegt sind. Java wird kaum verwendet, weil es auf herkömmliche Weise nicht
möglich ist, bestimmte Speicheradressen anzusprechen. Es kommt also nicht
nur auf den Programmierstil an, sondern auch darauf, was in der Sprache einfach ausdrückbar ist. Wenn Laufzeiteffizienz von großer Bedeutung ist und die
Hardware ausreichend Speicher hat, sind stark typisierte Sprachen wie C ideal.
Auf sehr kleinen Mikrocontrollern ist Speicherplatz oft zu begrenzt für umfangreiche C-Programme. Dafür kommen auch interpretierte, dynamisch typisierte
oder sogar untypisierte Sprachen wie etwa Forth und Assembler zum Einsatz,
die sehr sparsam mit Speicher umgehen können. Details der Programmierstile
richten sich ganz nach den Einschränkungen der Hardware. Der Zusatzaufwand
durch das notwendige Ansprechen spezieller Hardware ist in der Regel so groß,
dass wirklich große Programme sowieso kaum erstellt werden und die Nachteile
der prozeduralen Programmierung daher kaum wirksam werden.
Echtzeitprogrammierung. Hierbei kommt es darauf an, dass maximale Antwortzeiten eingehalten werden. Ein prozeduraler Programmierstil erleichtert die
71
2 Etablierte Denkmuster und Werkzeugkisten
Kontrolle der Programmabläufe und Datenauslegungen. Obwohl Java ursprünglich nicht dafür entwickelt wurde, ist es durchaus möglich, Echtzeitprogramme in Java zu schreiben, falls es nicht zu problematisch ist, wenn Antwortzeiten manchmal nicht eingehalten werden (sogenannte weiche Echtzeitsysteme).
Trotzdem sind zur möglichst guten Erreichung der zeitlichen Ziele gut kontrollierte, also prozedurale Programmierstile nötig. Häufig reicht es aber, wenn
einschränkende prozedurale Stile nur in bestimmten Programmteilen verwendet
werden, während andere Teile z. B. objektorientierte Stile verwenden.
Ganz anders ist die Situation bei harten Echtzeitsystemen, wo die Einhaltung
der Antwortzeiten absolut notwendig ist, insbesondere bei sicherheitskritischen
Echtzeitsystemen. Dafür wird häufig eine spezielle Form der hardwarenahen
Programmierung eingesetzt, manchmal mit redundanter Auslegung der Hardund Software. Alle Abläufe und auch die Datenstrukturen müssen ganz im Hinblick auf die Abschätzbarkeit der Antwortzeiten ausgelegt sein. In der Praxis
bedeutet das zahlreiche Einschränkungen in den verwendbaren Kontrollstrukturen und Befehlen genauso wie in den einsetzbaren Datenstrukturen und Speicherauslegungen. Im sicherheitskritischen Bereich kann auch der Einsatz von
formalen Beweistechniken nötig sein. Daher kommen nur sehr eingeschränkte
prozedurale Programmierstile in Frage.
Scripting. Speziell unter Unix ist ein Shell-Skript ein kleines, prozedurales
Programm, das als Befehle Programmaufrufe verwendet. Daraus hat sich ein
eigenständiger Zweig der Programmierung entwickelt, in dem mittels meist interpretierter und dynamisch typisierter Sprachen Programme geschrieben werden, die bereits bestehende Programme aufrufen, miteinander kombinieren und
in einen neuen Kontext einbetten. Solche Programme werden in der Regel in
kurzer Zeit im Hinblick auf eine bestimmte Aufgabe entwickelt und sind nur für
den einmaligen Gebrauch (also nicht für eine langfristige Wartung) oder die wiederholte Anwendung in einem ganz bestimmten, sehr eng gefassten Kontext (also ohne Portabilität) ausgelegt, wobei Entwickler_innen und Anwender_innen
häufig die gleichen Personen sind (also wenig Aufwand betrieben wird, um hohe
Qualität zu erreichen). Ohne dafür ausgelegt zu sein, werden solche Programme
in der täglichen Praxis dennoch wiederholt zum Einsatz gebracht (weil sie schon
da sind) und immer wieder an sich ändernde Situationen und Kontexte angepasst (weil das, was da ist, doch nicht ganz passt). Meist bleiben die Programme
relativ klein. Die meisten derartigen Programme entsprechen ganz der prozeduralen Programmierung, obwohl entsprechende Sprachen oft Konzepte für die
objektorientierte Programmierung anbieten. Das ist verständlich, weil es unsinnig wäre, den Zusatzaufwand der objektorientierten Programmierung zu tragen,
wenn die Software nicht für die langfristige Wartung ausgelegt ist. Scripting
kommt heute in einem weiten Feld zum Einsatz. Typische Anwendungsbereiche sind die regelmäßige Überprüfung und Wartung von Rechnernetzwerken,
wiederholt durchzuführende Datenaufbereitungen, sowie die Bereitstellung von
Daten aus Datenbanken auf Webseiten. Oft ist Scripting die einzig mögliche, aus
der Not heraus geborene Vorgehensweise, wenn Software rasch benötigt wird
72
2.1 Prozedurale Programmierung
und nicht genug Zeit für die Entwicklung einer qualitativ hochwertigeren und
für die längerfristige Wartung ausgelegten Lösung zur Verfügung steht. In solchen Fällen wird häufig nicht nur die Wartbarkeit der Software vernachlässigt,
sondern auch die Robustheit und der Datenschutz.
Micro-Services. Immer wieder entstehen neue Softwarearchitekturen und entsprechende Programmiertechniken, vor allem in der verteilten Programmierung.
In letzter Zeit wurden Micro-Services populär. Ein Micro-Service ist ein auf einem bestimmten Rechner laufender Prozess (also eine Programmausführung),
der eine bestimmte, relativ kleine Aufgabe erledigt, etwa einen Bestellvorgang
durchführt. Eine klar vorgegebene Schnittstelle verbindet das Micro-Service mit
der Außenwelt. Komplexe Anwendungssoftware ist aus vielen Micro-Services
zusammengesetzt. Jedes Micro-Service für sich genommen ist klein genug, um
trotz hoher Qualität mit vertretbarem Zeitaufwand entwickelt (und bei Bedarf
durch ein anderes Micro-Service ersetzt) werden zu können. Die Komplexität eines Micro-Services bleibt dadurch überschaubar. Es wird immer wieder betont,
dass jedes Micro-Service mit unterschiedlicher Technologie in unterschiedlichen
Programmierstilen realisiert sein kann. Dennoch fördert diese Vorgehensweise
prozedurale Programmierstile, einfach deswegen, weil wegen der Überschaubarkeit keine komplexeren Mechanismen eingesetzt werden müssen.
Micro-Services stehen hier stellvertretend für viele Software-Architekturen
und Vorgehensweisen, deren Ziel es ist, hochkomplexe und umfangreiche Software so in Einzelteile herunterzubrechen, dass jeder Teil für sich überschaubar
bleibt und weitgehend unabhängig von anderen Teilen entwickelt werden kann.
Das ermöglicht prozedurale Programmierung für die Einzelteile. Die Weiterführung dieses Gedankens lässt erkennen, dass die prozedurale Programmierung
im Kern praktisch jedes Programmierparadigmas steckt.
Hobby-Programmierung und Anwendungsprogrammierung. Der Einstieg
in das Programmieren erfolgt fast immer über die prozedurale Programmierung. In der Regel steht am Anfang statische Typisierung, wenn längerfristig eine professionelle Form der Softwareentwicklung angestrebt wird. Dagegen steht
am Anfang dynamische Typisierung wenn erwartet wird, dass die Programmierung im Hobby-Bereich oder in der Anwendungsprogrammierung bleibt. Diese
Unterscheidung ist historisch gewachsen und entstammt der Erfahrung, dass
dynamisch typisierte Programmierung mit einfachen Sprachen rasche Erfolgserlebnisse beschert, es aber erschwert, sich später komplexe Programmiertechniken anzueignen. Heute gibt es eine riesige, ständig wachsende Zahl an Menschen
unterschiedlicher beruflicher oder fachlicher Ausrichtungen, die gelernt haben,
in einem eher hobbymäßigen Bereich zu programmieren. Viele davon betreiben
die Programmierung gerne und auf durchaus hohem Niveau. Sie sind in der Lage
und willens, ihre Fähigkeiten auch in einem professionellen Bereich einzusetzen.
Aber sie bleiben häufig bei der prozeduralen Programmierung (oft trotz Verwendung einer objektorientierten Sprache). Solche Menschen sind für die Softwareentwicklung wertvoll, weil sie neben ihren Programmierfähigkeiten tiefgehende
73
2 Etablierte Denkmuster und Werkzeugkisten
Fähigkeiten in anderen Bereichen mitbringen, die als Domain-Wissen auf Anwendungsgebieten der zu erstellenden Software gebraucht werden. Es wird von
Anwendungsprogrammierung gesprochen, wenn Domain-Wissen auf dem Gebiet
der Anwendung einen bedeutenden Aspekt in der Softwareerstellung ausmacht.
Allerdings steht diese Form der Programmierung häufig vor dem Problem, dass
Wege gefunden werden müssen, wie auch komplexe Software auf Basis prozeduraler Programmierung erstellt werden kann. Es braucht Expert_innen, die entsprechende Softwarearchitekturen entwerfen können. Dafür reicht Wissen über
prozedurale Programmierung nicht aus, sondern es ist nötig, die Ideen hinter
unterschiedlichen Paradigmen zu verstehen und so miteinander zu kombinieren,
dass sich ein Gefüge ergibt, in dem alle beteiligten Menschen das Maximum aus
ihren jeweiligen Fähigkeiten herausholen können. Ein pragmatischer Umgang
mit Programmierparadigmen ist dabei ein entscheidender Faktor.
2.2 Objektorientierte Programmierung
Die objektorientierte Programmierung zählt zu den erfolgreichsten Paradigmen
der Informatik. Der Inhalt dieses Abschnitts sollte großteils schon aus anderen
Lehrveranstaltungen bekannt sein. Durch die Wiederholung soll vor allem die
verwendete Terminologie wieder in Erinnerung gerufen werden.
2.2.1 Modularisierungseinheiten und Abstraktion
Die objektorientierte Programmierung will Softwareentwicklungsprozesse unterstützen, die auf inkrementeller Verfeinerung aufbauen. Bei diesen Prozessen
spielt die leichte Wartbarkeit eine große Rolle. Im Wesentlichen geben uns objektorientierte Sprachen Werkzeuge in die Hand, die wir zum Schreiben leicht
wiederverwendbarer und änderbarer Software brauchen.
Objekt. Ein Objekt ist eine grundlegende Modularisierungseinheit – siehe Abschnitt 1.3.1. Zur Laufzeit besteht die Software aus einer Menge von Objekten,
die einander teilweise kennen und untereinander Nachrichten (Messages) austauschen. Das Schicken einer Nachricht entspricht dem Aufruf einer Methode.
Folgende Abbildung zeigt ein Objekt:
✬
✫
✩
✪
Objekt: einStack
private Variablen:
elems: "a" "b" "c" null null
size: 3
öffentlich sichtbare Methoden:
push: Implementierung der Methode
pop: Implementierung der Methode
74
2.2 Objektorientierte Programmierung
Dieses Objekt mit der Funktionalität eines Stacks fügt zwei Variablen und zwei
Methoden zu einer Einheit zusammen und grenzt sie vom Rest des Systems
weitgehend ab. Eine Variable enthält ein Array mit dem Stackinhalt, die andere
die Anzahl der Stackelemente. Das Array kann höchstens fünf Stackelemente
halten. Zur Zeit sind drei Einträge vorhanden.
Jedes Objekt besitzt folgende Eigenschaften [35]:
Identität (Identity): Das Objekt ist durch seine unveränderliche Identität eindeutig bestimmt. Über seine Identität kann ihm eine Nachricht geschickt
werden. Vereinfacht stellen wir uns die Identität als die Speicheradresse
des Objekts vor. Dies ist eine Vereinfachung, da die Identität erhalten
bleibt, wenn sich die Adresse ändert, etwa bei einer Speicherbereinigung
oder beim Auslagern in eine Datenbank. Jedenfalls sind gleichzeitig durch
zwei Namen bezeichnete Objekte identisch, wenn sie am selben Speicherplatz liegen, es sich also um ein Objekt mit zwei Namen handelt.
Zustand (State): Die Werte der Variablen im Objekt bilden den Zustand. Er
ist meist änderbar. In obigem Beispiel ändert sich der Zustand durch
Zuweisung neuer Werte an elems und size.
Zwei Objekte sind gleich, wenn sie den gleichen Zustand und das gleiche Verhalten haben. Identische Objekte sind trivialerweise gleich. Zwei
Objekte können auch gleich sein, ohne identisch zu sein; dann sind sie
Kopien voneinander. Zustände gleicher Objekte können sich unabhängig
voneinander ändern; die Gleichheit geht dadurch verloren. Identität geht
durch Zustandsänderungen nicht verloren. In der Praxis verwenden wir
viele Varianten des Begriffs Gleichheit. Oft werden nur manche, nicht alle
Variableninhalte in den Vergleich einbezogen. Gelegentlich betrachten wir
nur identische Objekte als gleich.
Verhalten (Behavior): Das Verhalten des Objekts beschreibt, wie sich das Objekt beim Empfang einer Nachricht verhält. In obigem Beispiel wird die
Methode push beim Empfang der Nachricht push("d") das Argument
"d" in den Stack einfügen (falls es einen freien Platz gibt), und pop wird
beim Empfang von pop() ein Element entfernen (falls eines vorhanden
ist) und an den Absender zurückgeben.
Schnittstelle und Implementierung. Unter der Implementierung einer Methode verstehen wir den Programmcode, der festlegt, was bei Ausführung der
Methode zu tun ist. Die Implementierungen aller Methoden eines Objekts, die
Deklarationen der Objektvariablen und die Konstruktoren bilden zusammen die
Implementierung des Objekts. Die Implementierung beschreibt das Verhalten
bis ins kleinste Detail. Für die Programmausführung ist die genaue Beschreibung essentiell; sonst wüsste der Computer nicht, was zu tun ist.
Für die Wartung ist es günstiger, wenn die Beschreibung des Verhaltens nicht
jedes Detail widerspiegelt. Statt der Implementierung haben wir eine abstrakte
Vorstellung im Kopf, die Details offen lässt. Beispielsweise gibt
75
2 Etablierte Denkmuster und Werkzeugkisten
push fügt beim Empfang der Nachricht push("d") das Argument
"d" in den Stack ein (falls es einen freien Platz gibt)
eine abstrakte Vorstellung davon wider, was tatsächlich passiert. Es bleibt offen,
wie und wo "d" eingefügt wird und wann Platz frei ist. Menschen können mit
solchen (nominalen) Abstraktionen einfacher umgehen als mit Implementierungen. Bei Computern ist es genau umgekehrt. Daher wollen wir Beschreibungen
des Objektverhaltens so weit wie möglich abstrakt halten und erst dann zur
Implementierung übergehen, wenn dies für den Computer notwendig ist.
Wir fordern eine weitere Objekteigenschaft, die den Abstraktionsgrad des
Verhaltens nach Bedarf steuern lässt:
Schnittstelle (Interface): Eine Schnittstelle eines Objekts beschreibt das Verhalten des Objekts in einem Abstraktionsgrad, der für Zugriffe von außen notwendig ist. Ein Objekt kann mehrere Schnittstellen haben, die
das Objekt aus den Sichtweisen unterschiedlicher Verwendungen beschreiben. Manchmal entsprechen Schnittstellen nur Signaturen (strukturelle
Abstraktion), meist ergeben sich durch Verwendung nominaler Typen
begleitet von Kommentaren nominale Abstraktionen – siehe Abschnitte
1.4.2 und 1.6.2. Über Zusicherungen kann das Verhalten beliebig genau
beschreiben werden. Ein Objekt implementiert seine Schnittstellen; das
heißt, die Implementierung des Objekts legt das in den Schnittstellen unvollständig beschriebene Verhalten im Detail fest. Jede Schnittstelle kann
das Verhalten beliebig vieler Objekte beschreiben. In Sprachen mit statischer Typprüfung entsprechen Schnittstellen den Typen des Objekts.
Beim Zugriff auf ein Objekt müssen wir nur eine Schnittstelle kennen, nicht
den Objektinhalt. Nur das, was in den Schnittstellen beschrieben ist, ist von
außen sichtbar. Schnittstellen trennen also die Innen- von der Außenansicht und
sorgen für Data-Hiding. Kapselung zusammen mit Data-Hiding ergibt Datenabstraktion: Die Daten sind nicht direkt sichtbar und manipulierbar, sondern
abstrakt. Im Beispiel sehen wir die Daten des Objekts nicht als Array von Elementen zusammen mit der Anzahl der Einträge, sondern als abstrakten Stack,
der über Methoden zugreifbar und manipulierbar ist. Die Abstraktion bleibt
unverändert, wenn wir das Array gegen eine andere Datenstruktur, etwa eine
Liste, austauschen. Datenabstraktionen helfen bei der Wartung: Details von
Objekten sind änderbar, ohne deren Außenansichten und damit deren Verwendungen zu beeinflussen. Außerdem ist die abstrakte Außenansicht viel einfacher
verständlich als die Implementierung mit all ihren Details.
Klasse. Viele, aber nicht alle objektorientierten Sprachen beinhalten ein Klassenkonzept: Jedes Objekt gehört zu der Klasse, in der das Objekt implementiert
ist. Die Klasse beschreibt auch Konstruktoren zur Initialisierung neuer Objekte.
Alle Objekte, die zur Klasse gehören, wurden durch Konstruktoren dieser Klasse initialisiert. Wir nennen diese Objekte auch Instanzen der Klasse. Genauer
gesagt sind die Objekte Instanzen der durch die Klasse beschriebenen Schnittstellen bzw. Typen. Die Klasse selbst ist die spezifischste Schnittstelle mit der
genauesten Verhaltensbeschreibung. Ein Stack könnte so implementiert sein:
76
2.2 Objektorientierte Programmierung
public class Stack {
private String[] elems;
private int size = 0;
public Stack(int sz) { // new stack of size sz
elems = new String[sz];
}
// push pushes elem onto stack if not yet full
public void push(String elem) {
if (size < elems.length) elems[size++] = elem;
}
// pop returns element taken from stack, null if empty
public String pop() {
if (size > 0) return elems[--size]; else return null;
}
}
Alle Objekte einer Klasse haben dieselbe Implementierung und auch dieselben
Schnittstellen. Aber unterschiedliche Objekte haben ihre eigenen Identitäten
und Objektvariablen, obwohl diese Variablen gleiche Namen und Typen tragen.
Auch die Zustände können sich unterscheiden.
Die Kommentare sind Zusicherungen, also mehr als nur Erläuterungen zum
besseren Verständnis. Sie beschreiben das abstrakte Verhalten, soweit dies für
Aufrufer relevant ist. Wer einen Aufruf in das Programm einfügt, soll sich auf
die Zusicherungen in Form von Kommentaren verlassen und nicht die Implementierung betrachten müssen. Details der Implementierung können geändert
werden, solange die unveränderten Kommentare auf die geänderte Implementierung zutreffen.
Anmerkung: Häufig wird gesagt, ein Objekt gehöre zu mehreren Klassen,
der spezifischsten und deren Oberklassen. Im Skriptum verstehen wir unter
der „Klasse des Objekts“ immer die spezifischste Schnittstelle und sprechen
allgemein von „Schnittstelle“, wenn wir eine beliebige meinen.
2.2.2 Polymorphismus und Ersetzbarkeit
Im Zusammenhang mit Programmiersprachen sprechen wir von Polymorphismus, wenn eine Variable oder Methode gleichzeitig mehrere Typen haben kann.
An einen formalen Parameter einer polymorphen Methode können Argumente von mehr als nur einem Typ übergeben werden. Objektorientierte Sprachen
sind polymorph. Im Gegensatz dazu sind statisch typisierte Sprachen wie C und
Pascal monomorph: Jede Variable oder Funktion hat einen eindeutigen Typ.
Wir unterscheiden verschiedene Arten des Polymorphismus [8]:
Polymorphismus



universeller
Polymorphismus



Generizität
Untertypbeziehungen
Ad-hocPolymorphismus



Überladen
Typumwandlung
77
2 Etablierte Denkmuster und Werkzeugkisten
Nur beim universellen Polymorphismus haben die Typen, die zueinander in
Beziehung stehen, eine gleichförmige Struktur. Generizität erreicht die Gleichförmigkeit durch gemeinsamen Code, der über Typparameter mehrere Typen
haben kann. Bei Verwendung von Untertypen wird Gleichförmigkeit durch gemeinsame Schnittstellen für unterschiedliche Objekte erzielt. Überladene Methoden müssen, abgesehen vom gemeinsamen Namen, keinerlei Ähnlichkeiten
in ihrer Struktur besitzen und sind daher ad-hoc-polymorph. Auch bei Typumwandlungen (Casts), insbesondere auf elementaren Typen wie von int auf
double, gibt es keine gemeinsame Struktur; int und double sind intern ja
ganz unterschiedlich dargestellt. Typumwandlungen auf Referenztypen fallen
dagegen eher in die Kategorie der Untertypbeziehungen.
In der objektorientierten Programmierung sind Untertypen von überragender
Bedeutung, die anderen Arten des Polymorphismus existieren eher nebenbei.
Daher wird alles, was mit Untertypen zu tun hat, oft auch objektorientierter
Polymorphismus oder kurz nur Polymorphismus genannt.
In einer objektorientierten Sprache hat eine Variable (in Java jede ReferenzVariable beliebiger Art, auch ein Parameter) gleichzeitig folgende Typen:
Deklarierter Typ: Das ist der Typ, mit dem die Variable deklariert wurde.
Dieser existiert natürlich nur in Sprachen mit expliziter Typdeklaration.
Dynamischer Typ: Das ist der spezifischste Typ, den der in der Variablen gespeicherte Wert hat. Dynamische Typen sind oft spezifischer als deklarierte und können sich mit jeder Zuweisung ändern. Der Compiler kennt
dynamische Typen im Allgemeinen nicht. Dynamische Typen werden unter anderem für dynamisches Binden eingesetzt.
Statischer Typ: Dieser Typ wird vom Compiler (statisch) ermittelt und liegt
irgendwo zwischen deklariertem und dynamischem Typ. In vielen Fällen ordnet der Compiler derselben Variable an unterschiedlichen Stellen
verschiedene statische Typen zu. Solche Typen werden beispielsweise für
Programmoptimierungen verwendet. Es hängt von der Qualität des Compilers ab, wie spezifisch der statische Typ ist. In Sprachdefinitionen kommen statische Typen (wenn nicht gleich den deklarierten) daher nicht vor.
Eine Konsequenz aus der Verwendung von Untertypen ist dynamisches Binden: Wenn eine Nachricht an ein Objekt geschickt wird, kennt der Compiler
nur den statischen und deklarierten Typ des Empfängers der Nachricht. Die
Methode soll aber entsprechend des dynamischen Typs ausgeführt werden. Daher kann die auszuführende Methode erst zur Laufzeit bestimmt werden. In
manchen Situationen kennt der Compiler die auszuführende Methode, etwa
wenn die Methode als static, final oder private deklariert ist, und kann
statisches Binden verwenden, sodass zur Laufzeit keine Bestimmung der passenden Methode nötig ist. Durch mögliche Optimierungen ist statisches Binden
etwas effizienter als dynamisches. Dennoch ist das dynamische Binden einer der
wichtigsten Eckpfeiler der objektorientierten Programmierung. Ein sinnvoller
Umgang mit dynamischem Binden kann die Wartbarkeit erheblich verbessern,
ohne dass dies zu Effizienzverlusten führen muss.
78
2.2 Objektorientierte Programmierung
Vererbung. Die Vererbung (Inheritance) ermöglicht es, neue Klassen aus bereits existierenden Klassen abzuleiten. Dabei werden nur die Unterschiede zwischen der abgeleiteten Klasse (Derived-Class, Unterklasse, Subclass) und der
Basisklasse (Base-Class, Oberklasse, Superclass), von der abgeleitet wird, angegeben. Vererbung erspart uns etwas Schreibaufwand. Außerdem werden einige Programmänderungen vereinfacht, da sich Änderungen von Klassen auf alle
davon abgeleiteten Klassen auswirken. Diese Vorteile werden jedoch vielfach
deutlich überschätzt.
In populären objektorientierten Programmiersprachen können bei der Vererbung Unterklassen im Vergleich zu Oberklassen nicht beliebig geändert werden.
Eigentlich gibt es nur zwei Änderungsmöglichkeiten:
Erweiterung: Die Unterklasse erweitert die Oberklasse um neue Variablen, Methoden und Konstruktoren.
Überschreiben: Methoden der Oberklasse werden in der Unterklasse durch
neue Methoden überschrieben, die jene aus der Oberklasse ersetzen.
Diese beiden Änderungsmöglichkeiten sind beliebig kombinierbar.
In Sprachen wie Java besteht ein enger Zusammenhang zwischen Vererbung
und Untertypen: Ein Objekt einer Unterklasse kann, soweit es vom Compiler
prüfbar ist, überall verwendet werden, wo ein Objekt einer Oberklasse erwartet
wird. Änderungsmöglichkeiten bei der Vererbung sind eingeschränkt, um die
Ersetzbarkeit von Objekten der Oberklasse durch Objekte der Unterklasse zu
ermöglichen. Es besteht eine direkte Beziehung zwischen Klassen und Typen:
Die Klasse eines Objekts ist gleichzeitig der spezifischste Typ bzw. die spezifischste Schnittstelle des Objekts. Dadurch entspricht eine Vererbungsbeziehung
begleitet von Verhaltensbeschreibungen einer Untertypbeziehung. Verhaltensbeschreibungen der Methoden in Unterklassen müssen denen in Oberklassen
entsprechen, können aber genauer sein. Das ist eine Voraussetzung für Untertypbeziehungen, obwohl der Compiler Kommentare nicht überprüfen kann.
Java unterstützt nur Einfachvererbung (Single-Inheritance) zwischen Klassen im engeren Sinn. Das heißt, jede Unterklasse wird von genau einer anderen
Klasse abgeleitet. Die Verallgemeinerung dazu ist Mehrfachvererbung (MultipleInheritance), wobei jede Klasse mehrere Oberklassen haben kann. Mehrfachvererbung gibt es zum Beispiel in C++. Neben Klassen gibt es in Java Interfaces, auf denen es auch Mehrfachvererbung gibt. Interfaces sind eingeschränkte
Klassen (im weiteren Sinn), die keine Objektvariablen enthalten und von denen
keine Instanzen erzeugt werden können. Wir verwenden in diesem Skriptum
für das Sprachkonstrukt in Java den englischen Begriff, während wir mit dem
gleichbedeutenden deutschen Begriff Schnittstellen im Allgemeinen bezeichnen.
2.2.3 Typische Vorgehensweisen
Faktorisierung. Zusammengehörende Programmteile (wie Variablen und Methoden) sollen zu Modularisierungseinheiten zusammengefasst werden. Wir sprechen eher von Faktorisierung (Factoring) statt Modularisierung, weil sich nicht
79
2 Etablierte Denkmuster und Werkzeugkisten
nur Module, sondern vorwiegend Klassen und zur Laufzeit den Klassen entsprechende Objekte ergeben. Gute Faktorisierung bewirkt, dass Programmänderungen lokal an jeweils nur einer Stelle durchzuführen sind. Bei schlechter
Faktorisierung müssen für eine einzige Änderung viele, einander ähnliche Programmstellen gefunden und geändert werden. Gute Faktorisierung verbessert
auch die Lesbarkeit des Programms, weil bestimmte Programmteile an nur einer
einzigen Stelle zu finden sind, nicht verteilt auf viele Programmstellen.
Objekte dienen durch Kapselung zusammengehöriger Programmteile der Faktorisierung. Durch Zusammenfügen von Daten mit Methoden ergeben sich mehr
Freiheiten zur Faktorisierung als in anderen Paradigmen, bei denen Daten und
Funktionen voneinander getrennt sind. Gute Faktorisierung kann die Wartbarkeit verbessern. Die objektorientierte Programmierung bietet viele Möglichkeiten zur Faktorisierung. Bei Weitem nicht jede Faktorisierung ist gut. Es ist
unsere Aufgabe, die Qualität von Faktorisierungen zu beurteilen.
Die Lesbarkeit eines Programms wird erhöht, wenn die Zerlegung in Objekte
so erfolgt, wie es der Erfahrung in der realen Welt entspricht. Software-Objekte
sollen die reale Welt simulieren, soweit dies zur Erfüllung der Aufgaben sinnvoll
erscheint. Namen für Software-Objekte sollen üblichen Bezeichnungen realer
Objekte entsprechen. Wir dürfen die Simulation aber nicht zu weit treiben. Vor
allem sollen keine Eigenschaften der realen Welt simuliert werden, die für die
entwickelte Software bedeutungslos sind. Die Einfachheit ist wichtiger.
Entwicklungsprozesse. Die Qualität eines Programms wird von vielen Faktoren bestimmt. Einflussgrößen sind zu Beginn der Entwicklung unbekannt oder
nicht kontrollierbar. Erfahrungen gibt es erst, wenn das Programm existiert.
Traditionell wird Software nach dem Wasserfallmodell entwickelt, in dem die
üblichen Entwicklungsschritte (Analyse, Design, Implementierung, Verifikation) einer nach dem anderen ausgeführt werden. Solche Entwicklungsprozesse
eignen sich gut für kleine Projekte mit klaren Anforderungen. Aber das Risiko
ist groß, dass etwas Unbrauchbares herauskommt, weil es erst ganz am Ende
Feedback gibt. Daher werden für größere Projekte eher zyklische Prozesse verwendet, die die einzelnen Schritte ständig auf jeweils einem anderen Teil der
gesamten Aufgabe wiederholen. So ergibt sich schon recht früh Feedback. Aber
der Fortschritt eines Projekts ist nur schwer planbar. Es kann leicht passieren,
dass sich die Programmqualität zwar ständig verbessert, das Programm aber
nie zum Einsatz gelangt, da die Mittel vorher schon erschöpft sind. Zyklische
Prozesse verkraften Anforderungsänderungen besser, aber Zeit und Kosten sind
schwerer planbar, auch wenn es heute Berechnungsmodelle dafür gibt.
Erfahrung. In den vergangenen Jahrzehnten hat sich in der Programmierung
ein umfangreicher Erfahrungsschatz darüber entwickelt, mit welchen Problemen
in Zukunft gerechnet werden muss, wenn eine Aufgabe auf eine bestimmte Art
gelöst wird. Mit dem dafür nötigen Wissen setzen wir Erfahrungen gezielt ein.
Eine Garantie für den Erfolg eines Projekts gibt es natürlich trotzdem nicht,
aber die Wahrscheinlichkeit für einen Erfolg steigt.
80
2.2 Objektorientierte Programmierung
In der objektorientierten Programmierung ist der gezielte Einsatz von Erfahrungen essenziell. Es gibt viele unterschiedliche Möglichkeiten zur Lösung
von Aufgaben, jede mit anderen Eigenschaften. Nur mit Erfahrung wird jene
Möglichkeit gewählt, deren Eigenschaften später am ehesten hilfreich sind. Mit
wenig Erfahrung wird nur die Möglichkeit gewählt, die zufällig zuerst in den
Sinn kommt und damit auf einen wichtigen Vorteil verzichtet.
Zusammenhalt und Kopplung. Beim Entwickeln von Software müssen wir in
jeder Phase wissen, wie wir vorgehen, um ein möglichst hochwertiges Ergebnis
zu produzieren. Vor allem eine gute Faktorisierung ist entscheidend, aber leider
erst gegen Ende der Entwicklung beurteilbar. Es gibt Faustregeln, die uns beim
Finden guter Faktorisierungen unterstützen. Wir wollen hier einige Faustregeln
betrachten, die in vielen Fällen einen Weg zu guter Faktorisierung weisen [5]:
Verantwortlichkeiten (Responsibilities): Die Verantwortlichkeiten einer Klasse können wir durch drei w-Ausdrücke beschreiben:
• „was ich weiß“ – Beschreibung des Zustands der Objekte
• „was ich mache“ – Verhalten der Objekte
• „wen ich kenne“ – sichtbare Objekte, Klassen, etc.
Das Ich steht dabei jeweils für die Klasse. Wenn etwas geändert werden
soll, das in den Verantwortlichkeiten einer Klasse liegt, dann sind dafür
die Personen zuständig, die die Klasse entwickelt haben.
Klassen-Zusammenhalt (Class-Cohesion): Darunter verstehen wir den Grad
der Beziehungen zwischen den Verantwortlichkeiten der Klasse. Dieser
Grad ist zwar nicht einfach messbar, aber intuitiv einfach fassbar. Der Zusammenhalt ist hoch, wenn alle Variablen und Methoden eng zusammenarbeiten und durch deren Namen und den Klassennamen gut beschrieben
sind. Einer Klasse mit hohem Zusammenhalt fehlt Wichtiges, wenn Variablen oder Methoden entfernt werden. Der Zusammenhalt wird niedriger,
wenn Klasse, Variablen oder Methoden sinnändernd umbenannt werden.
Objekt-Kopplung (Object-Coupling): Das ist die Abhängigkeit der Objekte
voneinander. Die Objekt-Kopplung ist stark, wenn
• viele Methoden und Variablen nach außen sichtbar sind,
• im laufenden System Nachrichten und Variablenzugriffe zwischen unterschiedlichen Objekten häufig auftreten
• und die Anzahl der Parameter dieser Methoden groß ist.
Der Klassen-Zusammenhalt soll hoch sein. Ein hoher Klassen-Zusammenhalt
deutet auf eine gute Zerlegung des Programms in einzelne Klassen beziehungsweise Objekte hin – gute Faktorisierung. Bei guter Faktorisierung ist die Wahrscheinlichkeit kleiner, dass bei Programmänderungen auch die Zerlegung in
Klassen und Objekte geändert werden muss (Refaktorisierung, Refactoring).
81
2 Etablierte Denkmuster und Werkzeugkisten
Natürlich ist es bei hohem Zusammenhalt schwierig, bei Refaktorisierungen
den Zusammenhalt beizubehalten oder noch weiter zu erhöhen.
Die Objekt-Kopplung soll schwach sein. Schwache Kopplung deutet darauf
hin, dass Objekte weitgehend unabhängig voneinander sind. Dadurch beeinflussen Programmänderungen wahrscheinlich weniger Objekte unnötig. Beeinflussungen durch unvermeidbare Abhängigkeiten sind unumgänglich.
Klassen-Zusammenhalt und Objekt-Kopplung stehen in enger Beziehung zueinander. Wenn der Klassen-Zusammenhalt hoch ist, dann ist oft, aber nicht
immer, die Objekt-Kopplung schwach und umgekehrt. Das ergibt sich nicht automatisch, sondern ist das Ergebnis eines bewussten Umgangs mit diesen Kriterien. Da Menschen auch dann gut im Assoziieren zusammengehöriger Dinge
sind, wenn sie Details noch gar nicht kennen, ist es relativ leicht, bereits in einer
frühen Entwicklungsphase zu erkennen, wie ein hoher Klassen-Zusammenhalt
und eine schwache Objekt-Kopplung erreichbar sein könnte. Die Simulation der
realen Welt hilft dabei vor allem zu Beginn der Softwareentwicklung.
Wenn eine Entscheidung zwischen Alternativen getroffen werden muss, können Klassen-Zusammenhalt und Objekt-Kopplung Beiträge zur Entscheidungsfindung liefern. Sowohl Klassen-Zusammenhalt als auch Objekt-Kopplung lassen sich im direkten Vergleich einigermaßen sicher prognostizieren. In manchen
Fällen können jedoch andere Faktoren ausschlaggebend sein.
Auch mit viel Erfahrung können wir kaum auf Anhieb einen optimalen Entwurf liefern. Normalerweise muss die Faktorisierung einige Male geändert werden; wir sprechen von Refaktorisierung. Sie ändert die Struktur eines Programms, lässt aber dessen Funktionalität unverändert. Es wird dabei also nichts
hinzugefügt oder weggelassen, und es werden auch keine inhaltlichen Änderungen vorgenommen. Solche Refaktorisierungen sind in einer frühen Projektphase
ohne größere Probleme und Kosten möglich und werden durch eine Reihe von
Werkzeugen unterstützt. Einige wenige Refaktorisierungen führen meist rasch
zu einer stabilen Zerlegung der betroffenen Programmteile, die später kaum
noch refaktorisiert werden müssen. Wir müssen nicht von Anfang an einen optimalen Entwurf haben, sondern nur alle nötigen Refaktorisierungen durchführen, bevor sich Probleme über das ganze Programm ausbreiten. Natürlich darf
nicht so häufig refaktorisiert werden, dass bei der inhaltlichen Programmentwicklung kein Fortschritt mehr erkennbar ist. Ein vernünftiges Maß rechtzeitiger
Refaktorisierungen führt häufig zu gut faktorisierten Programmen.
2.2.4 Worum es geht
Im Gegensatz zu jener der prozeduralen Programmierung ist die Werkzeugkiste der objektorientierten Programmierung gut gefüllt, wie die Länge obiger
Beschreibungen zeigt. Erst im komplexen Zusammenspiel entfalten die Werkzeuge ihre Wirkung. Diese Wirkung haben wir in Abschnitt 1.5.2 angerissen:
Wir arbeiten mit Abstraktionen auf hohem Niveau, die uns ein Verständnis des
Programms und seiner Teile ermöglichen, ohne alle Details des Programmablaufs verstehen zu müssen. Das Ziel ist fast konträr zu dem der prozeduralen
Programmierung. Ebenso verhält es sich mit dem Haupteinsatzgebiet: Die ob82
2.2 Objektorientierte Programmierung
jektorientierte Programmierung eignet sich für die Entwicklung und Wartung
langlebiger, komplexer Systeme durch Profis. Es wäre unsinnig, überschaubar
kleine oder für den einmaligen Gebrauch bestimmte Programme objektorientiert zu entwickeln, weil dies den Aufwand ohne Gegenleistung in die Höhe treiben würde. Wir müssen leider auch davon ausgehen, dass in der Programmierung noch wenig erfahrene Leute von der objektorientierten Programmierung
anfangs überfordert sind, weil das Zusammenspiel der Werkzeuge komplex ist.
Die ernsthaft betriebene objektorientierte Programmierung ist sehr fordernd,
setzt viel Erfahrung voraus und ist aufwändiger als andere Paradigmen. Andererseits bietet uns die objektorientierte Programmierung auch dann noch vertretbare Erfolgsaussichten, wenn Lösungsversuche in allen anderen Paradigmen
aufgrund der Größe und Komplexität der Aufgabe fehlzuschlagen drohen.
Der Schlüssel zur objektorientierten Programmierung liegt in der Abstraktion
(siehe Abschnitt 1.4), vor allem der Abstraktion auf der Ebene der Programmierung im Groben. Es geht um Modularisierungseinheiten, vorwiegend Objekte, die als abstrakte Datentypen gesehen werden. Wir stellen uns Objekte als
Software-Abbilder von (nicht nur materiellen) Dingen aus der realen Welt vor
und übertragen uns bekannte Eigenschaften dieser Dinge auf Software-Objekte.
Natürlich ist jedes Abbild nur eine unvollständige Simulation des realen Dings
und übernimmt nicht alle Eigenschaften. Hier kommt die Notwendigkeit von
Erfahrung ins Spiel: Es muss ein Verständnis dafür entwickelt werden, welche
Eigenschaften in der Software abgebildet werden und welche nicht. Außerdem
müssen die erwartbaren Eigenschaften auf eine Weise beschrieben werden, sodass alle an der Entwicklung beteiligten Personen diese auf annähernd gleiche
Weise verstehen. Es braucht eine gemeinsame Sprache. Es müssen auch Details auf gleiche Weise verstanden werden, auf die es in natürlichen Gesprächen
in der Regel nicht ankommt. Beispielsweise verwenden wir Zusicherungen entsprechend Design-by-Contract, um Eigenschaften in einer mehr oder weniger
standardisierten Art auszudrücken. Im Wesentlichen entwickelt sich die gemeinsame Sprache aber erst in der Zusammenarbeit in einem Team und unterliegt
ständigen Weiterentwicklungen, weil auch Design-by-Contract große Interpretationsspielräume lässt. Darüber hinaus ergeben sich Abstraktionen auf immer
höherem Niveau: Nicht nur ein Ding aus der realen Welt wird in Software simuliert, sondern auch Software-Objekte selbst oder nur in unserer Vorstellungswelt
existierende Objekte werden für die Programmierung als reale Dinge angesehen
und durch Objekte auf einer höheren Ebene simuliert. Mit der entsprechenden
Erfahrung wirkt der Umgang mit derart abstrakten Objekten ganz einfach und
natürlich, aber für Außenstehende ist es schwer, die für ein Verständnis nötigen
Beziehungen zwischen den Begriffen zu entwickeln.
In der objektorientierten Programmierung leben wir in einer Phantasiewelt.
Allerdings gibt es reale Bezüge zur echten Welt. Sie kommen nicht nur von den
simulierten Dingen, sondern auch aus Einschränkungen, die uns die Werkzeuge auferlegen. Beispielsweise müssen wir ständig auf Ersetzbarkeit achten, ein
Konzept mit klaren, aus der Theorie zweifelsfrei ableitbaren Einschränkungen.
Die Theorie (Ersetzbarkeitsprinzip) müssen wir ebenso wie die Einschränkungen
kennen und die Abstraktionen konform dazu entwickeln. Nachdem die Werkzeu83
2 Etablierte Denkmuster und Werkzeugkisten
ge kein Verständnis für unsere nominalen Abstraktionen haben, müssen wir uns
selbst um die Einhaltung der wesentlichen Einschränkungen kümmern; Werkzeuge können nur wenige Eigenschaften automatisch prüfen. Solche notwendigen
Bezüge zur echten Welt erschweren die Programmierung deutlich.
Menschen schaffen es oft schon nach kurzer Zeit, Objekte als Abstraktionen
von Dingen aus der realen Welt zu sehen und häufig verwendete Abstraktionen aus vorgegebenen Klassenbibliotheken (trotz hohen Abstraktionsgrads) gut
zu verstehen. Es dauert deutlich länger, bis Menschen sich klar genug in der
gemeinsamen Sprache ausdrücken können und ihre Feinheiten verstehen. Meist
dauert es auch sehr lange, sich an die vorgegebenen Einschränkungen zu gewöhnen. Das hat leider gravierende Konsequenzen: Werden Einschränkungen an nur
einer Stelle im Programm ignoriert oder Eigenschaften falsch verstanden, kann
das gesamte Gefüge wie ein Kartenhaus in sich zusammenbrechen und auch an
ganz anderen Programmstellen zu Fehlern führen. Es ist also gefährlich, wenig
erfahrene Personen unkontrolliert an größeren Projekten mitarbeiten zu lassen.
Aber auch erfahrene Personen können unkonzentriert sein und etwas übersehen.
Daher haben sich Vorgehensweisen wie Pair-Programming entwickelt, wobei jeder einzelne Schritt durch eine zweite Person begleitet und überwacht wird.
Großer Wert liegt auf Teambesprechungen, Schulungen, Code-Reviews, etc.,
um vermeidbare Fehler zu unterbinden. Die objektorientierte Programmierung
erfordert einen hohen Personaleinsatz und ist keinesfalls billig.
Die objektorientierte Programmierung hat einen großen evolutionären Wandel durchlebt. Bis in die 1990er-Jahre war wenig darüber bekannt, was Ersetzbarkeit genau bedeutet. Jede aus der realen Welt ableitbare „Is-a“-Beziehung
wurde als mögliche Ersetzung missverstanden. Programmiert wurde auf einer
intuitiven Ebene. Oft traten unerwartete Fehler auf, die pragmatisch beseitigt
wurden, ohne deren Ursachen zu verstehen. Obwohl stark typisierte Sprachen
zur Verfügung standen, wurden dynamische Sprachen bevorzugt, weil sie intuitiv einfacher und flexibler schienen. Im Hobby-Bereich war diese Art der
objektorientierten Programmierung populär. Mit dem Jahrtausendwechsel trat
ein Umdenkprozess ein, weil inhärente Widersprüche, die zu den unerwarteten
Fehlern führten, als Verletzungen des Ersetzbarkeitsprinzips erkannt waren und
nicht mehr ignoriert werden konnten. Vererbung wurde großteils verbannt und
durch Untertypbeziehungen auf Basis des Ersetzbarkeitsprinzips ersetzt. Das
erhöhte die Anforderungen an Programmierer_innen deutlich, was eine Professionalisierung zur Folge hatte. Es wurde versucht, durch Begriffe wie „DuckTyping“ und mit Slogans den früheren objektorientierten Stil als eigenständiges Paradigma aufrecht zu erhalten, aber ohne Ersetzbarkeit (durchbrochene
Werkzeugkette) sind viele Abstraktionen unwirksam und das Paradigma damit
auf die prozedurale Programmierung zurückgeführt. Im Hobby-Bereich herrscht
heute wieder die prozedurale Programmierung vor, auch wenn objektorientierte
Sprachen und einige objektorientierte Werkzeuge eingesetzt werden. Objektorientierte Programmierung im heutigen Sinn ist Profis vorbehalten, die meist
stark typisierte Sprachen bevorzugen und den Umgang mit allen Werkzeugen
in der Werkzeugkiste beherrschen, nicht nur einigen wenigen. Profis sind teuer.
Daher gehen aktuelle Trends in Richtung einer immer stärker werdenden Ein84
2.3 Funktionale Programmierung
beziehung anderer Paradigmen in die objektorientierte Programmierung. Profis
geben zwar die Architektur der zu entwickelnden Software vor und kümmern
sich um kritische Bereiche, aber wo immer es geht, werden Programmteile ausgelagert und im prozeduralen oder funktionalen Paradigma erstellt.
2.3 Funktionale Programmierung
In der funktionalen Programmierung werden Funktionen als Daten verwendet.
Diese einfache Eigenschaft ist schon fast die Definition des Paradigmas. Aber
nur fast. Funktionen als Daten ergeben Möglichkeiten, die genutzt werden wollen. Das hat Konsequenzen, etwa jene, dass auf destruktive Zuweisungen verzichtet wird. So ergibt sich eine logische Konsequenz aus der anderen. Am Ende
stehen funktionale Programmierstile, die mit den prozeduralen Programmierstilen, aus denen sie hervorgegangen sind, nur mehr wenig zu tun haben.
2.3.1 Sauberkeit aus Prinzip
Im Mittelpunkt der imperativen Programmierung steht die destruktive Zuweisung, also die Zuweisung eines neuen Werts an eine Variable, wobei der alte
Wert der Variablen verloren geht. Vertreter „sauberer“ Programmierstile sehen
jede solche Zuweisung als zerstörerischen Akt, etwas Unerwünschtes, Schmutziges, was durch Begriffe wie „Seiteneffekt“ und „Nebenwirkung“ zum Ausdruck
kommt. Ein Ziel der funktionalen Programmierung liegt in der Zurückdrängung
und schließlich Eliminierung jeder Art zerstörerischer Akte.
Unter einem Programmzustand verstehen wir die Gesamtheit der in allen Variablen enthaltenen Werte, unter einer Zustandsänderung die Änderung dieser
Werte. Eine destruktive Zuweisung bewirkt eine Zustandsänderung, durch die
etwas, was wir vorher über den Programmzustand gewusst haben, nachher nicht
mehr gilt. Diese zeitlich begrenzte Gültigkeit unseres Wissens ist das, was an
der destruktiven Zuweisung stört. Wir möchten, dass Wissen, das wir erworben
haben, von Dauer ist. Aus formalen Modellen wissen wir, dass das geht. Etwa
aus verschiedenen Varianten der monotonen Logik können wir Aussagen ableiten, die gültig bleiben; darauf können wir die logikorientierte Programmierung
aufbauen. Wir konzentrieren uns in diesem Abschnitt jedoch auf die funktionale Programmierung (eingebettet in Java), die auf Modellen wie dem λ-Kalkül
aufbaut. Durch das wiederholte Reduzieren von Ausdrücken sammeln wir immer mehr Wissen, müssen aber gesammeltes Wissen nie für ungültig erklären,
bis der maximal reduzierte Ausdruck, die Normalform erreicht ist. Alle Informationen waren zwar auch schon im ursprünglichen (unreduzierten) Ausdruck
enthalten, aber für uns nicht leicht zugänglich. Reduktionen bringen die Information in eine besser fassbare Form. Funktionsaufrufe stellen Reduktionen dar,
viele weitere Sprachelemente sind leicht auf Reduktionen zurückführbar.
In reinen Formen der funktionalen Programmierung verzichten wir auf destruktive Zuweisungen. Das heißt nicht, dass wir auf jede Form der Zuweisung
verzichten, weil die erstmalige Zuweisung eines Werts an eine Variable nicht
85
2 Etablierte Denkmuster und Werkzeugkisten
destruktiv und daher problemlos ist. Statt den Wert einer schon zuvor initialisierten Variablen zu verändern, führen wir eine neue Variable für einen neuen
Wert ein; eine Variable ist damit nur mehr ein Name für einen bestimmten Wert.
Am einfachsten geht das durch die Parameterübergabe bei einem Funktionsaufruf: Jeder Parameter fungiert als neuer, bisher nicht verwendeter Name, der für
das Argument steht. Innerhalb einer Funktion ist es meist nicht schwer, jedem
berechneten Teilergebnis einen neuen Namen zu geben (entspricht dem Ablegen
in einer neuen lokalen Variable). Würden wir jedoch eine Schleife verwenden,
hätten wir ein Problem: Eine Zuweisung in einem Schleifenrumpf bezieht sich
in jeder Iteration auf die gleiche Variable und ist daher destruktiv. Dieses Problem lässt sich nur durch gänzlichen Verzicht auf Schleifen lösen. Das ist keine
übermäßig große Einschränkung, weil jede Schleife durch Rekursion, also durch
Funktionsaufrufe ersetzbar ist. Insgesamt bieten Funktionen den einfachsten
und natürlichsten Weg zur Vermeidung destruktiver Zuweisungen.
Zur Illustration betrachten wir ein Java-Beispiel in einem einfachen funktionalen Stil. Inhaltlich ist es an das Beispiel zur prozeduralen Programmierung
in Abschnitt 2.1 angelehnt:
import java.io.*;
import java.util.Scanner;
public class FunctionalCourse {
public static void main(String[] args) throws IOException {
if (args.length != 4)
throw new IllegalArgumentException("wrong params");
System.out.println(status(
scan(new PointsF(2), args[3],
scan(new PointsF(1), args[2],
scan(new PointsF(0), args[1],
scan(new StudsF(), args[0], null))))));
}
private static Points scan(Func f, String file, Points p)
throws IOException {
return f.apply(new Scanner(new FileInputStream(file)), p);
}
private static String status(Points p) {
if (p == null) return "";
return p.stud.regNo+", "+p.stud.name+" ("+p.stud.curr+"): "
+ p.pnt[0]+"+"+p.pnt[1]+"+"+p.pnt[2]+" = "
+ (p.pnt[0]+p.pnt[1]+p.pnt[2])+"\n"+status(p.next);
}
}
interface Func {
Points apply(Scanner in, Points p);
}
86
2.3 Funktionale Programmierung
class PointsF implements Func {
final int part;
public PointsF(int part) {
this.part = part;
}
public Points apply(Scanner in, Points p) {
if (!in.hasNextInt()) return p;
final int regNo = in.nextInt();
final int pnt = in.nextInt();
if (pnt <= 0 || pnt > 100)
throw new RuntimeException("inappropriate points");
return apply(in, modify(p, regNo, pnt));
}
private Points modify(Points p, int regNo, int pnt) {
if (p == null) throw new RuntimeException("unknown regNo");
if (p.stud.regNo == regNo)
return new Points(p.stud, add(p.pnt, pnt), p.next);
return new Points(p.stud, p.pnt, modify(p.next, regNo, pnt));
}
private int[] add(int[] old, int pnt) {
if (part == 0 && old[0] == 0)
return new int[]{pnt, old[1], old[2]};
if (part == 1 && old[1] == 0)
return new int[]{old[0], pnt, old[2]};
if (old[2] == 0)
return new int[]{old[0], old[1], pnt};
throw new RuntimeException("contradictory");
}
}
class StudsF implements Func {
public Points apply(Scanner in, Points p) {
if (!in.hasNextInt()) return p;
final int regNo = in.nextInt();
if (exists(regNo, p))
throw new RuntimeException("double regNo");;
final int curr = in.nextInt();
final String name = in.nextLine().strip();
final Stud stud = new Stud(regNo, curr, name);
return apply(in, new Points(stud, new int[3], p));
}
private boolean exists(int regNo, Points p) {
if (p == null) return false;
if (p.stud.regNo == regNo) return true;
return exists(regNo, p.next);
}
}
87
2 Etablierte Denkmuster und Werkzeugkisten
class Points {
public final Stud stud;
public final int[] pnt;
public final Points next;
public Points(Stud stud, int[] pnt, Points next) {
this.stud = stud;
this.pnt = pnt;
this.next = next;
}
}
class Stud {
public final int regNo, curr;
public final String name;
public Stud(int regNo, int curr, String name) {
this.regNo = regNo;
this.curr = curr;
this.name = name;
}
}
Im Gegensatz zum Beispiel in Abschnitt 2.1 verwenden wir hier eher lineare
Listen statt Arrays. Arrays sind in der funktionalen Programmierung weniger
wertvoll, weil Arrayeinträge nicht destruktiv verändert werden dürfen. Im Beispiel werden Arrays, statt sie zu ändern, stets neu erzeugt. Beim Einfügen wird
die Liste mit Knoten vom Typ Points zum Teil immer wieder neu aufgebaut. Im
funktionalen Stil ist es einfach, in jedem Punkte-Eintrag eine direkte Referenz
auf die entsprechenden Studierendendaten zu halten, die beim Neuaufbau unverändert übernommen werden. Viele Variablen wurden als final deklariert;
bei formalen Parametern wurde aus Platzgründen darauf verzichtet, obwohl
auch die Parameter so verwendet werden, als ob sie final wären. Das ist eine
typische Eigenschaft von funktionalen Programmen in Java. Häufig kommen tief
verschachtelte Ausdrücke vor, die durch die Schachtelung helfen, die Anzahl der
nötigen Variablen klein zu halten. Die Klassen Points und Stud enthalten zwar
public Variablen und deren Instanzen sind eher Records als Objekte im Sinne
der objektorientierten Programmierung, aber dennoch haben die Records andere Eigenschaften als in der prozeduralen Programmierung, weil Variablen nach
der Initialisierung nur lesbar sind. Funktionen werden nicht direkt als Daten
verwendet, aber als Instanzen von Func sind über dieses Interface beschliebene Methoden fast wie Funktionen als Daten verwendbar. Objekte von Func
enthalten neben apply auch als lokal zu betrachtende Hilfsfunktionen und Objektvariablen, die Werte aus den Umgebungen der Funktionen enthalten. Wie zu
erwarten benötigt das funktionale Programm mehr Methoden und Parameter
als das prozedurale. Im Gegenzug werden keine Schleifen benötigt. Trotz dieser
Unterschiede bestehen viele Gemeinsamkeiten mit dem prozeduralen Beispiel
aus Abschnitt 2.1. Insbesondere bilden beide Programme die Ausführung recht
klar über den Kontrollfluss ab, der Datenfluss spielt keine große Rolle.
88
2.3 Funktionale Programmierung
Der in obigem Beispiel dargestellte Stil bildet nur einen ersten Schritt in
Richtung funktionaler Programmierung. Tatsächlich sind wir hauptsächlich mit
Sprachkonzepten der prozeduralen Programmierung in Java ausgekommen und
haben kaum Funktionen als Daten verwendet. Die gesamte Ein- und Ausgabe
erfolgt über prozedurale Seiteneffekte und verlangt in dieser Form nach viel
Kontrolle. Java ist keine funktionale Sprache und unterstützt die funktionale
Programmierung daher nur unzureichend. Dennoch hat diese einfache Form der
funktionalen Programmierung schon einige wichtige Eigenschaften:
• Auf destruktive Zuweisungen wird verzichtet. Daher gibt es (außer bei der
Parameterübergabe) keine Möglichkeit, dass Programmteile über gemeinsame Variablen miteinander kommunizieren. Ohne Funktionen als Daten
bestimmt nur der Kontrollfluss den Programmablauf.
• Gleiche Werte (auch Instanzen von Klassen) bleiben immer gleich, die
Gleichheit geht nicht durch destruktive Zuweisungen verloren. Diese Eigenschaft bildet die Basis für referentielle Transparenz: Wenn wir Objekte ausschließlich über equals (oder compareTo und ähnliche Methoden)
vergleichen, nicht mittels ==, ist der Unterschied zwischen Gleichheit und
Identität aufgehoben. Es gibt keinen Unterschied mehr zwischen einem
Original und dessen Kopie.
• Trotz der Verwendung von Rekursion sind Programme strukturell einfacher, weil keine Schleifen verwendet werden.
Einige weitere Eigenschaften einfacher funktionaler Programme kommen nur
zum Tragen, wenn speziell für die funktionale Programmierung ausgelegte Sprachen zum Einsatz kommen. Java-Programme können nicht davon profitieren:
• In Java wirkt sich tiefe Rekursion negativ auf den Speicherverbrauch
für den Stack der Methodenaufrufe aus. Funktionale Sprachen verwenden Techniken wie „Tail-End-Recursion-Optimization“, um die Stackbelastung klein zu halten und Funktionsaufrufe effizienter zu machen. In vielen
Fällen wird fast die gleiche Effizienz wie durch Schleifen erreicht. Einige
Optimierungen setzen die semantische Einfachheit funktionaler Programme voraus und sind nicht direkt auf imperative Sprachen übertragbar.
• Die strukturelle Einfachheit funktionaler Programme kann in der Syntax
der Sprache genutzt werden, um Programme kompakter, mit kürzeren
Programmtexten darzustellen. In obigem Java-Programm ist das nicht zu
erkennen. Das kommt daher, dass wir dem Programm in einer Sprache,
die für die objektorientierte Programmierung ausgelegt ist, einen fremden
Stil aufgezwungen haben. Alleine schon die vielen Verwendungen der in
einer funktionalen Sprache unnötigen Wörter private, public, static
und final erhöhen den Schreibaufwand deutlich.
• Ist durchgehend referentielle Transparenz gegeben, können Vergleichsmethoden wie equals leicht automatisch generiert werden, was den Programmieraufwand reduziert.
89
2 Etablierte Denkmuster und Werkzeugkisten
• Referentielle Transparenz ermöglicht einen effizienten Umgang mit Konstanten. Häufig wird in funktionalen Sprachen Pattern-Matching eingesetzt. Dabei erfolgt, ähnlich wie in einer switch-Anweisung, eine Verzweigung abhängig von vorhandenen Daten, wobei aber auch komplexe
Datenmuster herangezogen werden, nicht nur Zahlen und Zeichenketten.
Prinzipiell wäre Pattern-Matching auch in der imperativen Programmierung sinnvoll, entfaltet aber erst zusammen mit referentieller Transparenz
das volle Potential zur Verringerung der Programmkomplexität.
• Funktionale Programme eignen sich bestens für die Typinferenz, bei der
ein Compiler statische Typen aus dem Programm errechnet und Typkonsistenz garantiert, ganz ohne Typdeklarationen im Programm. Das ermöglicht kurze und zugleich statisch typsichere Programme. In objektorientierten Sprachen ist uneingeschränkte Typinferenz an Stellen, an denen
Ersetzbarkeit gegeben sein muss (also an allen wichtigen Stellen), prinzipiell unmöglich. Im Umkehrschluss gilt auch, dass funktionale Sprachen
mit Typinferenz keine Ersetzbarkeit garantieren können.
• Einfache lineare Datenstrukturen, etwa Listen, sind in der funktionalen
Programmierung besonders wichtig. Häufig werden auch komplexe Listenoperationen durch die Syntax der Sprache direkt unterstützt.
2.3.2 Streben nach höherer Ordnung
Auch ohne wesentliche Verwendung von Funktionen als Daten hat die funktionale Programmierung schon interessante Eigenschaften. Funktionale Programmierstile ändern sich deutlich, wenn Funktionen häufig als Daten verwendet
werden. Funktionen, die Funktionen als Parameter nehmen bzw. als Ergebnisse
zurückgeben, heißen Funktionen höherer Ordnung oder funktionale Formen. Sie
dienen in gewisser Weise als Ersatz für Kontrollstrukturen. Werden Funktionen
höherer Ordnung ernsthaft eingesetzt, dann nicht nur an einzelnen Programmstellen, sondern meist in großem Stil und fast überall. Der Einsatz von Funktionen höherer Ordnung reduziert den sichtbaren Bedarf an Rekursion deutlich,
im (nicht so gut sichtbaren) Hintergrund ist Rekursion nach wie vor wesentlich.
Der auf Funktionen höherer Ordnung aufbauende funktionale Programmierstil heißt auch applikative Programmierung um deutlich zu machen, dass es
nicht darauf ankommt, für jede Kleinigkeit eine eigene Funktion zu schreiben,
sondern darauf, bestehende (großteils vorgefertigte und mit dem System ausgelieferte) Funktionen geschickt anzuwenden und miteinander zu kombinieren,
um eine Aufgabe zu lösen. Es ergibt sich eine Steigerung der Produktivität
der Programmierer_innen, weil auch komplexe Algorithmen in vergleichsweise kurzer Zeit ausführbar gemacht werden können. Im Prinzip kann zwar jede
erdenkliche Funktion höherer Ordnung entwickelt werden. Letztendlich kommt
es aber darauf an, eine überschaubar kleine Menge an Funktionen höherer Ordnung in hoher Qualität zur Verfügung zu haben, die in Kombination zur Lösung
der meisten Aufgaben ausreicht. Sollte ausnahmsweise einmal keine passende
Funktion zur Verfügung stehen, können wir sie problemlos hinzufügen.
90
2.3 Funktionale Programmierung
Java unterstützt seit Version 8 die applikative Programmierung mit speziell
dafür eingeführten Spracherweiterungen, insbesondere den Java-8-Streams und
Lambda-Ausdrücken (kurz Lambdas) sowie vielen Erweiterungen vordefinierter
Klassen. Hier ist das schon bekannte Beispiel in einem applikativen Stil:
import java.io.IOException;
import java.nio.file.*;
import java.util.*;
import java.util.stream.*;
public class ApplicativeCourse {
public static void main(String[] args) {
if (args.length != 4)
throw new IllegalArgumentException("wrong params");
List<Stream<String>> ins = Stream.of(args).map(file -> {
try {return Files.lines(Paths.get(file));}
catch(IOException x) {throw new RuntimeException(x);}
}).collect(Collectors.toList());
Map<Integer, Stud> studs = ins.get(0)
.collect(HashMap::new, (smap, line) -> {
Scanner sc = new Scanner(line);
int regNo = sc.nextInt();
Stud stud = new Stud(regNo, sc.nextInt(),
sc.nextLine().strip());
if (smap.put(regNo, stud) != null)
throw new RuntimeException("double regNo");
}, HashMap::putAll);
List<Map<Integer, Integer>> lm = ins.stream().skip(1)
.map(in -> in.collect(HashMap::new,
(Map<Integer, Integer> pmap, String line) -> {
Scanner sc = new Scanner(line);
int regNo = sc.nextInt();
int points = sc.nextInt();
if (points <= 0 || points > 100)
throw new RuntimeException("inappropriate points");
if (studs.get(regNo) == null)
throw new RuntimeException("unknown regNo");
if (pmap.put(regNo, points) != null)
throw new RuntimeException("contradictory");
}, Map::putAll))
.collect(Collectors.toList());
System.out.println(
studs.values().stream().map(s -> line(s, lm))
.collect(Collectors.joining("\n")));
}
91
2 Etablierte Denkmuster und Werkzeugkisten
private static String line(Stud s,
List<Map<Integer, Integer>> lm) {
List<Integer> ps = lm.stream()
.map(m -> m.getOrDefault(s.regNo, 0))
.collect(Collectors.toList());
return s.regNo + ", " + s.name + " (" + s.curr + "): "
+ ps.stream().map(i -> i.toString())
.collect(Collectors.joining("+"))
+ " = " + ps.stream().reduce(0, Integer::sum);
}
}
class Stud {
public final int regNo, curr;
public final String name;
public Stud(int regNo, int curr, String name) {
this.regNo = regNo;
this.curr = curr;
this.name = name;
}
}
Es fällt trotz ausladender Java-Syntax auf, dass das Programm kürzer ist,
mit weniger Funktionen auskommt und nirgends sichtbar Rekursion verwendet. Außerdem werden keine eigenen Listen angelegt, sondern vorgefertigte
Container-Klassen zusammen mit Generizität eingesetzt. Das sind (bis auf die
Java-Syntax) typische Eigenschaften der applikativen Programmierung.
Das Beispiel verwendet Lambdas, die so ähnlich wie λ-Abstraktionen einsetzbar sind. Bei genauerer Betrachtung sind Lambdas Objekte, die nur eine einzige
Methode enthalten. Sehen wir uns s -> line(s, lm) aus obigem Programmtext an: Um diesen Ausdruck besser verstehen zu können, gehen wir davon
aus, dass s vom Typ Stud ist. Damit ist das Lambda einfach lesbar: s ist der
einzige Parameter einer Funktion, die den Teil nach -> durch einen Aufruf von
line(s, lm) berechnet und das Ergebnis des Aufrufs zurückgibt, wobei lm eine
weiter oben initialisierte Variable ist. Der Ausdruck entspricht dieser Methode:
String apply(Stud s) { return line(s, lm); }
Genau genommen wird an die Methode map, an die das Lambda als Argument
übergeben wird, ein neues Objekt übergeben, das nur diese eine Methode enthält. map hat (nach Auflösung der Generizität) folgende Signatur:
Stream<String> map(Function<Stud, String> mapper)
Dabei ist Function<Stud, String> (nach Auflösung der Generizität) ein Interface, das nur eine abstrakte Methode beschreibt:
92
2.3 Funktionale Programmierung
String apply(Stud t);
Lambdas können nur dort verwendet werden, wo das entsprechende Interface
(etwa als Parametertyp oder bei zurückgegebenen Lambdas der Ergebnistyp)
genau eine abstrakte Methode ohne Default-Implementierung enthält; solche
Interfaces heißen funktionale Interfaces. Das Lambda entspricht der Implementierung dieser einen Methode, von dieser Methodensignatur kommt auch der
Methodenname. Innerhalb von map kann das übergebene Lambda ganz normal
wie jede andere Methode aufgerufen werden: mapper.apply(...) wobei die
Punkte für das Argument vom Typ Stud stehen. Im Rumpf des Lambdas können Variablen aus der Umgebung vorkommen, etwa lm. Allerdings gibt es die
Einschränkung, dass solche Variablen als final deklariert sein müssen, oder nur
so verwendet werden, als ob sie final wären. Damit wird verhindert, dass auf
undurchschaubare oder gefährliche Weise über Variablen kommuniziert wird.
Bei anderen Lambdas im Beispiel verhält es sich ähnlich, wobei jedoch mehrere syntaktisch verschiedene Varianten vorkommen. Links von -> kann statt
eines Namens eine Liste von Namen in runden Klammern stehen; dabei entspricht jeder Name einem Parameter. Damit können Lambdas beliebig viele
Parameter haben. Zwecks besserer Lesbarkeit können auch Typen der Parameter dabei stehen. In einigen Fällen sind Typen bei solchen Parametern sogar
notwendig, wenn der Compiler in der lokalen Umgebung nicht ausreichend Information findet, um die Typen durch Typinferenz berechnen zu können. Wie jede
objektorientierte Sprache kann Java ja keine uneingeschränkte Typinferenz anbieten, sondern nur eine auf die lokale Umgebung eingeschränkte Form. Rechts
von -> können auch Anweisungen und Methodenrümpfe (mehrere Anweisungen
in geschwungenen Klammern) stehen, nicht nur Ausdrücke zur Berechnung von
Rückgabewerten. Im Beispiel werden mehrere solche Lambdas verwendet, auch
solche mit umfangreichen Rümpfen.
Nicht jede als Argument oder Ergebnis verwendete Methode muss in oben
beschriebener Lambda-Syntax definiert sein, es sind auch schon existierende
Methoden verwendbar. So ist HashMap::putAll die Methode putAll aus der
Klasse HashMap. Genauer: Als Argument wird ein neues Objekt übergeben, dessen einzige Methode sich genau so wie putAll aus HashMap verhält. Dementsprechend steht HashMap::new für ein Objekt, dessen einzige Methode ein neues
Objekt von HashMap erzeugt. Parameterzahl und -typen der Methode oder des
Konstruktors hängen vom funktionalen Interface ab.
Java-8-Streams stellen eine strukturierte Sammlung von Funktionen höherer
Ordnung dar, die vor allem für die Abarbeitung großer Datenmengen konzipiert
sind. Wie der Name sagt, entspricht jedes Objekt von Stream einem linearen
Strom an gleichartigen Daten, wobei der Typ der Daten über Generizität festgelegt wird. Bei Datenströmen (kurz Strömen) der Arten IntStream, LongStream
und DoubleStream sind Daten vom jeweiligen primitiven Typ. Die Abarbeitung
erfolgt in drei Stufen: Zuerst wird ein Strom in einer Operation erzeugt, danach
werden die Daten im Strom über beliebig viele modifizierende Operationen gefiltert oder geändert und am Ende steht eine abschließende Operation, die Daten
meist in einer anderen Form außerhalb des Stroms verfügbar macht.
93
2 Etablierte Denkmuster und Werkzeugkisten
Im Beispiel wird die Variable ins mit einer vierelementigen Liste von Stömen
initialisiert, die jeweils die Zeilen in den vier in den Kommandozeilenargumenten
args genannten Dateien enthalten. Stream.of(args) erzeugt einen Strom, der
die vier Dateinamen in args als Zeichenketten enthält. Die darauf angewandte
Methode map wendet auf jede Zeichenkette ein Lambda an, das für jeden Dateinamen einen Strom an in der Datei enthaltenen Zeilen erzeugt. Das Lambda
bewerkstelligt das durch Anwendung der vordefinierten Methode Files.lines
auf je einen Dateipfad, der mittels Paths.get(file) aus dem Dateinamen file
erzeugt wird. Wie bei jeder Ein- oder Ausgabe kann dabei eine IOException
auftreten, die über einen try-catch-Block abgefangen werden muss. Ein Weiterreichen dieser überprüften Ausnahme nach außen ist durch das Lambda nicht
möglich, weil die Signatur keine throws-Klausel enthält – ein Hinweis auf eine
andere Form der Abstraktion als in der objektorientierten Programmierung. Die
Ausnahme wird in eine unüberprüfte Ausnahme vom Typ RuntimeException
umgewandelt, die weitergereicht werden kann. Das Ergebnis der modifizierenden Operation map ist also ein Strom, der vier Ströme enthält. Der äußere Strom
wird durch die abschließende Operation collect(Collectors.toList()) abgeschlossen, wobei die Inhalte des Stroms in einer Liste in der gleichen Reihenfolge wie im Strom abgelegt werden. Das Argument von collect ist ein Objekt,
das für die passende Umwandlung zuständig ist. In Collectors sind Methoden
definiert, die solche Objekte zurückgeben. Das von toList() zurückgegebene
Objekt erzeugt eine Liste (hier vom Typ List<Stream<String>>), ohne Details
über die Implementierung der Liste bekanntzugeben.
Die Ströme in ins werden auf zwei verschiedene Weisen weiterverwendet; der
erste Strom enthält Studierendendaten, die weiteren Ströme Informationen zu
„Punkten“ für drei verschiedene Lehrveranstaltungsteile. Der erste dieser Ströme wird direkt durch collect angewandt auf ins.get(0) abgeschlossen. Hier
wird jedoch eine Variante von collect mit drei Parametern verwendet, über
die wir das Erzeugen der resultierenden Datenstruktur genauer steuern können.
Der erste Parameter wird zur Erzeugung einer Datenstruktur verwendet, der
zweite zum Einfügen eines Datenelements aus dem Strom und der dritte zum
Zusammenfassen mehrerer Datenstrukturen zu einer. Der dritte Parameter wird
für den Fall benötigt, dass der Datenstrom parallel abgearbeitet wird, wobei
mehrere gleichartige Datenstrukturen erzeugt und am Ende zusammengefügt
werden. Im Beispiel wird über den ersten Parameter HashMap::new eine neue
Hashtabelle erzeugt und über den dritten HashMap::putAll zwei Hashtabellen
zusammengefügt. Das zweite Argument ist ein Lambda mit zwei Parametern,
der Hashtabelle, in die eingefügt werden soll, sowie die zu bearbeitende Zeile
aus dem Strom. Über einen Scanner werden die Daten aus der Zeile gelesen und
das entsprechende neue Objekt von Stud in die Hashtabelle eingefügt, wobei es
zu einer Ausnahme kommt, wenn schon Daten für die gleiche Matrikelnummer
(regNo) vorhanden sind. Zur Bearbeitung der restlichen Ströme wird ein neuer
Strom über die Einträge in ins erzeugt, wobei skip(1) gleich einen Eintrag,
den ersten, der schon abgearbeitet ist, aus diesem Strom entfernt. Die restlichen
drei Ströme im Strom werden auf ähnliche Weise wie der erste Strom behandelt:
Aus jedem Strom werden Daten mit Punktebewertungen gelesen (jeweils eine
94
2.3 Funktionale Programmierung
Matrikelnummer mit einer Punktezahl) und nach diversen Prüfungen Hashtabellen erzeugt, die jeweils Matrikelnummern auf Punktezahlen abbilden. Durch
collect(Collectors.toList()) wird eine Liste der Hashtabellen erzeugt. Alle Ströme in ins sind damit abgearbeitet.
Schließlich wird aus den Daten über Studierende in studs und Daten über
gesammelte Punkte in lm die Ausgabe generiert. Dazu wird ein Strom über den
Werten in studs (die Schlüssel werden nicht mehr benötigt) erzeugt, map wandelt, wie oben angesprochen, durch Aufrufe von line jedes Objekt von Stud
im Strom in eine Ausgabezeile um und collect schließt den Strom ab. Die
Methode joining aus Collectors ist nur auf einen Strom mit Zeichenketten
anwendbar und sorgt dafür, dass alle Zeichenketten zu einer einzigen zusammengefügt werden, wobei die als Argument übergebene Zeichenkette "\n" als
Trennsymbol zwischen je zwei Zeichenketten gesetzt wird. Aus dem Strom an
Ausgabezeilen entsteht so der gesamte auszugebende Text.
Auch die Methode line ist über Ströme implementiert. Zunächst werden in
der Liste ps die pro Lehrveranstaltungsteil gesammelten Punkte für den/die
Studierende_n s abgelegt, wobei ein Strom über lm pro Lehrveranstaltungsteil eine Map enthält, ein Aufruf der modifizierenden Operation map durch
getOrDefault jeweils die Punkteanzahl für s aus der Map holt (oder 0 falls
keine Punkte für s eingetragen sind) und collect den resultierenden Strom
an ganzen Zahlen abschließt. Die Ausgabezeile wird aus allgemeinen Daten in
s, den einzelnen Werten in ps und der Summe der Werte in ps zusammengesetzt. Die Werte in ps werden dabei über einen Strom über ps erfasst, wobei
über map zunächst die Zahlen in Zeichenketten umgewandelt werden, die danach über joining ähnlich wie oben beschrieben zusammengesetzt werden. Die
Summe wird ebenfalls aus einem Stream über ps gebildet. Dieser Strom wird
jedoch nicht über collect abgeschlossen, sondern über reduce, eine Methode, die alle Werte im Stream zu einem einzigen Wert reduziert. Diese Methode
nimmt als ersten Parameter den Anfangswert, also das Ergebnis, das bei einem
leeren Strom zurückkommen soll, in unserem Fall 0. Der zweite Parameter ist
ein Lambda, das den bisher reduzierten Wert und den nächsten Wert im Strom
zu einem neuen Wert reduziert. Hier wird dafür Integer::sum verwendet, eine
Funktion, die einfach nur zwei ganze Zahlen addiert; stattdessen hätten wir
gleichbedeutend das Lambda (x,y) -> x + y verwenden können. Das Ergebnis ist also einfach die Summe aller Zahlen in ps.
Java-8-Streams verwenden Lazy-Evaluation (siehe Abschnitt 1.5.1). Das heißt,
Daten in einem Strom werden erst berechnet, wenn eine abschließende Operation diese Daten benötigt. In diesem Beispiel verwenden die abschließenden
Operationen collect und reduce alle verfügbaren Daten. Dadurch wirkt sich
Lazy-Evaluation semantisch kaum aus. Jedoch muss im Beispiel ins nie wirklich
alle Zeilen aus allen eingelesenen Dateien enthalten. Daten werden erst aus den
Dateien gelesen, wenn diese Daten verarbeitet werden, nach Abarbeitung können sie durch Speicherbereinigung entfernt werden. Lazy-Evaluation kann also
tatsächlich die Effizienz steigern, in diesem Fall den Speicherverbrauch senken.
Wir haben gesehen, dass sich die applikative Programmierung in Java aus
dem Zusammenspiel einiger nicht-trivialer Sprachkonzepte ergibt. Funktionen
95
2 Etablierte Denkmuster und Werkzeugkisten
höherer Ordnung in Java-8-Streams und Collections, Lambdas, funktionale Interfaces und Generizität spielen hinein. Ein Verständnis dieses Zusammenspiels
ist unerlässlich, der applikative Programmierstil in Java daher eher für Menschen mit Programmiererfahrung geeignet. Entsprechend erfahrene Menschen
können applikative Programme trotz geforderter hoher Qualitätsstandards in
deutlich kürzerer Zeit schreiben als in allen anderen Programmierstilen. Leider
sind applikative Programme auch von erfahrenen Menschen nicht so gut lesbar wie viele Programme in anderen erfolgreichen Stilen, was die Wartung auf
lange Sicht erschwert. Die Effizienz beim Schreiben kommt unter anderem von
der Typinferenz, die das Hinschreiben vieler Typen erspart, aber die Lesbarkeit
vermindert. Noch wichtiger ist der höhere Grad an Abstraktion, weil hinter jeder Funktion höherer Ordnung komplexe Denkmuster stehen, die den genauen
Programmablauf teilweise verschleiern. Die Idee hinter einem Lösungsansatz
bleibt im fertigen Programm oft verborgen.
2.3.3 Friedliche Koexistenz
Wir wollen nun untersuchen, welche Rolle die funktionale Programmierung,
insbesondere in der applikativen Form, in der Praxis spielen kann. Fassen wir
einige diesbezügliche Beobachtungen zusammen:
• Die Programmerstellung durch erfahrene Leute kann in der funktionalen
Programmierung sehr effizient sein. Der ICFP Programming Contest, bei
dem es darum geht, eine algorithmisch komplexe Aufgabe möglichst rasch
und effizient zu lösen, wurde häufig (aber nicht immer) von einem Team
gewonnen, das eine funktionale Sprache in einem applikativen Stil verwendete. Das ist nicht überraschend, wenn man bedenkt, dass ICFP die
„International Conference on Functional Programming“ ist. Aber alleine
die Existenz dieses Bewerbs zeigt deutlich, wo die Stärken der funktionalen Programmierung vermutet werden: rasche Entwicklung algorithmisch
komplexer Aufgaben, aber nicht deren langfristige Wartung,
• Der Schwerpunkt der funktionalen Programmierung liegt in der Programmierung im Feinen, nicht in der Programmorganisation. Damit steht die
funktionale Programmierung in Konkurrenz zur prozeduralen Programmierung, aber kaum zur objektorientierten Programmierung.
• Die funktionale Programmierung bietet nicht so gute Kontrollmöglichkeiten von Details des Programmablaufs wie die prozedurale Programmierung, dafür aber einen viel höheren Abstraktionsgrad, der die Programmiereffizienz verbessert. Die funktionale Programmierung eignet sich daher kaum in Bereichen, in denen es auf bestmögliche Kontrolle ankommt
(hardwarenahe Programmierung und Echtzeitprogrammierung).
• Eine für die parallele Programmierung vorteilhafte Strukturierung von
Daten ist oft auch gut für die funktionale Programmierung geeignet, jedoch nicht für die objektorientierte. In Teilbereichen der parallelen Programmierung ist die funktionale Programmierung stark vertreten.
96
2.3 Funktionale Programmierung
• Bei einigen Personengruppen im Bereich der Hobby- und Anwendungsprogrammierung ist, auch historisch bedingt, viel Wissen über funktionale Programmierung vorhanden. Es gibt offensichtlich keinen Widerspruch
dazu, dass die effiziente funktionale Programmierung Erfahrung voraussetzt. Auch in der Hobby-Programmierung kann viel Erfahrung vorhanden
sein, wenn auch häufig eingeschränkt auf enge Themenbereiche.
Die funktionale Programmierung in Java und ähnlichen Sprachen erfreut sich
immer größerer Beliebtheit, was sich anhand obiger Beobachtungen nur unvollständig erklären lässt. Die Anzahl an Programmen, in denen die funktionale
Programmierung ihre Vorteile voll ausspielen kann, ist ja begrenzt. Aber es
geht gar nicht darum, ganze Programme in einem funktionalen Stil zu schreiben. Vielmehr gibt es in großen Programmen häufig Teile, die sich perfekt für
einen funktionalen Programmierstil eignen würden, obwohl sich andere Programmteile nicht dafür eignen. Das heißt, die funktionale Programmierung hat
vor allem in Kooperation mit anderen Programmierparadigmen viel Potential.
Die Kombination mehrerer Paradigmen ist aber schwierig, weil die grundlegenden Annahmen, Vorgehensweisen und Werkzeuge nicht zusammenpassen.
Referentielle Transparenz verträgt sich nicht mit imperativen Veränderungen
des Programmzustands.
Trotz aller Widersprüche ist bekannt, wie die funktionale Programmierung
in Kombination mit anderen Paradigmen einsetzbar ist. Da die objektorientierte Programmierung sich auf die Programmierung im Groben bezieht, die
funktionale und prozedurale Programmierung aber auf die Programmierung im
Feinen, scheint es oberflächlich betrachtet einfach zu sein, den prozeduralen Anteil an der objektorientierten Programmierung durch einen funktionalen Anteil
zu ersetzen. So einfach geht das aber nicht, weil es rein auf Basis der funktionalen Programmierung keine veränderlichen Objektzustände geben könnte, die
in der Praxis wesentlich sind. Eine sinnvolle Kombination setzt sowohl prozedurale als auch funktionale Anteile voraus. Das ist möglich, indem Prozeduren
Funktionen (entsprechend der funktionalen Programmierung) aufrufen, aber
Funktionen keine Prozeduren aufrufen und keinen Zugriff auf veränderbare Variablen bekommen. Daten fließen über Parameter von den Prozeduren zu den
Funktionen und als Funktionsergebnisse zu den Prozeduren. So bleiben charakteristische Eigenschaften der Paradigmen erhalten, aber das Zusammenspiel der
Programmteile in unterschiedlichen Paradigmen ist eingeschränkt. Die größte
praktische Schwierigkeit besteht darin, an der Grenze zwischen prozeduralem
und funktionalem Teil die Art des Denkens beim Programmieren umzustellen.
Auch in der rein funktionalen Programmierung stehen wir vor dem Dilemma,
dass Ein- und Ausgabe notwendigerweise Seiteneffekte darstellen. In funktionalen Sprachen wie Haskell gibt es eine elegante Lösung: Die Ein- und Ausgabe
wird in Monaden verpackt, das sind mathematische bzw. rein funktionale Gebilde, welche die eigentlichen Inhalte in Form von Funktionen kapseln und vom
Rest des Systems abtrennen. Monaden bewirken, dass die darin enthaltenen
Funktionen erst bei Vorliegen von Eingabedaten in Ausgabedaten abgebildet
werden, unabhängig vom Ablauf des restlichen Programms. Das klingt kompli97
2 Etablierte Denkmuster und Werkzeugkisten
ziert und ist es auch, wenn wir versuchen, den Programmablauf genau nachzuvollziehen. Wegen des hohen Abstraktionsgrads durch referentielle Transparenz
müssen wir den Ablauf aber nicht verfolgen, um das Programm zu verstehen.
Im Endeffekt ergibt sich nämlich das, was wir oben beschrieben haben: Die gesamte Ein- und Ausgabe erfolgt in einem eher prozeduralen Programmteil, die
Prozeduren rufen reine Funktionen auf, aber diese Funktionen rufen niemals
Prozeduren auf; veränderbare Variablen sowie die gesamte Ein- und Ausgabe
sind den reinen Funktionen (durch das Typsystem geprüft) nicht zugänglich.
Andere funktionale Sprachen wie OCAML sorgen direkt, ohne Umweg über
Monaden, für eine Trennung zwischen Prozeduren mit Seiteneffekten und reinen Funktionen, wobei referentielle Transparenz auf Funktionen gewahrt bleibt.
Das „O“ in OCAML steht für „Object“, es wird also ein objektorientierter Programmierstil mit Seiteneffekten unterstützt. Das funktioniert auf herkömmliche
Weise: Methoden verändern den Zustand von Objekten durch destruktive Zuweisungen, aber Methoden können auch reine Funktionen aufrufen.
Lambdas in Java funktionieren nur zum Teil nach diesem Schema: Es ist zwar
nicht erlaubt, innerhalb von Lambdas auf veränderbare Variablen aus der Umgebung direkt zuzugreifen, aber Methoden, die als Prozeduren zu betrachten
sind, können aufgerufen werden. Durch den Aufruf von Prozeduren verlieren
wir referentielle Transparenz und damit eine Form der funktionalen Abstraktion. Das heißt, wir müssen die Ausführungsreihenfolge nachvollziehen, um das
Programm zu verstehen. Die Ausführungsreihenfolge ist die Reihenfolge, in der
Methoden tatsächlich ausgeführt werden, nicht nur der logische Zusammenhang zwischen Ausdrücken. Bei Lazy-Evaluation wie in Java-8-Streams kann
sich die Ausführungsreihenfolge erheblich von den logischen Zusammenhängen
unterscheiden. Betrachten wir dazu ein Beispiel:
import java.util.stream.Stream;
public class Elem {
private static int num = 0;
private int id;
private Elem() { System.out.print((id = num++)); }
public static Stream<Elem> stream() {
return Stream.generate(Elem::new);
}
public String toString() { return "-" + id; }
}
Objekte von Elem enthalten eine Variable id mit jeweils unterschiedlichen
Zahlen. Die Methode Elem.stream() erzeugt einen unbegrenzten Strom von
Elem-Objekten mit aufsteigenden Zahlenwerten. Da Java-8-Streams auf LazyEvaluation beruhen, stellt die unbeschränkte Strom-Länge kein Problem dar.
Zur Demonstration des Ablaufs wird bei der Objekterzeugung die jeweilige Zahl
ausgegeben, ebenso wie durch toString (nach einem Trennsymbol). Folgender
Aufruf gibt 10 Zeilen aus, jede von der Form 0-0, 1-1, 2-2 und so weiter:
Elem.stream().limit(10).forEach(System.out::println);
98
2.3 Funktionale Programmierung
Die Operation limit auf dem Strom lässt die gegebene Anzahl an Elementen
durch und schließt den Strom danach. Die abschließende Operation forEach
führt auf jedem Element das übergebene Lambda, in diesem Fall println aus.
Wer gewohnt ist, prozedural zu denken, würde erwarten, dass zuerst durch den
Konstruktor die Zahlen 0 bis 9 ausgegeben werden und erst danach die Ausgaben durch println erfolgen. Aber so funktionieren Ströme nicht. Erst bei
Aufruf von println wird das Strom-Element benötigt und erst dann wird es
(lazy) erzeugt. Nur dadurch ist es möglich, zuerst mit einem unbeschränkten
Strom zu arbeiten und diesen erst später durch limit zu begrenzen. Abschließende Operationen auf Strömen haben häufig Seiteneffekte wie das Ausgeben
von Werten oder das Hinzufügen von Werten zu Datenstrukturen, wobei die
Datenstrukturen selbst nicht funktional aufgebaut sind. Modifizierende Operationen1
sind dagegen meist frei von Seiteneffekten.
Es stellt sich die Frage, ob die Programmierung mit Lambdas und Java-8-
Streams überhaupt funktional ist. Im Vergleich zu aktuellen funktionalen Sprachen wie Haskell ist Java ganz anders. Es gibt allerdings auch Sprachen wie
Lisp, der Sprache, in der die funktionale Programmierung entwickelt wurde,
in der destruktive Zuweisungen zwar unerwünscht, aber möglich sind. Wie in
jeder objektorientierten oder prozeduralen Sprache ist es auch in Java möglich,
auf Seiteneffekte weitgehend zu verzichten und daher funktional zu programmieren. Lambdas können dabei hilfreich sein. Es ist auch möglich, einen objektorientierten, prozeduralen und funktionalen Stil zu mischen, wobei Prozeduren
Funktionen aufrufen, aber nicht umgekehrt. Es bleibt fraglich, ob das auch bei
Java-8-Streams möglich ist, wenn abschließende Operationen als prozedural zu
betrachten sind. Die Antwort ist ein vorsichtiges „vielleicht ja“, wenn wir die
Ausführungsreihenfolge betrachten: Abschließende Operationen stehen in gewisser Weise am Anfang, weil sie Ausführungen anderer Operationen im Strom
erst anstoßen; für alle anderen Operationen im Strom ist Seiteneffektfreiheit
wichtiger, weil nur schwer vorhersehbar ist, wann sie, wenn überhaupt, ausgeführt werden, sodass die Auswirkungen von Seiteneffekten kaum abschätzbar
sind. In obigem Beispiel ist das print im Konstruktor sicher ein größeres Problem als das println in der abschließenden Operation. In sehr eingeschränkter
Form, etwa num++ im Beispiel, sind Seiteneffekte vertretbar, wenn ihr Effekt örtlich gut abgegrenzt bleibt. Natürlich dürfen wir im Beispiel nicht davon ausgehen, dass alle zurückgegebenen Zahlen fortlaufend sind, weil ja auch außerhalb
des betrachteten Stroms Objekte von Elem erzeugt werden könnten.
Generell können wir sagen, dass die applikative Programmierung zwar wesentlich von der funktionalen Programmierung beeinflusst ist, aber nicht auf
funktionale Programmierung beschränkt bleibt. Es gibt durchaus auch prozedurale Elemente und die objektorientierte Datenkapselung kommt zum Tragen. Trotzdem müssen wir in der applikativen Programmierung vorsichtig und
sparsam mit Seiteneffekten umgehen, weil Effekte durch kaum vorhersehbare
Programmabläufe andernfalls rasch undurchschaubar werden – siehe Kapitel 5.
1
„Modifizierend“ bedeutet hier nicht, dass Variableninhalte destruktiv verändert werden.
Modifizierende Operationen wie map verändern den Strom durch Einführung neuer Werte.
99
2 Etablierte Denkmuster und Werkzeugkisten
Prinzipiell ist die applikative Programmierung eine wertvolle Ergänzung zur
objektorientierten Programmierung, gerade wegen ihrer Verschiedenartigkeit.
Applikative Programmteile können innerhalb eines großen objektorientierten
Programms klein genug bleiben, damit die Auswirkungen der erschwerten Lesbarkeit in Grenzen gehalten werden. Trotzdem können diese Teile groß genug
sein, um die Effizienz der Programmerstellung zu verbessern. Selbstverständlich
wird applikative Programmierung vorwiegend dort eingesetzt werden, wo eine
gewisse algorithmische Komplexität gegeben ist, damit die Vorteile voll zum
Tragen kommen; gerade für solche Aufgaben zeigt die objektorientierte Programmierung ohnehin Schwächen. Die Faktorisierung eines Programms erfolgt
in dieser Kombination meist wie in der objektorientierten Programmierung, sodass applikative Programmteile nur Ergänzungen sind, die den Charakter der
objektorientierten Programmierung kaum ändern.
2.4 Parallele Programmierung
Im Gegensatz zur prozeduralen, funktionalen und objektorientierten Programmierung liegt der Fokus in der parallelen Programmierung nicht auf einer bestimmten Sammlung an Werkzeugen, sondern in einer sehr konkreten Zielsetzung: Eine (meist auf einer großen Datenmenge beruhende) nicht-triviale Aufgabe soll unter Einbeziehung mehrerer oder vieler gleichzeitig arbeitender Recheneinheiten so schnell wie möglich gelöst werden. Zur Erreichung des Ziels ist ein
gutes Verständnis der verschiedenen Formen der Parallelverarbeitung nötig, von
der Ebene der Hardware über die Betriebssystem- und Programmiersprachunterstützung bis zu typischen Programmiertechniken. Insgesamt stehen durch die
vielen Ausprägungen der Parallelität sehr viele Werkzeuge zur Verfügung, aber
die Werkzeugkiste, die alle in einem bestimmten Projekt eingesetzten Werkzeuge enthält, bleibt kleiner. Das heißt, viele Werkzeuge kommen nicht gemeinsam,
sondern alternativ zum Einsatz.
2.4.1 Parallelität und Ressourcenverbrauch
Bringen wir zunächst mit wenig Aufwand Parallelität in ein Programm. Im
Beispielprogramm zur applikativen Programmierung in Abschnitt 2.3.2 ersetzen wir den Ausdruck ins.stream() durch ins.parallelStream() und Java
sorgt dafür, dass unterschiedliche Stromabschnitte von mehreren Recheneinheiten gleichzeitig bearbeitet werden, konkret Dateien mit Bewertungsdaten gleichzeitig eingelesen werden. Java-8-Streams sind für die Parallelverarbeitung ausgelegt. Durch Ausprobieren werden wir vielleicht (abhängig von vielen Details)
feststellen, dass das parallele Programm trotz höheren Ressourcenverbrauchs
(Speicher- und Prozessorauslastung) langsamer läuft als das sequentielle. Das
Ziel der parallelen Programmierung wird damit in vielen Fällen verfehlt.
Das bringt uns zu einem wesentlichen Grundsatz der Parallelprogrammierung: Die Einführung von Parallelität, gleichgültig in welcher Form, erhöht auf
jeden Fall den Ressourcenverbrauch. Größerer Ressourcenverbrauch wird sich
100
2.4 Parallele Programmierung
häufig in längerer Laufzeit niederschlagen. Nur wenn es gelingt, ausreichend
viel Rechenarbeit so effizient auf die vorhandenen Recheneinheiten zu verteilen,
dass dies den zusätzlichen Ressourcenverbrauch kompensiert, wird die Laufzeit
verkürzt. Es ist aber immer so, dass alle verwendeten Recheneinheiten zusammengenommen mehr zu tun haben als in einem sequentiellen Programm. Wir
müssen deshalb ausreichend viel parallelisierbare Rechenarbeit finden, um die
Recheneinheiten auszulasten und zugleich verstehen, woher der zusätzliche Ressourcenverbrauch kommt und wie wir ihn klein halten können.
Gehen wir zunächst zur Vereinfachung davon aus, dass wir auf einem Rechner mit einem Prozessor mit mehreren Prozessorkernen arbeiten. Das ist heute
üblich, am Smartphone genauso wie am Laptop und PC. Jeder Prozessorkern
hat eine kleine Menge eigener Register und kann eine eigene sequentielle Abfolge von Befehlen abarbeiten. Alle Prozessorkerne teilen sich einen gemeinsamen
Speicher, auf den in der Regel über mehrere Ebenen an Cache zugegriffen wird.
Parallelverarbeitung heißt in diesem Fall, dass wir jedem freien Prozessorkern
Aufgaben zuteilen, die unabhängig von Aufgaben auf anderen Kernen bearbeitet werden können. Mit folgenden Schwierigkeiten müssen wir dabei umgehen:
• Es ist einiges zu tun, um einen Prozessorkern von einer Aufgabe auf eine
andere Aufgabe umzuschalten. Die aktuellen Registerwerte müssen abgespeichert und neue Registerwerte geladen werden. Außerdem müssen
Cache-Inhalte für ungültig erklärt und frisch aus dem langsameren Speicher geladen werden. Das dauert alles zusammen nicht besonders lange,
aber lange genug, um ein Faktor zu sein, der zu berücksichtigen ist. Wir
müssen dafür sorgen, dass die bearbeiteten Aufgaben groß genug sind, um
die Anzahl der Umschaltvorgänge klein zu halten.
• Die Prozessorkerne greifen alle auf den gleichen Speicher zu. Trotzdem
müssen wir dafür sorgen, dass unterschiedliche Aufgaben nur auf unterschiedliche Speicherbereiche zugreifen, da sich die Berechnungen andernfalls gegenseitig in die Quere kommen und keine vernünftigen Ergebnisse
liefern. Wir sprechen von einer Race-Condition, wenn Ergebnisse von Berechnungen davon abhängen, wie schnell einzelne parallele Programmteile
ausgeführt werden. Race-Conditions sind unbedingt zu vermeiden.
• Wenn Abhängigkeiten zwischen einzelnen Aufgaben bestehen, indem die
eine Aufgabe von Daten abhängt, die von der anderen Aufgabe produziert
werden, oder wenn zwei Aufgaben auf gleiche Speicherbereiche (nicht nur
lesend) zugreifen, dann müssen diese Aufgaben miteinander synchronisiert werden. Jede Form der Synchronisation bedeutet einen Verlust an
sinnvoller Parallelabarbeitung, weil die Aufgaben nicht mehr gleichzeitig ausführbar sind.2 Wir müssen dafür sorgen, dass die Abhängigkeiten
zwischen den Aufgaben so klein wie möglich bleiben.
2Es gibt optimistische Formen der Synchronisation, bei denen die Aufgaben so ausgeführt
werden, als ob sie unabhängig wären. Erst nach den Berechnungen wird geprüft, ob es
durch Abhängigkeiten zu Problemen gekommen ist. In diesem Fall müssen Ergebnisse verworfen und Berechnungen erneut durchgeführt werden. Bei diesen Formen der Synchronisation haben wir zwar Recheneinheiten ausgelastet, dabei aber mit einer bestimmten
101
2 Etablierte Denkmuster und Werkzeugkisten
• Oft haben wir einen Vorrat an Aufgaben, die mehr oder weniger unabhängig voneinander abarbeitbar sind. Wir brauchen einen Algorithmus,
den Scheduler, der die einzelnen Aufgaben den verfügbaren Prozessorkernen zuordnet. Dabei müssen Synchronisationsbedingungen berücksichtigt
werden. Die verwendete Scheduling-Strategie kann durch Vorziehen wichtiger Arbeiten einen großen Einfluss auf die Gesamteffizienz haben.
• Manchmal ist Synchronisation unvermeidbar. Schlecht gewählte Synchronisationsbedingungen und Scheduling-Strategien können (häufig in Kombination) dazu führen, dass sich der Fortschritt in den Berechnungen
stark verlangsamt oder sogar ganz zum Erliegen kommt. Wir sprechen
von Liveness-Problemen. Gefürchtet sind
Starvation, wobei eine für den Berechnungsfortschritt wichtige Aufgabe
nicht oder nicht in ausreichendem Umfang zur Ausführung gelangt
(also „verhungert“), während andere, für den Fortschritt weniger
wichtige Aufgaben zu viele Ressourcen bekommen.
Deadlock, wobei zwei oder mehr Aufgaben sich durch Synchronisation gegenseitig blockieren, weil sie von anderen Aufgaben benötigte Ressourcen nicht freigeben und gleichzeitig auf die Freigabe von Ressourcen durch andere Aufgaben warten.
Livelock, wobei mehrere Aufgaben zwar fleißig „Scheinarbeiten“ leisten,
aber keinen inhaltlichen Fortschritt erzielen, häufig weil sie sich gegenseitig den Zugriff auf benötigte Ressourcen wegnehmen.
• Die größte Schwierigkeit besteht darin, ein Programm so in einzelne Aufgaben passender Größe zu zerlegen, dass diese Aufgaben nicht oder kaum
voneinander abhängen. Für manche Programme ist das relativ einfach
möglich, vor allem wenn viele Daten unabhängig voneinander verarbeitet
werden sollen und jeder Bearbeitungsschritt hinreichend groß ist. Für andere Programme ist das praktisch unmöglich. Häufig ist nicht das ganze
Programm gut parallelisierbar, sondern nur einzelne Teile davon bzw. einzelne hintereinander ablaufende Phasen. Das ist unproblematisch, solange
der weitaus größte Berechnungsaufwand in den gut parallelisierbaren Teilen steckt. Es ist allerdings zu bedenken, dass die sequenziellen, also nicht
parallelisierten Programmteile die Laufzeit im Wesentlichen bestimmen,
weil während der sequenziellen Phasen nur einer der Prozessorkerne zum
Programmfortschritt beitragen kann.
• Idealerweise werden alle Teile des Systems gleichmäßig gut ausgelastet.
Meist ergibt sich jedoch irgendwo ein Engpass (Bottleneck). Manchmal
gibt es nicht genug Aufgaben, um alle Prozessorkerne auszulasten, dann
gibt es wieder zu viele davon, sodass deren Verwaltung einen größeren
Aufwand nach sich zieht. Manchmal muss auf so viele Daten gleichzeitig
Wahrscheinlichkeit keine sinnvolle Arbeit geleistet, sodass Abhängigkeiten einen Verlust
an „sinnvoller“ Parallelberechnung bedeuten.
102
2.4 Parallele Programmierung
zugegriffen werden, dass die Caches überlastet sind und die langsamen
Zugriffe auf den Hauptspeicher zum Engpass werden, wodurch die Prozessorkerne viel Zeit mit Warten verbringen. Manchmal kann das Netzwerk
oder die Festplatte nicht schnell genug Daten liefern oder übernehmen.
• Das von uns betrachtete Programm existiert nicht alleine auf dem Rechner. Wenn wir das Programm optimal parallelisieren, sodass es alle Teile
des Systems bestmöglich auslastet, wird das System wahrscheinlich überlastet sein, wenn andere Programme unvorhergesehen gleichzeitig laufen.
• Wir dürfen nicht vergessen, dass der Umgang mit diesen Schwierigkeiten
einen erheblichen Programmieraufwand verursacht. Der Aufwand für das
Erstellen, Testen und Warten paralleler Programme kann um ein Vielfaches größer sein als für entsprechende sequenzielle Programme.
Im eingangs gebrachten Beispiel wird Parallelität durch parallelStream so
genutzt, dass die betroffenen Daten nicht voneinander abhängen und für jedes
Element im Strom relativ viel zu tun ist. Aber nur ein Teil der gesamten Aufgabe
kann davon profitieren. Es kann trotz geschickter Auswahl sein, dass die parallel
ablaufenden Aufgaben zu klein sind, um den Mehraufwand in der Verwaltung
kompensieren zu können, oder die Eingabe wird zum Engpass. Abhängig von
der Hardware und dem Umfang der Daten kann die Laufzeit kürzer oder länger
sein als in der nicht-parallelen Variante.
Das Ziel der Parallelisierung ist immer die Verkürzung der Laufzeit. Solange
die angestrebte Programmeffizienz durch geeignete Wahl der eingesetzten Algorithmen erreicht werden kann, ist die Parallelisierung nicht sinnvoll. Wenn
ein Programm wegen nicht ausreichender Effizienz parallelisiert werden muss,
ist die Auswahl der Algorithmen neu zu überdenken. Der für ein sequenzielles
Programm ideale Algorithmus muss nicht auch für die parallele Variante ideal
sein. Im parallelen Programm ist es wesentlich, eine Aufteilung in ausreichend
viele unabhängige Aufgaben geeigneter Größe zu finden. Es kann vorteilhaft
sein, gleiche Berechnungsschritte in den einzelnen Aufgaben zu wiederholen,
wenn die Aufgaben dadurch unabhängig voneinander werden. Aber die Anzahl
an Prozessorkernen ist beschränkt. Daher ist kein beliebig großer Mehraufwand
in den Berechnungen durch Parallelisierung kompensierbar.
2.4.2 Formen der Parallelität
Ein Prozessor mit mehreren Kernen stellt nur eine von vielen Formen der Parallelität dar. Auf der Ebene der Hardware begegnen wir häufig folgenden Formen:
• Ein Prozessorkern arbeitet einen sequenziellen Strom an Maschinenbefehlen ab. Aktuelle Prozessorkerne können mehrere Befehle gleichzeitig ausführen. Bei einer VLIW-Architektur (Very Long Instruction Word) sorgt
der Compiler für parallel ausführbare Befehlssequenzen. Bei einer Superscalar-Architektur analysiert die Hardware aufeinander folgende Befehle
dahingehend, ob sie voneinander abhängen, also beispielsweise ein Befehl
103
2 Etablierte Denkmuster und Werkzeugkisten
Daten produziert, die ein anderer zu seiner Durchführung benötigt. Unabhängige Befehle können parallel ausgeführt werden. Wenn die Ausführung
komplexerer Befehle mehrere Prozessorzyklen dauert, kann die Wartezeit
mit der Ausführung nachfolgender Befehle gefüllt werden, solange die Befehle unabhängig sind. Teilweise werden Befehle sogar dann vorgezogen,
wenn die Unabhängigkeit nicht im Vorhinein garantiert ist; wird später
festgestellt, dass Abhängigkeiten zu Fehlern geführt haben, müssen die
fehlerhaften Berechnungen wiederholt werden (Spekulative Ausführung).
Schön daran ist, dass sich der Compiler (etwa durch Umreihung von Befehlen) und die Hardware ohne unser Zutun um diese Formen der Parallelausführung kümmern. Wir sprechen von impliziter Parallelität.
• Ein Prozessor mit mehreren Kernen ist eine Form von MIMD-Parallelität
(Multiple-Instructions-Multiple-Data). Es gibt mehrere sequenzielle Ströme von Befehlen, die unabhängig voneinander auf unterschiedlichen Daten
arbeiten. Mehrere Kerne auf einem Prozessor teilen sich in der Regel eine
gemeinsame Schnittstelle zum Hauptspeicher und gemeinsame Caches.
Diese gemeinsame Schnittstelle kann leicht zu einem Engpass werden,
weshalb die Anzahl der Kerne pro Prozessor recht klein, häufig einstellig
ist. Es ist auch möglich, mehrere Prozessoren (mit jeweils mehreren Kernen) zu einer Einheit zusammenzuschließen und auf einen gemeinsamen
Speicher zugreifen zu lassen. Wir sprechen dann von Symmetric SharedMemory Multiprocessors (SMP) oder Uniform-Memory-Access (UMA).
Die Komplexität der Schnittstelle zum gemeinsamen Speicher steigt mit
der Anzahl an Prozessoren rasch an, weil Vorkehrungen dagegen getroffen werden müssen, dass die Schnittstelle zum Engpass wird. Außerdem
müssen die Caches auf den unterschiedlichen Prozessoren konsistent gehalten werden. Obwohl die in Abschnitt 2.4.1 genannten Schwierigkeiten
gelöst werden müssen, ist die SMP-Programmierung eine der einfachsten
Formen der parallelen Programmierung.
• Wir sprechen von Distributed-Shared-Memory (DSM) oder Non-UniformMemory-Access (NUMA) wenn jeder Prozessor seinen eigenen Speicher
hat, aber alle Prozessoren und ihre Speicher über ein Verbindungsnetzwerk miteinander verbunden sind. Alle Speicher liegen in einem gemeinsamen Adressraum. Der wesentliche Unterschied zu SMP besteht darin,
dass Speicher, der zum eigenen Prozessor gehört, effizienter zugreifbar ist
als Speicher, der zu einem anderen gehört. Das ist in der Programmierung
zusätzlich zu beachten. Programmausführungen können nicht ohne Effizienzverlust von einem Prozessor zu einem anderen verschoben werden.
• Wenn mehrere oder viele Rechner ohne gemeinsamen Adressraum für
Speicherzugriffe eingesetzt werden, geht es meist um verteilte Programmierung. Die Kommunikation über ein vergleichsweise langsames Netzwerk ist dabei in der Regel ein Engpass, dar die parallele Programmierung
erschwert. Kürzestmögliche Ausführungszeiten stehen für die meisten Programme auf Rechnernetzwerken nicht zentral im Fokus, eher das Verkraf104
2.4 Parallele Programmierung
ten einer hohen Last an Arbeitsaufträgen. Es wird also darauf geachtet,
jeden Auftrag mit möglichst wenig Ressourcen zu bearbeiten, nicht darauf, alle vorhandenen Ressourcen zur Erreichung einer möglichst kurzen
Antwortzeit einzusetzen. Für manche Problemstellungen mit sehr hohem
Rechenaufwand bei gleichzeitig geringem Kommunikationsaufwand sind
Rechnernetzwerke auch für die parallele Programmierung gut geeignet.
In diesem Fall werden alle verfügbaren Ressourcen so eingesetzt, dass in
möglichst kurzer Zeit ein Ergebnis erzielt wird, ohne auf einen sparsamen
Umgang mit Ressourcen zu achten.
• Für Datenparallelität (viele Daten, die auf gleiche Art zu bearbeiten sind)
eignen sich SIMD-Instruktionen (Single-Instruction-Multiple-Data), wobei eine einzige Instruktion gleichzeitig auf ein ganzes Array an Daten
angewandt wird. Typischerweise wird eine Instruktion auf etwa 128 bis
512 oder mehr Bits gleichzeitig angewandt, was klarerweise effizienter ist,
als sie nur auf 64 Bits anzuwenden. Die Bits sind zu Worten mit jeweils 8,
16 oder 32 Bits zusammengefasst. Um bedingte Anweisungen auszuführen, ist häufig wählbar, auf welchen Worten eine Instruktion wirken soll
und auf welchen nicht. SIMD-Instruktionen kommen in Vektoreinheiten
vieler üblicher Prozessoren, vor allem aber in GPUs (Graphikprozessoren)
zum Einsatz, weil sie sich ideal zum Bearbeiten von Bilddaten eignen. Eine
GPU ist zwar langsamer getaktet als eine CPU (Hauptprozessor), enthält
aber viel mehr Prozessorkerne, von denen jeder einfacher als in der CPU
ist, aber SIMD-Anweisungen ausführt. Die Bandbreite für Speicherzugriffe
muss für eine GPU deutlich größer sein als für eine CPU, damit die vielen
parallelen Einheiten versorgt werden können. Während CPUs dafür ausgelegt sind, jeden Befehl so rasch wie möglich abzuschließen (kurze und
häufig unsichtbare Pipelines) und Wartezeiten durch Vorziehen nachfolgender Befehle nach Möglichkeit zu füllen, sind Pipelines auf GPUs meist
länger und sichtbar, das heißt, die durch Ausführung eines Befehls erzeugten Daten stehen erst einige Zyklen später für die Weiterverarbeitung zur
Verfügung. In der Zwischenzeit müssen davon unabhängige Befehle ausgeführt werden, in jedem Zyklus einer. Manche GPUs sind als billige,
aber dennoch recht leistungsfähige Parallelrechner einsetzbar. Bei anderen GPUs ist das schwierig, weil viele Kerne auf ganz bestimmte, nur für
die Bildverarbeitung notwendige Aufgaben spezialisiert sind.
Unter einem Prozess3 verstehen wir die Ausführung eines Programms auf einem Rechner gesteuert durch ein Betriebssystem. Häufig wird jede parallel zu
3Das ist ein vielfach überladener Begriff. Wenn wir umgangssprachliche Bedeutungen und jene in anderen technischen Bereichen und im Wirtschafts- und Rechtswesen ausklammern,
bleibt noch die Bedeutung im Softwareengineering, wo ein Prozess die Vorgehensweise
zur Steuerung der Softwareentwicklung von der Konzeption bis zum Ende des SoftwareLebenszykluses ist. Sogar im Sinn einer Ausführung eines Programmstücks ist der Begriff
überladen. Wir verwenden Prozess nur zur Bezeichnung der Ausführung eines Programms
durch ein Betriebssystem, nicht die Ausführung eines Ausführungsstrangs (Thread) innerhalb eines Programms. Diverse vor allem theoretische Modelle (etwa die in Abschnitt 1.1.1
erwähnten Prozesskalküle) verwenden diesen Begriff auch für Ausführungsstränge.
105
2 Etablierte Denkmuster und Werkzeugkisten
lösende Aufgabe in Form eines Programms beschrieben, natürlich viele Aufgaben durch das gleiche Programm, jedoch auf jeweils andere Daten angewandt.
Das Betriebssystem startet für jede Aufgabe einen eigenen Prozess. Jeder Prozess bekommt eine eigene Ablaufumgebung, die alle für die Ausführung nötigen
Ressourcen bereitstellt und den Prozess gleichzeitig vor unerwünschten Einflüssen durch andere Prozesse abschirmt. Insbesondere ist kein Zugriff auf Speichersegmente anderer Prozesse möglich, da jeder Prozess nur seinen eigenen
virtuellen Speicher sieht, der durch eine Memory-Management-Unit (MMU)
im Hintergrund auf den von allen Prozessen gemeinsam verwendeten physikalischen Speicher abgebildet wird. Virtuelle und physikalische Speicheradressen
unterscheiden sich voneinander. Die MMU sorgt dafür, dass jeder Prozess immer eine in sich konsistente Sichtweise auf seinen virtuellen Speicher bekommt,
obwohl im Hintergrund möglicherweise bei Speichermangel ganze Seiten vom
Hauptspeicher in einen externen Speicher ausgelagert und später wieder an einer anderen physikalischen Adresse aus dem externen Speicher geladen werden.
Die starke Abschottung der Prozesse voneinander hat auch Nachteile, weil oft
mehrere Prozesse auf gemeinsame Daten im Speicher zugreifen oder Daten von
einem Prozess zum anderen weiterreichen müssen, voneinander abhängige Prozesse aufeinander warten und diesbezügliche Information austauschen müssen
und die gemeinsame Nutzung von Systemressourcen (etwa Ein- und Ausgabe)
koordiniert werden muss. Wir sprechen von Interprozesskommunikation, wenn
es um den Austausch von Daten zwischen Prozessen geht. Dafür kommen einige
Verfahren häufig zum Einsatz:
• Am bekanntesten ist der Datenaustausch über das Schreiben und Lesen von Dateien. Die Synchronisation kann z. B. durch das Anlegen und
Löschen von Dateien erfolgen. Betriebssysteme bieten zur Vereinfachung
auch spezialisierte Konzepte (etwa Locks) zur Synchronisation an.
• Dateien werden (nicht nur in Java) über Ein- und Ausgabe-Ströme (nicht
zu verwechseln mit Java-8-Streams) gelesen und geschrieben. Es spielt
dafür keine Rolle, woher ein Strom kommt oder wohin er geht. Eine Pipeline nutzt das aus: Das Betriebssystem verknüpft einen Ausgabe-Strom
mit einem Eingabe-Strom, sodass die Daten ohne Umweg über eine Datei weitergereicht werden. Synchronisiert wird dadurch, dass beim Lesen
so lange gewartet wird, bis die Daten auf der anderen Seite geschrieben
wurden. In Linux können Pipelines bei Programmaufrufen sehr einfach angelegt werden. Z. B. startet a|b neue Prozesse für die Programme a und
b gleichzeitig und verbindet die Standardausgabe von a mit der Standardeingabe von b. Neue Prozesse sind auch von einem bestehenden Prozess
aus erzeugbar und dabei können Verbindungen über Pipelines aufgebaut
werden. Aber über anonyme Pipelines ist es nicht möglich, im Nachhinein Verbindungen zwischen schon bestehenden Prozessen aufzubauen. Für
diesen Zweck gibt es benannte Pipelines, die wie Dateien angesprochen
und geöffnet werden können.
• Etwas komplexer ist die Kommunikation über Sockets. Erzeugt und geöff106
2.4 Parallele Programmierung
net werden Sockets, also die Endpunkte der Kommunikationsverbindungen, durch Aufrufe entsprechender Betriebssystemfunktionen. Auch zum
Lesen und Schreiben stehen Betriebssystemfunktionen bereit. Sockets sind
vor allem dann geeignet, wenn die darüber kommunizierenden Prozesse
auf verschiedenen Rechnern liegen, es also keinen gemeinsamen physikalischen Speicher gibt. Häufig werden Sockets zusammen mit TCP, das ist
ein relativ zuverlässiges Netzwerkprotokoll, sowie IP zur Adressierung im
Internet eingesetzt; für manche Aufgaben ist aber UDP, ein einfaches, effizientes Netzwerkprotokoll besser geeignet. Im Bereich des Internets ist
die Verwendung von Sockets heute praktisch unumgänglich.
• Auf einem Rechner mit gemeinsamem Speicher ist Shared-Memory die
wahrscheinlich effizienteste Form der Interprozesskommunikation. Wie der
Name schon sagt, bekommen dabei mehrere Prozesse Zugang zum gleichen (physikalischen) Speicherbereich. Ein Prozess reserviert durch Aufruf einer entsprechenden Betriebssystemfunktion Shared-Memory, andere
Prozesse können danach Zugriff auf dieses Shared-Memory nehmen. Ein
guter Teil der Arbeit wird von der MMU erledigt, wodurch der Mechanismus im Detail jedoch stark vom Betriebssystem und der Hardware
abhängt und wenig portabel ist. Shared-Memory sorgt nur für den gemeinsamen Speicher, nicht für die Synchronisation der Zugriffe darauf.
Dafür müssen andere Mechanismen, etwa Semaphore eingesetzt werden.
Viele Programmiersprachen unterstützen eigene Mechanismen zur Parallelausführung, die bei der Erzeugung und Verwaltung weniger Aufwand verursachen als Prozesse. Java bietet Threads an. Im Gegensatz zu einem Prozess
hat ein Thread keinen eigenen virtuellen Speicher, sondern greift auf den gleichen Speicher zu wie das restliche Programm. Threads sind nicht voneinander
abgeschirmt. Daher sind auch keine Mechanismen zur Interprozesskommunikation nötig. Kommuniziert wird über den gemeinsamen Speicher im gemeinsamen Adressraum. Besonderer Wert muss auf die Synchronisation gelegt werden, weil natürliche Formen der Synchronisation wie durch Dateien, Pipelines
oder Sockets für Threads nicht zur Verfügung stehen. Dennoch sind Dateien,
Pipelines und Sockets auch in Java verfügbar, weil die Ausführung eines JavaProgramms durch einen Java-Interpreter auch ein Prozess ist, der möglicherweise mit anderen Prozessen kommunizieren muss. Threads bieten zusätzliche
Parallelausführungen innerhalb von Prozessen, ersetzen Prozesse aber nicht.
Der elementarste Synchronisationsmechanismus ist das Semaphor. Das ist
im Wesentlichen eine nicht-negative ganzzahlige Variable s, auf der es nur zwei
Operationen gibt, die historisch P und V heißen (holländische Kürzel), in Programmiersprachen und Betriebssystemen aber verschiedene Namen haben können. P(s) verringert den Wert von s um 1, wenn s dabei nicht negativ wird,
andernfalls wird der Aufrufer von P(s) (ein Prozess oder Thread) suspendiert.
V (s) weckt einen suspendierten Prozess oder Thread wieder auf, falls ein solcher
existiert, andernfalls wird der Wert von s um 1 erhöht. Einsatzzwecke sind vielfältig, am bekanntesten ist der Einsatz zur Erreichung von Mutual-Exclusion,
107
2 Etablierte Denkmuster und Werkzeugkisten
also um zu gewährleisten, dass ein bestimmter Programmtext, der kritische Abschnitt, niemals von zwei Prozessen oder Threads gleichzeitig ausgeführt wird.
Bei Erreichen des kritischen Abschnitts wird P(s) ausgeführt, an dessen Ende
V (s), wobei s zuvor mit 1 initialisiert wurde. So müssen stets alle außer einem
Prozess oder Thread am Eingang des kritischen Abschnitts warten, bis der eine
Prozess oder Thread diesen wieder verlässt. Jede komplexere Form der Synchronisation lässt sich durch ein Semaphor oder das Zusammenspiel mehrerer
Semaphore beschreiben. Java verwendet für die Synchronisation von Threads
vorwiegend das Monitor-Konzept, das in Abschnitt 2.5 beschrieben ist.
Von großer praktischer Bedeutung ist die verwendete Scheduling-Strategie.
Jedes Betriebssystem hat eine solche Strategie für Prozesse, meist mit der Möglichkeit durch Ändern von Prioritäten darauf Einfluss zu nehmen, wie viel
Rechenzeit dem Prozess zugestanden werden soll. Ähnliches gibt es auch für
Threads in Programmiersprachen. Da Betriebssysteme und Programmiersprachen praktisch nichts über die Charakteristika der Prozesse und Threads wissen,
sind diese Strategien kaum auf den Anwendungszweck abgestimmt. Häufig verlassen sich Programme nicht nur darauf. In vielen Fällen wird einfach eine für
den Einsatzzweck und die verfügbare Anzahl an Prozessorkernen optimale Anzahl an Prozessen oder Threads erzeugt, die von ihnen ausgeführten Aufgaben
werden ihnen dynamisch je nach Bedarf innerhalb des Programms zugewiesen.
Beispielsweise wird ein zentraler Pool an Aufgaben verwaltet; jeder Prozess oder
Thread, der gerade nichts zu tun hat, holt sich eine Aufgabe aus dem Pool und
erledigt diese. In Wahrheit ist es etwas komplexer, weil ein zentraler Pool leicht
zu einem Engpass werden könnte, aber die Idee bleibt gleich. Die Auswahl der
nächsten zu erledigenden Aufgabe liegt somit in unserer Hand.
2.4.3 Streben nach Unabhängigkeit
Der Erfolg in der parallelen Programmierung lässt sich im Wesentlichen an einer Zahl ablesen: dem Speedup. Gemessen wird er durch Sp = T1/Tp, wobei Sp
für den Speedup bei Lösung einer Aufgabe auf einem System mit p Prozessoren
steht, Tp für die dafür aufgewendete Zeit und T1 für die Zeit zur Lösung der
gleichen Aufgabe auf einem System mit nur einem Prozessor. Auf Basis dieser
Definition lassen sich Diagramme zeichnen, die den Speedup für verschiedene
Anzahlen an Prozessoren veranschaulichen. Durch die vielen verschiedenen Formen der Parallelität dürfen wir die Definition von Speedup nicht immer wörtlich
nehmen, sondern müssen sie auf die Art der Parallelität beziehen, etwa statt auf
die Anzahl der Prozessoren auf die Anzahl der Prozessorkerne oder auf Wortlängen bei SIMD-Parallelität. Jedenfalls kann der Speedup aus theoretischen
Überlegungen niemals größer als p sein. Tatsächlich bleibt er immer kleiner als
p, weil es in jedem Programm auch Teile gibt, die nicht parallelisierbar sind.
Gar nicht so selten wird stolz von erreichten Speedups berichtet, die p übersteigen. Dahinter stecken immer unzulässige Vergleiche, Tp bezieht sich etwa
nicht auf die gleiche Aufgabe wie T1, oder der Speedup gilt nur für bestimmte
Daten, nicht für andere (etwa, wenn in großen Datenmengen aufgeteilt auf die
Prozessoren nach einem bestimmten Wert gesucht wird, der bei einem einzigen
108
2.4 Parallele Programmierung
Prozessor erst recht weit hinten gefunden wird, bei einer Aufteilung auf eine
bestimmte Anzahl von Prozessoren von einem der Prozessoren vielleicht schon
im ersten Schritt). Eine faire Messung des Speedups ist aus solchen Gründen
schwierig. Auch verantwortungsvoll gemessene Zahlen spiegeln nicht die ganze
Wahrheit wider. Meist sind nur Durchschnittswerte aus vielen Messungen unter
Variation aller möglichen Einflussgrößen aussagekräftig.
Die wesentliche Frage bei der Programmerstellung besteht darin, durch welche Vorgehensweise für eine gegebene Aufgabe der größtmögliche Speedup erreicht werden kann. Es gibt keine immer und überall anwendbare Vorgehensweise, die sicher zum Ziel führt. Die größten Erfolgsaussichten verspricht eine
Reduktion der Abhängigkeiten zwischen Daten. Es ist relativ problemlos möglich, dieselben Daten von verschiedenen Prozessen gleichzeitig lesen zu lassen,
solange garantiert werden kann, dass diese Daten nicht überschrieben werden.
Die Ergebnisse von Berechnungen müssen in der Regel irgendwo abgelegt, also
geschrieben werden. Für jedes erzeugte Datenelement muss ein Platz gefunden
werden, auf dem kein anderes Datenelement abgelegt wird, ohne sich für die
Findung des Platzes mit anderen Threads oder Prozessen koordinieren zu müssen. Sockets und Pipelines sind bestens dafür geeignete Mechanismen, SharedMemory und Dateiinhalte aber nicht. Insbesondere für die Koordination von
Threads steht im Wesentlichen nur gemeinsamer Speicher mit ausreichend guter
Effizienz zur Verfügung. In Java gibt es durch Java-8-Streams einen Mechanismus, der den Pipelines ähnliche Strukturen im Speicher nachbildet. Tatsächlich
werden die Schwierigkeiten bei Verwendung von Pipelines oder Java-8-Streams
nur auf eine andere Ebene verlagert, weil noch immer eine Organisationsstruktur gefunden werden muss, sodass jede Pipeline und jeder Strom nur an einer
Stelle gelesen und an einer anderen geschrieben wird.
Folgendes Beispiel soll demonstrieren, wie in den zu verarbeitenden Daten
Abhängigkeiten zwischen Lesen und Schreiben vermieden werden können. Alle
diesbezüglichen Überlegungen hängen stark von der Problemstellung ab und
können nicht verallgemeinert werden. Im Beispiel wollen wir zählen, wie viele
Primzahlen kleiner einer gegebenen Obergrenze MAX_NUM existieren. Algorithmisch bietet sich dafür das bekannte „Sieb des Eratosthenes“ an, bei dem jede
Zahl n versuchsweise durch alle kleineren Primzahlen bis zur Wurzel von n dividiert werden. Das Problem dabei: Wir müssen die Primzahlen in aufsteigender
Reihenfolge finden, wobei immer wieder auf schon gefundene Zahlen zugegriffen
werden muss. Es besteht also eine natürliche Abhängigkeit zwischen dem Schreiben und nachfolgenden Lesen gefundener Primzahlen. Glücklicherweise müssen
Primzahlen, die schon gefunden wurden, nachträglich nicht mehr geändert werden und die Wurzel aus n ist recht klein im Vergleich zu n. Diese Eigenschaften
machen wir uns zu Nutze: Wir geben die kleine Zahl an Primzahlen bis 16 vor
und berechnen daraus alle Primzahlen bis 256, weil die Wurzel aus 256 gleich
16 ist, sodass während dieser Berechnung nicht schon auf die neu berechneten
Primzahlen zugegriffen werden muss. Dann fassen wir die schon bekannten mit
den gefundenen Primzahlen zusammen. In der nächsten Phase berechnen wir
aus den Primzahlen bis 256 jene bis 256 ∗ 256, dann jene bis 256 ∗ 256 ∗ 256 ∗ 256
und so weiter. Nach wenigen Phasen sind alle long-Zahlen abgearbeitet.
109
2 Etablierte Denkmuster und Werkzeugkisten
import java.util.Arrays;
import java.util.stream.LongStream;
public class Par {
public static long MAX_NUM = 1L << 28; // Wert anpassbar
private static long[] prims = { 2L, 3L, 5L, 7L, 11L, 13L };
public static void main(String[] args) {
long low = 16L, high = 256L;
do {
int size = prims.length;
long[] nPrims = inRange(low, high);
prims = Arrays.copyOf(prims, size + nPrims.length);
System.arraycopy(nPrims,0,prims,size,nPrims.length);
low = high;
high = high * high;
if (high > MAX_NUM)
high = MAX_NUM;
} while (low < MAX_NUM);
System.out.println(prims.length);
}
private static long[] inRange(long low, long high) {
return LongStream.range(low, high).parallel()
.filter(Par::isPrime).toArray();
}
private static boolean isPrime(long n) {
long sqrt = (long) Math.sqrt(n);
return Arrays.stream(prims).takeWhile(v -> v <= sqrt)
.allMatch(v -> n % v != 0);
}
}
Für die eigentliche Parallelisierung des Programms wurde wenig Aufwand betrieben. Einzig und alleine in inRange wird auf einen Strom von long-Zahlen
zwischen low (inklusive) und high (exklusive) die Methode parallel() angewandt, die automatisch eine passende Zahl an Threads erzeugt, wovon jeder
einen Ausschnitt aus dem Wertebereich verarbeitet. Ein Aufruf von filter
lässt nur die Primzahlen durch, die von isPrime als solche identifiziert wurden und die danach in einem Array passender Größe abgelegt werden. In main
werden die einzelnen so entstandenen Arrays zusammenkopiert. Die Methode
isPrime ist selbst nicht parallel, sondern verwendet einen sequentiellen Strom
von Arrayinhalten (das sind die schon bekannten Primzahlen), beendet den
Strom wenn alle Primzahlen bis zur Wurzel von n abgearbeitet sind und gibt
true zurück, wenn sich n durch keine dieser Zahlen ohne Rest dividieren lässt.
Es ist häufig so wie im Beispiel, dass der Umgang mit Parallelität selbst nicht
schwer ist, diesen Teil können wir Werkzeugen überlassen. Die Suche nach einer geeigneten Zerlegung der Aufgabenstellung und der Daten in unabhängige
110
2.5 Nebenläufigkeit
Teile ist nicht so leicht automatisierbar. Natürlich ist das Beispielprogramm
noch nicht optimal. Zur Verbesserung könnten wir an vielen Schräubchen drehen (etwa andere Aufteilung der zu untersuchenden Wertebereiche), was die
Programmkomplexität aber vermutlich erhöhen würde.
In der Praxis wird Parallelprogrammierung vor allem dort eingesetzt, wo riesige Datenmengen mittels komplexer Algorithmen verarbeitet werden müssen.
Häufig kommen immer wieder gleiche oder ähnliche grundlegende Algorithmen zum Einsatz, die von bestimmten Strukturierungen der Daten ausgehen.
Beispielsweise beruhen viele technische oder physikalische Problemstellungen
auf der Lösung umfangreicher Gleichungssysteme mittels linearer Algebra. Für
solche Aufgaben wird überwiegend LAPACK eingesetzt, eine Bibliothek mit
fertigen Lösungen für zahlreiche Aufgaben der linearen Algebra. LAPACK beruht wiederum auf BLAS, einer für viele Hardwarearchitekturen und Sprachen
optimierten Bibliothek mit grundlegender Funktionalität für lineare Algebra.
In solchen Bibliotheken stecken über viele Jahrzehnte gesammelte Erfahrungen
in der Optimierung paralleler Algorithmen, die ständig an neue Entwicklungen
angepasst wurden. Für Aufgaben, für die diese Bibliotheken geeignet sind, wird
es ohne Zuhilfenahme von Bibliotheken nur mit sehr großem Aufwand und viel
Wissen gelingen können, eine ähnlich gute Effizienz zu erreichen. LAPACKProgrammierung gilt daher heute praktisch schon als Synonym für das Lösen
von Aufgaben im Bereich der linearen Algebra. Generell werden heute viele,
auch sehr komplexe Aufgaben in der parallelen Programmierung durch Verwendung spezialisierter und gut optimierter fertiger Bibliotheken erledigt.
2.5 Nebenläufigkeit
Es mag überraschen, dass Threads und die Synchronisation von Threads im
Abschnitt über parallele Programmierung nur am Rande erwähnt wurden. Tatsächlich kommen Threads vorwiegend im Bereich der nebenläufigen Programmierung (concurrent programming) zum Einsatz. Jeder Thread beschreibt einen
in sich logisch konsistenten Handlungsstrang. Meist laufen viele Handlungsstränge gleichzeitig ab. Die Anzahl der Threads richtet sich vorwiegend nach
dem Bedarf an Handlungssträngen, nicht nur nach der Zahl verfügbarer Recheneinheiten. Häufig wird die Ausführung eines Handlungsstrangs unterbrochen, etwa weil auf eine Eingabe, das Eintreffen von Daten von einem entfernten Rechner oder das Freiwerden geteilter Ressourcen gewartet werden muss.
Im Gegensatz zur parallelen Programmierung steht der rasche Abschluss einer
aufwändigen Rechenaufgabe nicht zentral im Fokus. Das heißt nicht, dass die
Programmeffizienz vernachlässigt wird, sondern nur, dass ein sparsamer Umgang mit Rechenzeit, Speicherverbrauch und anderen Ressourcen eher im Mittelpunkt steht als ein möglichst rasch vorliegendes Endergebnis. Wie z. B. bei
einem Betriebssystem gibt es häufig gar kein „Endergebnis“, sondern es wird
ständig etwas gemacht, etwa auf Aufträge gewartet, die abzuarbeiten sind. Beispielsweise wartet eine Telefonanlage auf Anrufe und leitet Datenpakete weiter,
viele gleichzeitig; daneben werden Abrechnungen erstellt und Zusatzdienste an111
2 Etablierte Denkmuster und Werkzeugkisten
geboten. Das Ziel besteht manchmal in einem möglichst großen Durchsatz, also
der Erledigung möglichst vieler Aufträge pro Zeiteinheit. Manchmal gibt es
Threads, deren einzige Aufgabe es ist, lange auf das Eintreten eines Ereignisses
zu warten, um dann eine kleine Aufgabe zu erledigen, etwa zu einer bestimmten
Uhrzeit einen Weckton erklingen zu lassen. Aber auch Simulationen zeitabhängiger Abläufe lassen sich damit gut realisieren. Es gibt also eine breite Palette
an Einsatzmöglichkeiten.
2.5.1 Threads und Mutual-Exclusion
Folgendes Beispiel setzt Nebenläufigkeit zur Simulation der Ausbreitung einer
Bakterienkultur in einer quadratischen Schale ein. Ein zweidimensionales Array stellt die Schale dar, in jedem Eintrag befindet sich ein Objekt vom Typ
State, das angibt, wie viel Nahrung ein Bakterium an dieser Stelle finden kann
(positive Zahl) bzw. ob sich an der Stelle schon ein Bakterium befindet und
den Platz gegen andere verteidigt. Jedes Bakterium wird durch einen Thread
dargestellt. Bakterien können sich abhängig vom vorhandenen Nahrungsvorrat
nach gewissen Wartezeiten vermehren und Nachbarfelder besiedeln. Nach einer
gewissen Zeit ohne Nahrung sterben sie, ebenso wenn sie ein Feld besiedeln
wollen, das schon besiedelt ist. Simpler Ausgabetext zeigt an, in welcher Reihenfolge Bakterien an welchen Stellen entstehen und sterben. In diesem Beispiel
dient Nebenläufigkeit dazu, mit durch einen Zufallszahlengenerator gewählten
Wartezeiten zwischen den Schritten im Leben eines Bakteriums umzugehen.
Zufall spielt an mehreren Stellen eine Rolle und bewirkt, dass jede Programmausführung zu anderen Simulationsergebnissen führt. Dennoch entstehen immer
wieder einander ähnliche Muster, die genauer untersucht werden können.
public class BactSim implements Runnable {
private static final int SIZE = 100;
private static final State[][] field = new State[SIZE][SIZE];
private int x, y, fitness;
public static void main(String[] args) {
for (int i = 0; i < SIZE; i++)
for (int j = 0; j < SIZE; j++)
field[i][j] = new State();
new Thread(new BactSim(SIZE/2, SIZE/2)).start();
}
private BactSim(int x, int y) {
this.x = x;
this.y = y;
fitness = field[x][y].occupy();
System.out.println(x + "," + y + ": " + fitness);
}
112
2.5 Nebenläufigkeit
public void run() {
while (fitness-- > -(int)(Math.random() * 10)) {
try { Thread.sleep((int)(Math.random() * 50));
} catch (InterruptedException e) { break; }
int dx, dy;
do { dx = (int) (Math.random() * 3) - 1;
dy = (int) (Math.random() * 3) - 1;
} while ((dx == 0 && dy == 0) || x+dx < 0 ||
x+dx >= SIZE || y+dy < 0 || y+dy >= SIZE);
if (fitness > 0)
new Thread(new BactSim(x + dx, y + dy)).start();
}
field[x][y].gone();
System.out.println(x + "," + y + ": gone");
}
}
class State {
private int food = (int) (Math.random() * 20);
public synchronized int occupy() {
int res = food;
food = -10;
return res;
}
public synchronized void gone() {
food = 0;
}
}
Die Klasse BactSim implementiert das Interface Runnable, wodurch die Methode run() in jedem Objekt von BactSim in einem eigenen Thread ausführbar
wird. Jedes Objekt von Thread stellt einen eigenen Thread dar. Der Konstruktor von Thread nimmt ein Objekt von Runnable als Parameter, in diesem Fall
ein Objekt von BactSim. Durch einen Aufruf von start() beginnt der Thread
zu laufen, der run() ausführt. In main() wird nach der Initialisierung des Arrays nur ein Thread (für ein Bakterium etwa in der Mitte der Schale) erzeugt
und gestartet. Der Konstruktor von BactSim ruft occupy() im entsprechenden
State-Objekt auf, um den Platz und die dort gefundene Nahrung in Besitz
zu nehmen. Innerhalb von run() werden von gut genährten Bakterien weitere
Threads auf Nachbarfeldern erzeugt und gestartet, sodass sich nach kurzer Zeit
meist viele gleichzeitig laufende Threads ergeben. Sind die Nahrungsvorräte
(zufallsabhängig) aufgebraucht, endet die Schleife in run() und der Arrayeintrag wird darüber informiert, dass der Platz nicht mehr besiedelt ist; mit dem
Ende von run() endet auch die Ausführung des entsprechenden Threads. In der
Schleife in run() wird Thread.sleep(...) ausgeführt, eine Methode, die den
aktuellen Thread für die gegebene Anzahl an Millisekunden (das ist nur ein gro113
2 Etablierte Denkmuster und Werkzeugkisten
ber Richtwert) pausieren lässt. Während des Wartens können andere Threads
ausgeführt werden. Es ist nicht ausgeschlossen, dass ein wartender Thread von
außen durch die Methode interrupt() beendet wird. In einem solchen Fall
wird eine InterruptedException ausgelöst, um dem beendeten Thread noch
die Möglichkeit zu geben, den Zustand aufzuräumen. Diese Ausnahme muss
behandelt werden, was sich im Beispiel wie aufgebrauchte Nahrung auswirkt.
Wenn mehrere Threads auf gleiche Variablen zugreifen oder sie sogar ändern,
ist Synchronisation notwendig, um Race-Conditions zu vermeiden. Solche Variablenzugriffe erfolgen im Beispiel in den Methoden occupy() und gone(), da
diese Methoden von mehreren Threads aus auch auf dem gleichen Objekt gleichzeitig aufrufbar sind. Diese Methoden sind als synchronized deklariert. Im
gleichen Objekt ist zu jedem Zeitpunkt höchstens eine synchronized-Methode
ausführbar; werden weitere Methoden aufgerufen, bevor die erste beendet ist,
werden die Threads, die hinter den weiteren Aufrufen stehen, so lange blockiert,
bis die erste und alle vorherigen Methoden auf diesem Objekt beendet sind. Java garantiert damit Mutual-Exclusion und schließt Parallelausführungen von
synchronized-Methoden auf gleichen Objekten aus. Die Threads bekommen
nacheinander exklusiven Zugriff auf das betrachtete Synchronisationsobjekt.
Mutual-Exclusion durch synchronized gibt es nur auf gleichen Objekten.
Das heißt, im Beispiel können mehrere Bakterien gleichzeitig unterschiedliche
Plätze in der Schale besetzen, ohne sich gegenseitig zu stören. Das geht, weil
jeder Arrayeintrag als Synchronisationsobjekt dient, nicht das gesamte Array.
Auch wenn der Schwerpunkt nicht auf kürzestmöglicher Laufzeit liegt, muss
darauf geachtet werden, dass sich Threads möglichst wenig gegenseitig behindern und die kritischen Abschnitte (also die Zeiten für die Ausführungen von
occupy() und gone()) möglichst kurz bleiben. Im Beispiel ist es zwar möglich,
aber relativ unwahrscheinlich, dass zwei Bakterien gleichzeitig auf den gleichen
Arrayeintrag zugreifen wollen. Das ist der Idealfall für Synchronisation. Noch
besser wäre es aber, auf Synchronisation gänzlich verzichten zu können.
Neben synchronized-Methoden unterstützt Java synchronized-Blöcke, bei denen wir das zu verwendende Synchronisationsobjekt explizit angeben. Beispielsweise könnte der Aufruf von occupy() so aussehen, wenn wir synchronized in
der Definition von occupy() weglassen:
synchronized (field[x][y]) { fitness = field[x][y].occupy(); }
Im Wesentlichen ist das eine syntaktische Variante mit gleicher Semantik. Durch
Verwendung dieser Variante können wir das Synchronisationsobjekt beeinflussen. Beispielsweise könnten wir statt field[x][y] das gesamte Array field
verwenden (natürlich konsistent, also auch für Aufrufe von gone()). Dadurch
würden sich viel mehr Aufrufe gegenseitig behindern, das wäre also nicht gut.
Von Vorteil wäre, dass wir das Array von State-Objekten durch ein int-Array
ersetzen und Aufrufe der Methoden und des Konstruktors von State direkt
in den Programmtext von BactSim verschieben könnten, eine Vereinfachung.
Für feingranulare Synchronisation brauchen wir jedoch State-Objekte, weil
synchronized als Synchronisationsobjekte nur Objekte, keine primitiven Datenelemente verwenden kann. In Java müssen wir den Zusatzaufwand der Ob114
2.5 Nebenläufigkeit
jekterzeugung in Kauf nehmen. Den Schreibaufwand könnten wir etwas reduzieren, indem wir statt State die vordefinierte Klasse AtomicInteger verwenden;
ein Aufruf von getAndSet(-10) wäre damit ein direkter Ersatz für einen Aufruf
von occupy() und set(0) für gone().
Synchronisation durch synchronized kann leicht zu Liveness-Problemen führen, insbesondere wenn gleichzeitig auf mehrere Synchronisationsobjekte zugegriffen wird. Nehmen wir an, dass ein Bakterium von einem Arrayeintrag auf
einen anderen wandern kann. In State fügen wir folgende Methode ein:
public synchronized int enter(State from) {
from.gone();
return occupy();
}
In der Schleife von run(), z. B. als else-Zweig der if-Anweisung rufen wir diese
Methode auf, um von field[x][y] auf field[x+dx][y+dy] zu wandern:
... = field[x+dx][y+dy].enter(field[x][y]);
Wahrscheinlich zeigt das geänderte Programm bei der Ausführung keine Auffälligkeiten. Dennoch haben wir einen potentiellen Deadlock eingebaut, der
sich sehr selten zeigt: enter(...) ruft gone() auf, wobei ein Thread sowohl
field[x+dx][y+dy] als auch das Nachbarfeld field[x][y] gleichzeitig als
Synchronisationsobjekt benötigt. Ein anderer Thread will ungefähr gleichzeitig
durch einen Aufruf von enter(...) von field[x+dx][y+dy] auf field[x][y]
wandern. Dabei kann es passieren, dass der eine Thread exklusiven Zugriff auf
field[x+dx][y+dy] bekommt, der andere auf field[x][y]. Nun benötigen
beide Threads für den Aufruf von gone() exklusiven Zugriff auf das jeweils andere Objekt, können diesen aber nicht bekommen, weil schon der jeweils andere
Thread exklusiven Zugriff auf dieses Objekt hat. Das ist ein Deadlock, bei dem
zwei Threads endlos darauf warten, dass der jeweils andere die Ausführung einer
Methode beendet. Ein Deadlock kann auch eine größere Zahl an über exklusive
Objektzugriffe zyklisch miteinander verbundenen Threads umfassen, die jeweils
auf das Ende des exklusiven Zugriffs durch den vorigen Thread warten.
Die Methode enter(...) ruft auch occupy() auf, etwas ausführlicher hingeschrieben this.occupy(). Dieser Aufruf ist problemlos, weil der Thread, der
enter(...) ausführt, ohnehin schon exklusiven Zugriff auf this hat und daher
occupy() direkt ausführen kann, ohne warten zu müssen.
Die größte Schwierigkeit besteht darin, mögliche Deadlock-Situationen zu erkennen. Ein hoher Abstraktionsgrad erschwert das gewaltig, weshalb nebenläufige Programmierung häufig auf prozeduraler Programmierung aufbaut, nicht auf
objektorientierter, obwohl objektorientierte Sprachen eingesetzt werden. Sobald
eine mögliche Deadlock-Situation erkannt ist, gibt es wenige Ansätze zu deren
Beseitigung. In obigem Beispiel ist das Problem einfach zu beseitigen, indem
kein Thread exklusiven Zugriff auf mehrere Objekte gleichzeitig bekommt, sondern nur hintereinander. Meist ist dieser einfache Ansatz nicht zielführend, weil
es eben inhaltlich unvermeidbar ist, gleichzeitig exklusiv auf mehrere Objekte
115
2 Etablierte Denkmuster und Werkzeugkisten
zuzugreifen. In solchen Fällen bietet es sich an, die Reihenfolge einzuschränken,
in der exklusiver Zugriff auf Objekte erlangt werden kann. Im Beispiel könnten
wir verlangen, dass field[u][v].enter[x][y] nur für u<x oder bei u==x für
v<y aufrufbar ist, sodass keine Zyklen in der Reihenfolge entstehen können.
Die Folgen dieser Einschränkung lassen sich etwa durch Bereitstellung einer
weiteren Methode lösen, in der die Bedeutungen von this und dem expliziten
Parameter from vertauscht sind und für deren Aufruf genau die umgekehrten Einschränkungen gelten. Ein solcher Ansatz hat in der Regel gravierende Auswirkungen auf die gesamte Organisation des Programms. Nebenläufige
Programmierung ist damit nicht nur eine Erweiterung eines prozeduralen oder
funktionalen Programmierstils, sondern bekommt einen eigenen Charakter, der
ganz wesentlich von den Notwendigkeiten zur Synchronisation bestimmt wird.
Livelocks sind oft noch schwieriger zu erkennen als Deadlocks. Häufig steht
„Livelock“ für „aktives Warten“, also Situationen, in denen viel Zeit damit
verbracht wird, immer wieder nachzufragen, ob eine bestimmte benötigte Ressource zur Verfügung steht. Damit können zyklische Abhängigkeiten wie bei
Deadlocks entstehen, die jedoch nicht auffallen, weil nicht gewartet, sondern
stets viel getan wird, allerdings ohne einen Fortschritt zu erzielen. Wegen der
Gefahr von Livelocks sollte auf aktives Warten generell verzichtet werden.
2.5.2 Warten bis es passiert
Häufig muss auf Ereignisse reagiert werden, die zu unvorhersehbaren Zeitpunkten eintreten. Das typische Beispiel dafür ist das Produzenten-KonsumentenProblem, bei dem einige „Produzenten“ wiederholt (meist zu nicht vorhersehbaren Zeitpunkten) Daten produzieren, die von einigen „Konsumenten“ verarbeitet werden. Der übliche Lösungsansatz besteht darin, die Kommunikation und
Synchronisation zwischen Produzenten und Konsumenten über einen Puffer beschränkter Größe zu organisieren. Die Beschränkung der Größe verhindert, dass
Produzenten ständig Daten produzieren, während Konsumenten „verhungern“
(Starvation). Produzenten und Konsumenten müssen aufeinander warten:
public class ProducerConsumer {
public static void main(String[] args) {
Buffer buffer = new Buffer();
for (char c = ’a’; c <= ’z’; c++) {
new Thread(new Producer("" + c, buffer)).start();
new Thread(new Consumer("" + c, buffer)).start();
}
}
}
class Buffer {
private final static int SIZE = 1 << 3;
private String[] buffer = new String[SIZE];
private int in, out;
116
2.5 Nebenläufigkeit
public synchronized void put(String v) {
while (buffer[in] != null)
try { wait(); }
catch (InterruptedException e) { return; }
notifyAll();
buffer[in] = v;
in = (in + 1) & (SIZE - 1);
}
public synchronized String get() {
while (buffer[out] == null)
try { wait(); }
catch (InterruptedException e) { return null; }
notifyAll();
String res = buffer[out];
buffer[out] = null;
out = (out + 1) & (SIZE - 1);
return res;
}
}
class Producer implements Runnable {
private String id;
private Buffer buffer;
public Producer(String id, Buffer buffer) {
this.id = id;
this.buffer = buffer;
}
public void run() {
for (int i = 1; i <= 100; i++) buffer.put(id + i);
}
}
class Consumer implements Runnable {
private String id;
private Buffer buffer;
public Consumer(String id, Buffer buffer) {
this.id = id;
this.buffer = buffer;
}
public void run() {
for (int i = 1; i <= 100; i++)
System.out.println(id + ": " + buffer.get());
}
}
117
2 Etablierte Denkmuster und Werkzeugkisten
Produzent und Konsument sind irgendwelche Threads (entsprechende Klassen
durch andere ersetzbar), die entweder über put(...) Daten an den gemeinsamen Puffer übergeben oder über get() Daten aus dem Puffer holen. Die
Klasse Buffer enthält die gesamte Komplexität der Kommunikation und Synchronisation. Natürlich sind put(...) und get() als synchronized deklariert,
um exklusiven Zugriff auf den Puffer und die darin vorkommenden Objektvariablen zu erhalten. Mutual-Exclusion ist für die Synchronisation jedoch nicht
hinreichend, weil zwei Sonderfälle berücksichtigt werden müssen:
• Wenn der Puffer leer ist (erkennbar durch buffer[out] == null), muss
jeder Thread, der get() ausführt, so lange blockiert werden, bis wieder
etwas im Puffer ist.
• Wenn der Puffer voll ist (erkennbar durch buffer[in] != null), muss
jeder Thread, der put(...) ausführt, so lange blockiert werden, bis wieder
ein freier Platz im Puffer ist.
Genau für solche Zwecke gibt es in Java die Methode wait(), die nur in einem
Synchronisationsobjekt mit exklusivem Zugriff aufgerufen werden darf (andernfalls wird eine Ausnahme ausgelöst). Ein Aufruf von wait() bewirkt, dass der
aktuelle Thread seinen exklusiven Zugriff auf das Synchronisationsobjekt verliert und auf unbestimmte Zeit suspendiert, also von der weiteren Ausführung
ausgeschlossen wird. Irgendwann (spätestens vor Beendigung des Programms)
wird der suspendierte Thread wieder aufgeweckt und bekommt erneut exklusiven Zugriff auf das Synchronisationsobjekt, wobei die Programmausführung
an der Stelle direkt nach dem Aufruf von wait() fortgesetzt wird. Wie bei
sleep(. . . ) muss die InterruptedException behandelt werden, die zu jedem
Zeitpunkt auftreten könnte.4 Der Zustand des Puffers könnte während des Wartens verändert worden sein, vielleicht aber auch nicht. Daher müssen wir die
Bedingung nach der Rückkehr aus wait() in einer Schleife erneut überprüfen
und wieder wait() aufrufen, wenn der Zustand für die Fortsetzung der Methodenausführung noch nicht passt. Wenn der Zustand passt, wird die Methode
normal fortgesetzt und entweder ein neuer Eintrag in das Array gemacht oder
ein bestehender Eintrag entfernt und zurückgegeben. Zusätzlich wird durch
einen Aufruf von notifyAll() jeder Thread aufgeweckt, der zuvor durch einen
Aufruf von wait() auf dem gleichen Synchronisationsobjekt suspendiert wurde.
Aufgeweckte Threads dürfen nicht sofort weiterlaufen, sondern bekommen erst
nacheinander, nachdem die gerade ausgeführte synchronized-Methode beendet ist, wieder exklusiven Zugriff auf das Synchronisationsobjekt. In unserem
Fall bewirkt notifyAll() daher, dass jeder auf einen Puffer-Zugriff wartende
Thread erneut die Gelegenheit bekommt zu überprüfen, ob der Puffer jetzt in
einem für ihn passenden Zustand ist.
4Abgesehen von dieser Gemeinsamkeit unterscheiden sich wait() und sleep(. . . ) grundlegend voneinander. So darf wait() nur aufgerufen werden, während exklusiver Zugriff auf
ein Synchronisationsobjekt besteht, aber sleep(. . . ) würde große Probleme verursachen,
wenn der Aufruf während des exklusiven Zugriffs auf ein Synchronisationsobjekt erfolgen
würde. Darüber hinaus wartet wait() für unbestimmte Zeit, sleep(. . . ) jedoch nur eine
bestimmte, ungefähr angegebene Zeit.
118
2.5 Nebenläufigkeit
Neben notifyAll() gibt es auch die Methode notify(), die nicht alle, sondern maximal einen suspendierten Thread aufweckt. Für unsere Zwecke ist
notify() nicht brauchbar, weil wir nicht bestimmen können, welcher suspendierte Thread aufgeweckt werden soll. Beispielsweise könnte notify() einen
Thread aufwecken, der gleich wieder wait() aufrufen muss, weil der Objektzustand nicht passt, obwohl andere suspendierte Threads ausführbar wären.
Dadurch könnten wichtige Threads verhungern. Eingeführt wurde notify()
für Fälle, in denen der Objektzustand nach dem Aufwecken immer passt. Es
gibt viele Gründe, aus denen suspendierte Threads unvorhersehbar aufgeweckt
werden können. Sind zufällig gerade alle Threads aufgeweckt worden, wirkt sich
notify() auf keinen Thread aus und hat damit keinen Effekt.
Das Produzenten-Konsumenten-Problem ist leicht verallgemeinerbar. Als Produzenten verwenden wir beispielsweise einen Thread, der auf einem Socket auf
das Eintreffen von Nachrichten wartet und diese entsprechend ihrer Art sowie
auf Basis der Systemauslastung an einen von mehreren Puffern weiterleitet.
Weitere Produzenten könnten an anderen Datenquellen sitzen und eintreffende
Nachrichten auf die gleiche Weise an Puffer weiterleiten. An den anderen Enden der Puffer sitzen Konsumenten, die eintreffende Nachrichten (unabhängig
von ihrer Quelle, aber nach ihrer Art sortiert) bearbeiten und die Ergebnisse
als Antworten verschicken. Auf diese Weise sind hochkomplexe Systeme aufbaubar, die die Rechenlast auf viele Rechner verteilen und mit einer hohen
Auslastung zurechtkommen. Zwischengeschaltete Puffer sorgen dafür, dass eintreffende Nachrichten nicht sofort bearbeitet werden müssen, sondern erst dann,
wenn Kapazitäten dafür frei werden. Durch Beschränkung der Größe der Puffer
kann dennoch dafür gesorgt werden, dass die Wartezeiten nicht beliebig lang
werden, sondern mangels Kapazität nicht mehr bearbeitbare Nachrichten gleich
zurückgewiesen werden. Aus Anwendersicht ist es besser, wenn das System einen
Auftrag wegen Überlastung ablehnt (und so gegen mögliche Denial-of-ServiceAttacken auftritt), als nach übermäßig langer Wartezeit den dann vielleicht
nicht mehr aktuellen Auftrag auszuführen oder gar nicht zu reagieren.
Das Konzept von Java, in dem über synchronized-Methoden sowie wait()
und notifyAll() synchronisiert wird, ist eine Variante der schon sehr lange
bekannten Monitore [14]. Über Semaphore hinausgehend wurden im Laufe der
Zeit viele Sprachkonzepte für die Synchronisation entwickelt. Bekannt ist das in
Ada integrierte Rendezvous-Konzept, bei dem Nachrichten an Thread-ähnliche
Einheiten (als Tasks bezeichnet) geschickt werden, durchaus vergleichbar mit
dem Schicken von Nachrichten an Objekte in der objektorientierten Programmierung. Allerdings gibt es keine Methoden zum Abarbeiten der Nachrichten.
Stattdessen enthalten Tasks, die so wie Java-Threads meist in Endlosschleifen
laufen, accept-Anweisungen, welche die Nachrichten bearbeiten. Das heißt, der
Task, der eine Nachricht schickt, muss warten, bis der Task, an den die Nachricht geschickt wird, eine passende accept-Anweisung ausführt und die acceptAnweisung muss warten, bis eine Nachricht da ist (synchrone Kommunikation).
Während der Ausführung der accept-Anweisung treffen sich die beiden Tasks
(daher der Begriff „Rendezvous“), nach Abarbeitung dieser Anweisung laufen
beide Tasks wieder getrennt voneinander weiter.
119
2 Etablierte Denkmuster und Werkzeugkisten
Noch direkter als das Rendezvous-Konzept verdeutlicht das Actor-Modell [3],
wie ein nebenläufiges Vorgehen als Zusammenarbeit Thread-ähnlicher Einheiten (Actors genannt) durch den Austausch von Nachrichten verstanden werden
kann: Jeder Actor ist ein eigener Handlungsstrang mit eigenen, nicht von außen zugreifbaren Variablen und einer eigenen Warteschlange an eingehenden
Nachrichten. Der Actor kann Nachrichten aus seiner Warteschlange lesen und
verarbeiten, dabei kann er auch Nachrichten an andere Actors schicken, die
dann in der Warteschlange des Empfängers landen. Das Senden von Nachrichten ist asynchron, das heißt, es wird nicht gewartet, bis die Nachricht abgearbeitet ist. Obwohl das Actor-Modell schon recht alt ist (ebenso wie das
Monitor- und Rendezvous-Konzept), ist es seit kurzem wieder häufiger in aktuellen Programmiersprachen verfügbar (etwa in Scala, es gibt auch mehrere
Actor-Implementierungen für Java).
Schon vor der Ausformulierung des Actor-Modells waren ähnliche Konzepte
im Umlauf. Eines dieser frühen Konzepte stand hinter der Entwicklung von
Smalltalk, einer der ersten objektorientierten Sprachen. Das Ziel bestand darin,
Actor-ähnliche Objekte, die jeweils einen eigenen Handlungsstrang haben, durch
den Austausch von Nachrichten miteinander kommunizieren zu lassen. Dieses
Ziel konnte nicht erreicht werden, weil die damalige Hardware mit der benötigten großen Anzahl an Threads nicht ausreichend effizient umgehen konnte. Als
Ausweg wurde das heute übliche Objektkonzept entwickelt, in dem Objekte
keine eigenen Handlungsstränge besitzen. Die frühe Terminologie ist teilweise erhalten geblieben; ein Methodenaufruf wird als „Senden einer Nachricht“
bezeichnet, was ursprünglich wörtlich zu verstehen war. Es gibt also zumindest historische Gemeinsamkeiten zwischen objektorientierter und nebenläufiger Programmierung. Gemeinsamkeiten dürfen nicht darüber hinwegtäuschen,
dass es heute sehr deutliche Unterschiede gibt. Klar ist, dass parallele und objektorientierte Programmierung kaum zusammenpassen können, weil in diesen
Paradigmen von gänzlich unterschiedlichen Strukturierungen der Daten ausgegangen werden muss. Die Diskrepanzen zwischen nebenläufiger und objektorientierter Programmierung sind dagegen nicht so offensichtlich, weil es ein breit
gefächertes Einsatzgebiet mit unterschiedlichen Ausprägungen gibt. Manchmal
sind diese Paradigmen miteinander kombinierbar, in anderen Fällen nicht.
120
3 Ersetzbarkeit und Untertypen
In diesem Kapitel konzentrieren wir uns auf das zentrale Konzept der objektorientierten Programmierung. In Abschnitt 3.1 untersuchen wir Ersetzbarkeit als
Grundlage von Untertypbeziehungen. Der Schwerpunkt liegt auf Ersetzbarkeit
basierend auf Signaturen sowie die Realisierung über nominale Typen in Java,
die Abstraktionen der realen Welt ermöglichen. In Abschnitt 3.2 gehen wir auf
Beschreibungen des Objektverhaltens ein, die bei der Verwendung von Untertypen zu beachten sind. Es geht um die Berücksichtigung von Zusicherungen
über Design-by-Contract sowie Liskov-Ersetzbarkeit. Danach betrachten wir in
Abschnitt 3.3 die Vererbung im Zusammenhang mit direkter Codewiederverwendung und stellen Sie der Ersetzbarkeit gegenüber. In Abschnitt 3.4 beleuchten wir einige Details zu Klassen und Interfaces in Java. Schließlich behandeln
wir in Abschnitt 3.5 den Umgang mit Exceptions unter Berücksichtigung von
Untertypen und Ersetzbarkeit.
3.1 Ersetzbarkeitsprinzip
Untertypbeziehungen sind durch das Ersetzbarkeitsprinzip definiert:
Definition: Ein Typ U ist Untertyp eines Typs T, wenn jedes Objekt
von U überall verwendbar ist, wo ein Objekt von T erwartet wird.
Ein Objekt eines Untertyps ist per Definition verwendbar, wo ein Objekt eines
Obertyps erwartet wird. Wir benötigen das Ersetzbarkeitsprinzip vor allem für
• einen Methodenaufruf mit einem Argument, dessen deklarierter Typ Untertyp des deklarierten Typs des entsprechenden Eingangsparameters ist,
• die Zuweisung eines Objekts an eine Variable, wobei der deklarierte Typ
des Objekts ein Untertyp des deklarierten Typs der Variablen ist.
Beide Fälle kommen in der objektorientierten Programmierung häufig vor.
3.1.1 Untertypen und Schnittstellen
Die Frage danach, wann das Ersetzbarkeitsprinzip erfüllt ist, wurde in der
Fachliteratur intensiv behandelt [2, 4, 24]. Wir wollen diese Frage hier nur so
weit beantworten, als es in der Praxis relevant ist. Fast alles, was wir anhand
von Java untersuchen, gilt auch für andere objektorientierte Sprachen, zumindest für solche mit statischer Typprüfung wie C# und C++. Wir gehen davon
121
3 Ersetzbarkeit und Untertypen
aus, dass Typen Schnittstellen von Objekten sind, die in Klassen beziehungsweise Interfaces spezifiziert wurden. Es gibt in Java auch elementare Typen
wie int, die keiner Klasse entsprechen. Auf solchen Typen haben wir keine
Untertypbeziehungen. Deshalb werden wir sie hier nicht näher betrachten.
Eine Voraussetzung für das Bestehen einer Untertypbeziehung in Java ist eine
Vererbungsbeziehung auf den entsprechenden Klassen und Interfaces. Die dem
Untertyp entsprechende Klasse bzw. das Interface muss durch extends oder
implements von der dem Obertyp entsprechenden Klasse oder dem Interface
direkt oder indirekt abgeleitet sein. Anders ausgedrückt: Es werden nominale
Typen verwendet – siehe Abschnitte 1.4.2 und 1.6.2. Damit eignen sich Typen
gut für die Abstraktion.
Es ist möglich, Untertypbeziehungen auch ohne Vererbung nur auf Basis
struktureller Typen zu definieren. Das geschieht hauptsächlich in der Theorie, um formal schwer nachvollziehbare, auf Intuition beruhende Abstraktionen
unberücksichtigt lassen zu können.
Strukturelle Untertypbeziehungen. Unter den folgenden Bedingungen stehen strukturelle Typen in einer Untertypbeziehung. Diese Bedingungen gelten
allgemein und sind nicht auf eine bestimmte Programmiersprache bezogen. Alle
Untertypbeziehungen sind stets
• reflexiv – jeder Typ ist Untertyp von sich selbst,
• transitiv – ist ein Typ U Untertyp eines Typs A und ist A Untertyp eines
Typs T, dann ist U auch Untertyp von T,
• antisymmetrisch – ist ein Typ U Untertyp eines Typs T und ist T außerdem Untertyp von U, dann sind U und T äquivalent.
Beliebige strukturelle Typen bezeichnen wir mit U und T sowie A und B. Es
gilt „U ist Untertyp von T“ wenn folgende Bedingungen erfüllt sind:
• Für jede Konstante (also jede Variable, die nach der Initialisierung nur
lesende Zugriffe erlaubt) in T gibt es eine entsprechende Konstante in
U, wobei der deklarierte Typ B der Konstante in U ein Untertyp des
deklarierten Typs A der Konstante in T ist.1
Begründung: Auf eine Konstante kann außerhalb des Objekts nur lesend
zugegriffen werden. Wenn wir die Konstante in einem Objekt vom Typ
T sehen und lesend darauf zugreifen, erwarten wir uns, dass wir einen
Wert vom Typ A erhalten. Diese Erwartung soll auch erfüllt sein, wenn
das Objekt vom Typ U ist, wenn also ein Objekt von U verwendet wird,
wo wir ein Objekt von T erwarten. Aufgrund der Bedingung gibt es im
Objekt vom Typ U eine entsprechende Konstante vom Typ B. Wir bekommen beim lesenden Zugriff einen Wert vom Typ B, wo wir ein Objekt
vom Typ A erwarten. Da B ein Untertyp von A sein muss, ist das Objekt
1
In Java ist es nicht möglich, eine in T eingeführte Konstante in U zu ändern, sodass A und
B immer gleich sind. Wir betrachten hier aber nicht Java, sondern den allgemeinen Fall.
122
3.1 Ersetzbarkeitsprinzip
von B überall verwendbar, wo ein Objekt von A erwartet wird. Unsere
Erwartungen sind daher erfüllt. Die Initialisierung der Konstante müssen
wir nicht berücksichtigen, weil sie nur dort (etwa im Konstruktor) erfolgen kann, wo keine Ersetzbarkeit nötig ist (Konstante in T wird in T
initialisiert, Konstante in U in U).
• Für jede nach außen sichtbare2 Variable in T gibt es eine entsprechende
Variable in U, wobei die deklarierten Typen der Variablen äquivalent sind.
Begründung: Auf eine Variable kann lesend und schreibend zugegriffen
werden. Ein lesender Zugriff entspricht der oben beschriebenen Situation
bei Konstanten; der deklarierte Typ B der Variablen in U muss ein Untertyp des deklarierten Typs A der Variablen in T sein. Wird eine Variable
eines Objekts vom Typ T außerhalb des Objekts geschrieben, erwarten
wir uns, dass der Variablen jedes Objekt vom Typ A zugewiesen werden
darf. Diese Erwartung soll erfüllt sein, wenn das Objekt vom Typ U und
die Variable vom Typ B ist. Die Erwartung ist nur erfüllt, wenn A ein
Untertyp von B ist. Bei gemeinsamer Betrachtung lesender und schreibender Zugriffe muss B ein Untertyp von A und A ein Untertyp von B
sein. Das ist nur möglich, wenn A und B äquivalent sind.
• Für jede Methode in T gibt es eine entsprechende gleichnamige Methode
in U, wobei
– der deklarierte Ergebnistyp der Methode in U ein Untertyp des deklarierten Ergebnistyps der Methode in T ist,
– die Anzahl der Parameter beider Methoden gleich ist,
– der deklarierte Typ jeden Eingangsparameters3
in U ein Obertyp des
deklarierten Typs des entsprechenden Parameters in T ist (der auch
ein Eingangsparameter sein muss),
– der deklarierte Typ jeden Durchgangsparameters in U äquivalent
zum deklarierten Typ des entsprechenden Parameters in T ist (der
auch ein Durchgangsparameter sein muss)
– und der deklarierte Typ jeden Ausgangsparameters in U ein Untertyp
des deklarierten Typs des entsprechenden Parameters in T ist (der
auch ein Ausgangsparameter sein muss).
Begründung: Für die Ergebnistypen der Methoden gilt das Gleiche wie
für Typen von Konstanten beziehungsweise lesende Zugriffe auf Variablen:
Der Aufrufer einer Methode möchte ein Ergebnis des in T versprochenen
2Es wird dringend emfohlen, nur private Variablen zu verwenden, die nicht nach außen
sichtbar sind. Eine private Variable ist nur innerhalb einer Klasse sichtbar, sodass das
Ersetzbarkeitsprinzip nicht zur Anwendung kommt oder (anders formuliert) eine Verletzung dieser Regel gar nicht möglich ist. Aus Gründen der Vollständigkeit und zum besseren
Verständnis müssen wir dennoch auch den Fall einer nicht-privaten Variable untersuchen.
Außerdem kann in Java eine in T eingeführte Variable in U nicht verändert werden, was
aber irrelevant ist, weil wir hier den allgemeinen Fall betrachten, nicht Java.
3Siehe Abschnitt 1.5.1 für die Unterscheidung zwischen den Parameterarten.
123
3 Ersetzbarkeit und Untertypen
Ergebnistyps bekommen, auch wenn die entsprechende Methode in U ausgeführt wird. Für die Typen der Eingangsparameter gilt das Gleiche wie
für schreibende Zugriffe auf Variablen: Der Aufrufer möchte alle Argumente der Typen an die Methode übergeben können, die in T deklariert
sind, auch wenn die entsprechende Methode in U ausgeführt wird. Daher
dürfen die Eingangsparametertypen in U nur Obertypen der Eingangsparametertypen in T sein. Durchgangsparameter werden vom Aufrufer vor
dem Aufruf geschrieben und nach der Rückkehr gelesen; da gelesen und geschrieben werden muss, gelten die gleichen Bedingungen wie für Variablen.
Ausgangsparameter werden vom Aufrufer nach der Rückkehr gelesen, wodurch die gleichen Bedingungen wie für Methodenergebnisse gelten. Das
Schreiben des Rückgabewerts und Zugriffe auf Parameter innerhalb der
aufgerufenen Methode müssen wir nicht betrachten, da innerhalb der Methode der genaue Typ (U oder T) immer bekannt ist; das Ersetzbarkeitsprinzip wird dafür nicht benötigt.
Alle diese Bedingungen gelten nur für Variablen und Methoden, die außerhalb
eines Objekts sichtbar sind, die also zur Schnittstelle gehören. Private Inhalte
haben keinen Einfluss auf Untertypbeziehungen.
Diese Bedingungen hängen nur von den Strukturen der Typen ab. Sie berücksichtigen das Verhalten in keiner Weise. Solche Untertypbeziehungen sind
auch gegeben, wenn die Strukturen nur zufällig zusammenpassen.
Obigen Regeln entsprechend kann ein Untertyp einen Obertyp nicht nur um
neue Elemente erweitern, sondern auch deklarierte Typen der Elemente (z. B.
Parameter) ändern; das heißt, die deklarierten Typen der Elemente können
variieren. Folgende Arten der Varianz werden unterschieden:
Kovarianz: Der deklarierte Typ eines Elements im Untertyp ist Untertyp des
deklarierten Typs des entsprechenden Elements im Obertyp. Deklarierte
Typen von Konstanten und von Ergebnissen der Methoden sowie von
Ausgangsparametern sind kovariant. Typen und die betrachteten darin
enthaltenen Elementtypen variieren in die gleiche Richtung.
Kontravarianz: Der deklarierte Typ eines Elements im Untertyp ist Obertyp
des deklarierten Typs des Elements im Obertyp. Deklarierte Typen von
Eingangsparametern sind kontravariant. Typen und die betrachteten darin enthaltenen Elementtypen variieren in entgegengesetzte Richtungen.
Invarianz: Der deklarierte Typ eines Elements im Untertyp ist äquivalent zum
deklarierten Typ des entsprechenden Elements im Obertyp. Deklarierte
Typen von Variablen und Durchgangsparametern sind invariant. Die betrachteten in den Typen enthaltenen Elementtypen variieren nicht.
Möglichkeiten und Grenzen. Folgendes Beispiel in einer Java-ähnlichen Sprache (Java kann es wegen zusätzlicher Einschränkungen nicht sein) zeigt einige
Möglichkeiten von strukturellen Untertypbeziehungen auf:
124
3.1 Ersetzbarkeitsprinzip
public class A {
public A meth(B par) { ... }
}
public class B {
public B meth(A par) { ... }
public void foo() { ... }
}
Entsprechend den oben angeführten Bedingungen ist B ein Untertyp von A.
Die Methode meth in B kann an Stelle von meth in A verwendet werden: Der
Ergebnistyp ist kovariant verändert, der Parametertyp kontravariant. Wäre die
Methode foo in B nicht vorhanden, dann könnten A und B sogar als äquivalent
betrachtet werden (der Unterschied zwischen A und B würde verschwinden).
Das alles gilt in einer Java-ähnlichen Sprache, die auf strukturellen Typen
beruht. Java verwendet jedoch nominale Typen, und in Java ist B kein Untertyp
von A. Wenn B mit der Klausel „extends A“ deklariert wäre, würde meth in B
die Methode in A nicht überschreiben; stattdessen würde meth von A geerbt und
überladen, so dass es in Objekten von B beide Methoden nebeneinander gäbe.
Obige Bedingungen für Untertypbeziehungen sind notwendig und für strukturelle Typen auch vollständig. Man kann keine weglassen oder aufweichen,
ohne mit dem Ersetzbarkeitsprinzip in Konflikt zu kommen. Die meisten dieser
Bedingungen stellen keine praktische Einschränkung dar. Wir kommen kaum in
Versuchung sie zu brechen. Nur eine Bedingung, nämlich die geforderte Kontravarianz der Eingangsparametertypen, möchten wir manchmal gerne umgehen.
Sehen wir uns dazu ein Beispiel an:
public class Point2D {
protected int x, y; // von außen sichtbar
public boolean equal(Point2D p) {
return x == p.x && y == p.y;
}
}
public class Point3D {
protected int x, y, z;
public boolean equal(Point3D p) {
return x == p.x && y == p.y && z == p.z;
}
}
Wegen der zusätzlichen von außen sichtbaren Variable in Point3D sind die beiden Typen nicht äquivalent und kann Point2D kein Untertyp von Point3D sein.
Außerdem kann Point3D kein Untertyp von Point2D sein, da equal nicht die
Kriterien für Untertypbeziehungen erfüllt. Der Parametertyp wäre ja kovariant
und nicht, wie gefordert, kontravariant.
Eine Methode wie equal, bei der mindestens ein Eingangsparametertyp stets
gleich der Klasse (oder dem Interface) ist, in der die Methode definiert ist, heißt
125
3 Ersetzbarkeit und Untertypen
binäre Methode.4 Die Eigenschaft binär bezieht sich darauf, dass der Name
der Klasse (des Interfaces) in der Methode mindestens zweimal vorkommt –
einmal als Typ von this und mindestens einmal als Typ eines Parameters.
Binäre Methoden werden häufig benötigt, sind über Untertypbeziehungen (ohne
dynamische Typabfragen und Casts) aber prinzipiell nicht realisierbar.
Faustregel: Kovariante Eingangsparametertypen und binäre Methoden widersprechen dem Ersetzbarkeitsprinzip. Es ist sinnlos, in solchen Fällen Ersetzbarkeit anzustreben.
Untertypbeziehungen in Java setzen entsprechende Vererbungsbeziehungen
voraus. Vererbung ist in Java so eingeschränkt, dass zumindest alle Bedingungen für Untertypbeziehungen auf strukturellen Typen erfüllt sind. Die Bedingungen werden bei der Übersetzung eines Java-Programms fast lückenlos
überprüft. (Eine Ausnahme bezogen auf Arrays und Generizität werden wir in
Abschnitt 4.1.3 kennen lernen.)
Untertypbeziehungen sind in Java nicht nur aufgrund nominaler Typen stärker eingeschränkt, als durch obige Bedingungen notwendig wäre. In Java sind
alle Typen invariant, abgesehen von kovarianten Ergebnistypen ab Version 1.5.
Der Grund dafür liegt darin, dass bei Zugriffen auf Variablen und Konstanten
nicht dynamisch, sondern statisch gebunden wird (dafür also keine Ersetzbarkeit nötig ist) und darin, dass Methoden überladen sein können. Da überladene Methoden durch die Typen der Parameter unterschieden werden, wäre es
schwierig, überladene Methoden von Methoden mit kontravariant veränderten
Typen auseinanderzuhalten.
3.1.2 Untertypen und Codewiederverwendung
Die wichtigste Entscheidungsgrundlage für den Einsatz von Untertypen ist die
erzielbare Wiederverwendung. Der richtige Einsatz eröffnet Möglichkeiten, die
auf den ersten Blick nicht so leicht zu erkennen sind.
Nehmen wir als Beispiel die Treiber-Software für eine Grafikkarte. Anfangs
genügt ein einfacher Treiber für einfache Ansprüche. Wir entwickeln eine Klasse,
die den Code für den Treiber enthält und nach außen eine Schnittstelle anbietet,
über die wir die Funktionalität des Treibers verwenden können. Letzteres ist
der Typ des Treibers. Wir schreiben einige Anwendungen, welche die Treiberklasse verwenden. Daneben werden vielleicht auch von anderen Personen, die
wir nicht kennen, Anwendungen erstellt, die unsere Treiberklasse verwenden.
Alle Anwendungen greifen über dessen Schnittstelle beziehungsweise Typ auf
den Treiber zu.
4Genau genommen ist das nur ein Spezialfall einer binären Methode. Es handelt sich auch
dann um eine binäre Methode, wenn für mindestens zwei Eingangsparameter Argumente
vom gleichen dynamischen Typ übergeben werden müssen, unabhängig von this. Es gibt
einige objektorientierte Sprachen wie Ada und Eiffel, die das (durch dynamische Typprüfungen) unterstützen. In den meisten objektorientierten Sprachen sind binäre Methoden
jedoch nur dadurch ausdrückbar, dass mindestens ein Eingangsparameter gleich dem Typ
der Klasse sein muss, auch in Untertypen davon.
126
3.1 Ersetzbarkeitsprinzip
Mit der Zeit wird unser einfacher Treiber zu primitiv. Wir entwickeln einen
neuen, effizienteren Treiber, der auch Eigenschaften neuerer Grafikkarten verwenden kann. Wir erben von der alten Klasse und lassen die Schnittstelle unverändert, abgesehen davon, dass wir neue Methoden hinzufügen. Nach obiger
Definition ist der Typ der neuen Klasse ein Untertyp des alten Typs. Neue Treiber – das sind Objekte des Treibertyps – können überall verwendet werden, wo
alte Treiber erwartet werden. Daher können wir in den vielen Anwendungen, die
den Treiber bereits verwenden, den alten Treiber ganz einfach gegen den neuen
austauschen, ohne die Anwendungen sonst irgendwie zu ändern. In diesem Fall
haben wir Wiederverwendung in großem Umfang erzielt: Viele Anwendungen
sind sehr einfach auf einen neuen Treiber umgestellt worden. Darunter sind
auch Anwendungen, von deren Existenz wir nichts wissen. Das Beispiel können
wir beliebig fortsetzen, indem wir immer wieder neue Varianten von Treibern
schreiben und neue Anwendungen entwickeln oder bestehende Anwendungen
anpassen, welche die jeweils neuesten Eigenschaften der Treiber nützen. Dabei
kann es passieren, dass aus einer Treiberversion mehrere weitere Treiberversionen entwickelt werden, die nicht zueinander kompatibel sind. Folgendes Bild
zeigt, wie die Treiberversionen nach drei Generationen aussehen könnten:
Treiber 1
% ↑ -
Treiber 2a Treiber 2b Treiber 2c
↑ - ↑ ↑
Treiber 3a Treiber 3b Treiber 3c
An dieser Struktur fällt Version 3b auf: Sie vereinigt die zwei inkompatiblen Vorgängerversionen 2a und 2b. Ein Untertyp kann mehrere Obertypen haben, die
zueinander in keiner Untertypbeziehung stehen. Das ist ein Beispiel für Mehrfachvererbung, während in den anderen Fällen nur Einfachvererbung nötig ist.
Diese Hierarchie kann in Java nur realisiert werden, wenn die Treiberschnittstellen Interfaces (keine Klassen) sind.
Faustregel: Wir sollen auf Ersetzbarkeit achten, um Codewiederverwendung zwischen Versionen zu erreichen.
Die Wiederverwendung zwischen verschiedenen Versionen funktioniert nur
dann gut, wenn die Schnittstellen bzw. Typen zwischen den Versionen stabil
bleiben. Das heißt, eine neue Version darf die Schnittstellen nicht beliebig ändern, sondern nur so, dass die in Abschnitt 3.1.1 beschriebenen Bedingungen
erfüllt sind. Im Wesentlichen kann die Schnittstelle also nur erweitert werden.
Wenn die Aufteilung eines Programms in einzelne Objekte (also die Faktorisierung) gut ist, bleiben Schnittstellen normalerweise recht stabil.
Faustregel: Schnittstellen sollen stabil bleiben. Eine gute Faktorisierung hilft dabei.
127
3 Ersetzbarkeit und Untertypen
Das, was in obigem Beispiel für verschiedene Versionen funktioniert, ist auch
innerhalb eines einzigen Programms nützlich, wie wir an einem modifizierten
Beispiel sehen. Wir wollen ein Programm zur Verwaltung der Personen an einer
Universität entwickeln. Die dafür verwendete Typstruktur könnte so aussehen:
Person
% ↑ -
Teacher Student AdminPers
↑ % - ↑
Tutor StudTrainee
Tutor_innen sind sowohl Lehrende als auch Studierende, Werkstudent_innen
(StudTrainee) gehören zum Verwaltungspersonal (AdminPers) und sind Studierende. Wir benötigen im Programm eine Komponente, die Serienbriefe –
Einladungen zu Veranstaltungen, etc. – an alle Personen adressiert. Für das Erstellen einer Anschrift benötigen wir nur Informationen aus der Klasse Person.
Die entsprechende Methode muss nicht zwischen verschiedenen Arten von Personen unterscheiden, sondern funktioniert für jedes Objekt des deklarierten
Typs Person, auch wenn es ein Objekt des dynamischen Typs Tutor ist. Diese Methode wird also für alle Arten von Personen (wieder)verwendet. Ebenso
funktioniert eine Methode zum Ausstellen eines Zeugnisses für alle Objekte von
Student, auch wenn es Tutor_innen oder Werkstudent_innen sind. Für dieses
Beispiel müssen in Java ebenso Interfaces verwendet werden.
Faustregel: Wir sollen auf Ersetzbarkeit achten, um interne Codewiederverwendung im Programm zu erzielen.
Solche Typstrukturen helfen, Auswirkungen nötiger Programmänderungen
lokal zu halten. Ändern wir einen Typ, zum Beispiel Student, bleiben andere
Typen, die nicht Untertypen von Student sind, unberührt. Von der Änderung
betroffene Typen sind an der Typstruktur leicht erkennbar. Unter „betroffen“
verstehen wir dabei, dass als Folge der Änderung möglicherweise weitere Änderungen in den betroffenen Programmteilen nötig sind. Die Änderung kann nicht
nur diese Typen (Klassen oder Interfaces) selbst betreffen, sondern auch alle
Programmstellen, die auf Objekte der Typen Student, Tutor oder StudTrainee
zugreifen. Aber Programmteile, die auf Objekte des deklarierten Typs Person
zugreifen, sollten von der Änderung auch dann nicht betroffen sein, wenn die
Objekte tatsächlich vom dynamischen Typ Student sind. Diese Programmteile
haben keinen Zugriff auf geänderte Objekteigenschaften.
Faustregel: Wir sollen auf Ersetzbarkeit achten, um Programmänderungen lokal zu halten.
Falls bei der Programmänderung alle Schnittstellen unverändert bleiben, sind
keine Programmstellen betroffen, an denen Student und dessen Untertypen verwendet werden. Lediglich diese Typen selbst sind betroffen. Auch daran können
wir sehen, wie wichtig es ist, dass Schnittstellen und Typen stabil sind. Eine
128
3.1 Ersetzbarkeitsprinzip
Programmänderung führt möglicherweise zu vielen weiteren nötigen Änderungen, wenn dabei eine Schnittstelle geändert wird. Die Anzahl wahrscheinlich
nötiger Änderungen hängt auch davon ab, wo in der Typstruktur die geänderte Schnittstelle steht. Eine Änderung ganz oben in der Struktur hat wesentlich
größere Auswirkungen als eine Änderung ganz unten. Eine Schlussfolgerung aus
diesen Überlegungen ist, dass wir möglichst nur von solchen Klassen ableiten
sollen, deren Schnittstellen bereits – oft nach mehreren Refaktorisierungsschritten – recht stabil sind.
Faustregel: Die Stabilität von Schnittstellen an der Wurzel der Typhierarchie ist wichtiger als an den Blättern. Wir sollen nur Untertypen von stabilen Obertypen bilden.
Aus obigen Überlegungen folgt auch, dass wir die Typen von Parametern
möglichst allgemein halten sollen. Wenn in einer Methode von einem Parameter nur die Eigenschaften von Person benötigt werden, sollte der Parametertyp
Person sein und nicht StudTrainee, auch wenn die Methode voraussichtlich nur
mit Argumenten vom Typ StudTrainee aufgerufen wird. Wenn aber die Wahrscheinlichkeit hoch ist, dass nach einer späteren Programmänderung in der Methode vom Parameter auch Eigenschaften von StudTrainee benötigt werden,
sollten wir gleich von Anfang an StudTrainee als Parametertyp verwenden, da
nachträgliche Änderungen von Schnittstellen sehr teuer werden können.
Faustregel: Wir sollen Parametertypen vorausschauend und möglichst allgemein wählen.
Trotz der Wichtigkeit stabiler Schnittstellen dürfen wir nicht bereits zu früh
zu viel Zeit in den detaillierten Entwurf der Schnittstellen investieren. Zu Beginn haben wir häufig noch nicht genug Information, um stabile Schnittstellen
zu erhalten. Schnittstellen werden trotz guter Planung oft erst nach einigen
(wenigen) Refakorisierungsschritten stabil.
3.1.3 Dynamisches Binden
Bei Verwendung von Untertypen kann der dynamische Typ einer Variablen oder
eines Eingangsparameters ein Untertyp des deklarierten Typs sein. Eine als
Person deklarierte Variable kann etwa ein Objekt von StudTrainee enthalten.
Häufig ist zur Übersetzungszeit der dynamische Typ nicht bekannt, das heißt,
der dynamische Typ kann sich vom statischen Typ unterscheiden. Dann können
Aufrufe einer Methode im Objekt, das in der Variablen steht, erst zur Laufzeit
an die auszuführende Methode gebunden werden. In Java wird unabhängig
vom deklarierten Typ immer die Methode ausgeführt, die im spezifischsten
dynamischen Typ definiert ist. Dieser Typ entspricht der Klasse des Objekts in
der Variablen.
Wir demonstrieren dynamisches Binden an einem kleinen Beispiel:
129
3 Ersetzbarkeit und Untertypen
public class A {
public String foo1() { return "foo1A"; }
public String foo2() { return fooX(); }
public String fooX() { return "foo2A"; }
}
public class B extends A {
public String foo1() { return "foo1B"; }
public String fooX() { return "foo2B"; }
}
public class DynamicBindingTest {
public static void test(A x) {
System.out.println(x.foo1());
System.out.println(x.foo2());
}
public static void main(String[] args) {
test(new A());
test(new B());
}
}
Die Ausführung von DynamicBindingTest liefert folgende Ausgabe:
foo1A
foo2A
foo1B
foo2B
Die ersten Zeilen sind einfach erklärbar: Nach dem Programmaufruf wird die
Methode main ausgeführt, die test mit einem neuen Objekt von A als Argument aufruft. Diese Methode ruft zuerst foo1 und dann foo2 auf und gibt die
Ergebnisse in den ersten beiden Zeilen aus. Dabei entspricht der deklarierte
Typ A des Parameters x dem spezifischsten dynamischen Typ. Es werden daher
foo1 und foo2 in A ausgeführt.
Der zweite Aufruf von test übergibt ein Objekt von B als Argument. Dabei ist
A der deklarierte Typ von x, aber der dynamische Typ ist B. Wegen dynamischen
Bindens werden diesmal foo1 und foo2 in B ausgeführt. Die dritte Zeile der
Ausgabe enthält das Ergebnis des Aufrufs von foo1 in einem Objekt von B.
Die letzte Zeile der Ausgabe lässt sich folgendermaßen erklären: Da die Klasse
B die Methode foo2 nicht überschreibt, wird foo2 von A geerbt. Der Aufruf von
foo2 in B ruft fooX in der aktuellen Umgebung auf, das ist ein Objekt von B. Die
Methode fooX liefert als Ergebnis die Zeichenkette "foo2B", die in der letzten
Zeile ausgegeben wird.
Bei dieser Erklärung müssen wir vorsichtig sein: Wir machen leicht den Fehler
anzunehmen, dass foo2 und daher auch fooX in A aufgerufen wird, da foo2 ja
nicht explizit in B steht. Tatsächlich wird aber fooX in B aufgerufen, da B der
spezifischste Typ der Umgebung ist.
130
3.1 Ersetzbarkeitsprinzip
Dynamisches Binden ist mit switch-Anweisungen und geschachtelten ifAnweisungen verwandt. Wir betrachten als Beispiel eine Methode, die eine Anrede in einem Brief, deren Form auf konventionelle Weise über eine ganze Zahl
bestimmt ist, in die Standardausgabe schreibt:
public void addressPerson(int form, String name) {
switch(form) {
case 1: System.out.print("Sehr geehrte Frau " + name);
break;
case 2: System.out.print("Sehr geehrter Herr " + name);
break;
default: System.out.print(name);
}
}
In der objektorientierten Programmierung wird die Form der Anrede eher durch
die Klassenstruktur zusammen mit dem Namen beschrieben:
public class Addressee {
private String name;
protected String name() {
return name;
}
public void addressPerson() {
System.out.print(name());
}
... // Konstruktoren und weitere Methoden
}
public class FemaleAddressee extends Addressee {
public void addressPerson() {
System.out.print ("Sehr geehrte Frau " + name());
}
}
public class MaleAddressee extends Addressee {
public void addressPerson() {
System.out.print ("Sehr geehrter Herr " + name());
}
}
Durch dynamisches Binden wird automatisch die gewünschte Variante von
addressPerson() aufgerufen. Statt einer switch-Anweisung wird also dynamisches Binden verwendet. Ein Vorteil des objektorientierten Ansatzes ist die
bessere Lesbarkeit. Wir wissen anhand der Namen, wofür bestimmte Unterklassen von Addressee stehen. Die Zahlen 1 oder 2 bieten diese Information nicht.
Außerdem ist die Form der Anrede mit dem auszugebenden Namen verknüpft,
wodurch im Programm stets nur ein Objekt von Addressee anstatt einer ganzen Zahl und einem String verwaltet werden muss. Ein anderer Vorteil des
131
3 Ersetzbarkeit und Untertypen
objektorientierten Ansatzes ist besonders wichtig: Wenn sich herausstellt, dass
neben „Frau“ und „Herr“ noch weitere Formen von Anreden, etwa „Firma“,
benötigt werden, können wir diese leicht durch Hinzufügen weiterer Klassen
einführen. Es sind keine zusätzlichen Änderungen nötig. Insbesondere bleiben
die Methodenaufrufe unverändert. Letzteres können wir durch Verwendung benannter Konstanten oder Enums statt der Zahlen nicht erreichen.
Auf den ersten Blick mag es scheinen, als ob der konventionelle Ansatz mit
switch-Anweisung kürzer und auch einfach durch Hinzufügen einer Zeile änderbar wäre. Am Beginn der Programmentwicklung trifft das oft auch zu. Leider
haben solche switch-Anweisungen die Eigenschaft, dass sie sich sehr rasch über
das ganze Programm ausbreiten. Beispielsweise gibt es bald auch spezielle Methoden zur Ausgabe der Anrede in generierten e-Mails, abgekürzt in Berichten,
oder über Telefon als gesprochener Text, jede Methode mit zumindest einer
eigenen switch-Anweisung. Dann ist es schwierig, zum Einfügen der neuen Anredeform alle solchen switch-Anweisungen zu finden und noch schwieriger, diese
Programmteile über einen längeren Zeitraum konsistent zu halten. Der objektorientierte Ansatz hat dieses Problem nicht, da alles auf die Klasse Addressee
und ihre Unterklassen konzentriert ist. Es bleibt auch dann alles konzentriert,
wenn zu addressPerson() weitere Methoden hinzukommen.
Faustregel: Dynamisches Binden ist switch-Anweisungen und geschachtelten if-Anweisungen vorzuziehen.
3.2 Ersetzbarkeit und Objektverhalten
In Abschnitt 3.1.1 haben wir Bedingungen kennen gelernt, unter denen ein
struktureller Typ Untertyp eines anderen ist. Für nominale Typen gilt daneben
noch die Bedingung, dass der Untertyp explizit vom Obertyp abgeleitet sein
muss. Die Erfüllung dieser Bedingungen wird vom Compiler überprüft. In Java
und den meisten anderen Sprachen mit statischer Typprüfung werden sogar
etwas strengere Bedingungen geprüft, die nicht für Untertypen, sondern z. B.
für das Überladen von Methoden sinnvoll sind.
Jedoch sind alle prüfbaren Bedingungen nicht ausreichend, um die uneingeschränkte Ersetzbarkeit eines Objekts eines Obertyps durch ein Objekt eines Untertyps zu garantieren. Dazu müssen weitere Bedingungen hinsichtlich
des Objektverhaltens erfüllt sein, die von einem Compiler nicht überprüft werden können. Wir müssen diese Bedingungen beim Programmieren selbst (ohne
Werkzeugunterstützung) sicherstellen.
3.2.1 Client-Server-Beziehungen
Für die Beschreibung des Objektverhaltens ist es hilfreich, das Objekt aus der
Sicht anderer Objekte, die auf das Objekt zugreifen, zu betrachten (siehe Abschnitt 1.3.3). Wir sprechen von Client-Server-Beziehungen zwischen Objekten.
Einerseits sehen wir ein Objekt als Server, der anderen Objekten seine Dienste
132
3.2 Ersetzbarkeit und Objektverhalten
zur Verfügung stellt. Andererseits ist ein Objekt ein Client, der Dienste anderer
Objekte in Anspruch nimmt. Ein Objekt ist gleichzeitig Server und Client.
Für die Ersetzbarkeit von Objekten sind Client-Server-Beziehungen bedeutend. Ein Objekt ist gegen ein anderes ersetzbar, wenn das ersetzende Objekt
als Server allen Clients zumindest dieselben Dienste anbietet wie das ersetzte
Objekt. Um das gewährleisten zu können, brauchen wir eine Beschreibung der
Dienste, also das Verhalten der Objekte.
Das Objektverhalten beschreibt, wie sich das Objekt beim Empfang einer
Nachricht verhält, das heißt, was das Objekt beim Aufruf einer Methode macht.
Diese Definition von Objektverhalten lässt etwas offen: Es ist unklar, wie exakt
die Beschreibung des Verhaltens sein soll. Einerseits beschreibt die Signatur das
Objekt nur sehr unvollständig. Eine genauere Beschreibung wäre wünschenswert. Andererseits enthält die Implementierung, also der Programmcode in der
Klasse, oft zu viele Implementierungsdetails, die bei der Betrachtung des Verhaltens hinderlich sind. Im Programmcode gibt es meist keine Beschreibung, deren
Detailiertheitsgrad zwischen dem der Signatur und dem der Implementierung
liegt. Wir haben es beim Objektverhalten also mit einem abstrakten Begriff zu
tun. Er wird vom Programmcode nicht notwendigerweise widergespiegelt.
Es hat sich bewährt, das Verhalten eines Objekts als einen Vertrag zwischen
dem Objekt als Server und seinen Clients zu sehen – Design-by-Contract. Der
Server muss diesen Vertrag ebenso einhalten wie jeder einzelne Client, in einigen Fällen auch die Gemeinschaft aller Clients zusammen. Generell sieht der
Softwarevertrag folgendermaßen aus [25, 29]:
Vertrag: Jeder Client kann Dienste des Servers in Anspruch nehmen, wenn die festgeschriebenen Bedingungen dafür erfüllt sind. Im
Falle einer Inanspruchnahme setzt der Server die festgeschriebenen
Maßnahmen und liefert dem Client ein Ergebnis, das die festgeschriebenen Bedingungen erfüllt.
Im einzelnen regelt der Vertrag für jeden vom Server angebotenen Dienst, also
für jede aufrufbare Methode (unter der Annahme, dass auf Objektvariablen nur
über Methoden des Servers zugegriffen wird), folgende Details:
Vorbedingung (Precondition): Für die Erfüllung der Vorbedingung einer Methode vor Ausführung der Methode ist jeder einzelne Client verantwortlich. Vorbedingungen beschreiben hauptsächlich, welche Eigenschaften die
Argumente, mit denen die Methode aufgerufen wird, erfüllen müssen. Zum
Beispiel muss ein bestimmtes Argument ein Array aufsteigend sortierter
ganzen Zahlen im Wertebereich von 0 bis 99 sein. Vorbedingungen können
auch den Zustand des Servers einbeziehen, soweit Clients diesen kennen.
Z. B. ist eine Methode nur aufrufbar, wenn eine Objektvariable des Servers
(etwa über einen Getter abfragbar) einen Wert größer 0 hat.
Nachbedingung (Postcondition): Für die Erfüllung der Nachbedingung einer Methode nach Ausführung der Methode ist der Server verantwortlich. Nachbedingungen beschreiben Eigenschaften des Methodenergebnisses und Änderungen beziehungsweise Eigenschaften des Objektzustands.
133
3 Ersetzbarkeit und Untertypen
Als Beispiel betrachten wir eine Methode zum Einfügen eines Elements in
eine Menge: Das Boolesche Ergebnis der Methode besagt, ob das Argument vor dem Aufruf bereits in der Menge enthalten war; am Ende muss
es auf jeden Fall in der Menge sein. Diese Beschreibung kann man als
Nachbedingung auffassen.
Invariante (Invariant): Für die Erfüllung von Invarianten auf Objektvariablen
sowohl vor als auch nach Ausführung jeder Methode ist grundsätzlich der
Server zuständig. Direkte Schreibzugriffe von Clients auf Variablen des
Servers kann der Server aber nicht kontrollieren; dafür sind die Clients
verantwortlich. Zum Beispiel darf das Guthaben auf einem Sparbuch nie
kleiner 0 sein, egal welche Operationen darauf durchgeführt werden.
History-Constraint: Diese Bedingungen schränken die Entwicklung von Objekten im Laufe der Zeit ein. Wir unterscheiden zwei Unterarten:
Server-kontrolliert: Sie ähneln Invarianten, schränken aber zeitliche Veränderungen der Variableninhalte eines Objekts ein. Z. B. kann der
ganzzahlige Wert einer Objektvariablen, die als Zähler verwendet
wird, im Laufe der Zeit immer größer, aber niemals kleiner werden. Wie bei Invarianten ist für die Einhaltung der Server zuständig. Wenn jedoch Clients die betroffenen Variablen direkt schreiben
können, sind auch die Clients verantwortlich.
Client-kontrolliert: Über History-Constraints ist auch die Reihenfolge
von Methodenaufrufen einschränkbar. Beispielsweise ist eine Methode namens initialize in jedem Objekt nur einmal aufrufbar, davor sind keine Aufrufe anderer Methoden erlaubt. Methodenaufrufe
erfolgen durch Clients. Nur Clients können die Aufrufreihenfolge bestimmen und sind für die Einhaltung der Bedingungen verantwortlich. Manchmal ist es gar nicht möglich, die Aufrufreihenfolge im
Objektzustand abzubilden, z. B. wenn initialize in einem durch
Kopieren (clone) erzeugten Objekt ausgeführt werden soll; der kopierte Objektzustand sagt darüber ja nichts aus.
History-Constraints werden derzeit in Softwareverträgen nicht so häufig
verwendet wie die anderen Arten von Zusicherungen, hauptsächlich weil
die Regeln hinter ihnen weniger einheitlich und nicht so leicht verständlich
sind. Dennoch steckt in ihnen sehr viel Potenzial.
Vorbedingungen, Nachbedingungen, Invarianten und History-Constraints sind
verschiedene Arten von Zusicherungen (Assertions).
Zum Teil sind Vorbedingungen und Nachbedingungen bereits in der Objektschnittstelle in Form von Parameter- und Ergebnistypen von Methoden beschrieben. Typkompatibilität wird vom Compiler überprüft. In der Programmiersprache Eiffel gibt es Sprachkonstrukte, mit denen man komplexere Zusicherungen schreiben kann [28]. Diese werden zur Laufzeit überprüft. Sprachen
wie Java unterstützen überhaupt keine Zusicherungen – abgesehen von trivialen assert-Anweisungen, die sich aber nur beschränkt zur Beschreibung von
134
3.2 Ersetzbarkeit und Objektverhalten
Verträgen eignen. Sogar in Eiffel sind viele sinnvolle Zusicherungen nicht direkt
ausdrückbar. In diesen Fällen können und sollen wir Zusicherungen als Kommentare in den Programmcode schreiben und händisch überprüfen. Umgekehrt
sollen wir fast jeden Kommentar als Zusicherung lesen.
Für Interessierte, nicht Prüfungsstoff. Ein Beispiel mit Zusicherungen in Eiffel:
class ACCOUNT feature {ANY}
balance: Integer
maxOverdraft: Integer
payIn (amount: Integer) is
require amount >= 0
do balance := balance + amount
ensure balance = old balance + amount
end -{}- payIn
payOut (amount: Integer) is
require amount >= 0;
balance + maxOverdraft >= amount
do balance := balance - amount
ensure balance = old balance - amount
end -{}- payOut
invariant balance >= -maxOverdraft
end -{}- class ACCOUNT
Bei jeder Methode5 kann vor der eigentlichen Implementierung (do-Klausel) eine Vorbedingung (require-Klausel) und danach eine Nachbedingung (ensure-Klausel) stehen. Invarianten stehen ganz am Ende der Klasse, History-Constraints werden nicht
unterstützt. In jeder Zusicherung steht eine Liste von implizit durch Und verknüpften
Booleschen Ausdrücken. Sie werden zur Laufzeit zu Ja oder Nein ausgewertet. Bei
Auswertung einer Zusicherung zu Nein wird eine Exception geworfen. In Nachbedingungen ist die Bezugnahme auf Variablen- und Parameterwerte zum Zeitpunkt des
Methodenaufrufs erlaubt. So bezeichnet old balance den Wert von balance zum
Zeitpunkt des Methodenaufrufs.
Diese Klasse sollte bis auf einige syntaktische und semantische Details selbsterklärend sein. Die Klausel feature {ANY} besagt, dass die danach folgenden Methodendefinitionen sowie automatisch generierte Getter für die Variablen (deren Aufrufe
syntaktisch wie lesende Variablenzugriffe ausschauen) überall im Programm sichtbar
sind; Variablen sind nur im eigenen Objekt schreibbar. Nach dem Schlüsselwort end
und einem (in unserem Fall leeren) Kommentar kann zur besseren Lesbarkeit der
Name der Methode oder der Klasse folgen. Ende des Einschubs für Interessierte
5Genau genommen kennt Eiffel keine Methoden. Stattdessen wird streng zwischen Prozeduren (die Seiteneffekte haben und keine Ergebnisse zurückgeben) und Funktionen (die
Ergebnisse zurückgeben, aber keine Seiteneffekte haben) unterschieden. Durch die Freiheit von Seiteneffekten können Funktionen in Zusicherungen aufgerufen werden, ohne den
eigentlichen Programmablauf zu stören.
135
3 Ersetzbarkeit und Untertypen
Hier ist ein Java-Beispiel für Kommentare als Zusicherungen:
public class Account {
private long balance, maxOverdraft;
// balance >= -maxOverdraft
// add amount to balance; amount >= 0
public void payIn(long amount) {
balance = balance + amount;
}
// subtract amount from balance;
// amount >= 0; disposable() >= amount
public void payOut(long amount) {
balance = balance - amount;
}
public long disposable() { return balance+maxOverdraft; }
}
Beachten Sie, dass Kommentare in der Praxis (so wie im Beispiel) oft keine
expliziten Aussagen darüber enthalten, ob und wenn Ja, um welche Arten von
Zusicherungen es sich dabei handelt. Solche Informationen sind aus dem Kontext herauslesbar. Die erste Kommentarzeile kann nur eine Invariante darstellen,
da allgemein gültige (das heißt, nicht auf einzelne Methoden eingeschränkte)
Beziehungen zwischen Variablen hergestellt werden. Die zweite Kommentarzeile enthält gleich zwei verschiedene Arten von Zusicherungen: Die Aussage „add
amount to balance“ bezieht sich darauf, wie die Ausführung der auf den Kommentar folgenden Methode den Objektzustand verändert. Das kann nur eine
Nachbedingung sein. Nachbedingungen lesen sich häufig wie Beschreibungen
dessen, was eine Methode tut. Aber die Aussage „amount ≥ 0“ bezieht sich auf
eine erwartete Eigenschaft eines Parameters und ist daher eine Vorbedingung
von payIn. Mit derselben Begründung ist „subtract amount from balance“ eine
Nachbedingung und sind „amount ≥ 0“ und „disposable() ≥ amount“ Vorbedingungen von payOut. Die Methode disposable ist nötig, damit Clients die
Bedingung prüfen können; die Objektvariablen sind ja nicht zugreifbar.
Nebenbei bemerkt sollen Geldbeträge wegen möglicher Rundungsfehler niemals durch Gleitkommazahlen dargestellt werden. Verwenden Sie lieber wie in
obigem Beispiel ausreichend große ganzzahlige Typen oder noch besser spezielle Typen für Geldbeträge. Aufgrund komplexer Rundungsregeln sind in der
Praxis fast immer spezielle Typen nötig.
Bisher haben wir die Begriffe Typ (bzw. Schnittstelle) und Signatur (bei
nominalen Typen zusammen mit Namen) als im Wesentlichen gleichbedeutend
angesehen. Ab jetzt betrachten wir Zusicherungen, unabhängig davon, ob sie
durch eigene Sprachkonstrukte oder in Kommentaren beschrieben sind, als zum
Typ (und zur Schnittstelle) eines Objekts gehörend. Ein nominaler Typ besteht
demnach aus
136
3.2 Ersetzbarkeit und Objektverhalten
• dem Namen einer Klasse, eines Interfaces oder elementaren Typs,
• der entsprechenden Signatur
• und den dazugehörenden Zusicherungen.
Der Name sollte eine kurze Beschreibung des Zwecks der Objekte des Typs geben und der Abstraktion dienen. Die Signatur enthält alle vom Compiler überprüfbaren Bestandteile des Vertrags zwischen Clients und Server. Zusicherungen
enthalten alle über die Abstraktion durch Namen hinausgehenden Vertragsbestandteile, die nicht vom Compiler überprüft werden. Wir gehen hier davon aus,
dass Zusicherungen Kommentare und alle Kommentare Zusicherungen sind.
In Abschnitt 3.1.2 haben wir gesehen, dass Typen wegen der besseren Wartbarkeit stabil sein sollen. Solange eine Programmänderung den Typ der Klasse
unverändert lässt oder nur auf unbedenkliche Art und Weise erweitert (siehe
Abschnitt 3.2.2), hat die Änderung keine Auswirkungen auf andere Programmteile. Das betrifft auch Zusicherungen. Eine Programmänderung kann sich sehr
wohl auf andere Programmteile auswirken, wenn dabei eine Zusicherung (= ein
Kommentar) geändert wird.
Faustregel: Zusicherungen sollen stabil bleiben. Das ist für Zusicherungen in Typen nahe an der Wurzel der Typhierarchie ganz
besonders wichtig.
Wir können die Genauigkeit der Zusicherungen selbst bestimmen. Dabei sind
Auswirkungen der Zusicherungen zu beachten: Clients dürfen sich nur auf das
verlassen, was in der Signatur und in den Zusicherungen vom Server zugesagt
wird, und der Server auf das, was von den Clients zugesagt wird. Beispiele dafür
folgen in Abschnitt 3.2.2. Sind die Zusicherungen sehr genau, können sich die
Clients auf viele Details des Servers verlassen und auch der Server kann von
den Clients viel verlangen. Aber Programmänderungen werden mit größerer
Wahrscheinlichkeit dazu führen, dass Zusicherungen geändert werden müssen,
wovon alle Clients betroffen sind. Steht hingegen in den Zusicherungen nur das
Nötigste, sind Clients und Server relativ unabhängig voneinander. Der Typ ist
bei Programmänderungen eher stabil. Aber vor allem die Clients dürfen sich
nur auf Weniges verlassen. Wenn keine Zusicherungen gemacht werden, dürfen
sich Clients auf nichts verlassen, was nicht aus der Signatur folgt.
Faustregel: Zur Verbesserung der Wartbarkeit sollen Zusicherungen
keine unnötigen Details festlegen.
Zusicherungen bieten viele Möglichkeiten zur Gestaltung der Client-ServerBeziehungen. Aus Gründen der Wartbarkeit sollen wir Zusicherungen aber nur
dort einsetzen, wo tatsächlich Informationen benötigt werden, die über jene in
der Signatur hinausgehen. Wir sollen Zusicherungen einsetzen, um den Klassenzusammenhalt zu maximieren und die Objektkopplung zu minimieren. In obigem Konto-Beispiel wäre es besser, die Vorbedingung „disposable() ≥ amount“
wegzulassen und die Einhaltung der Bedingung direkt in der Implementierung
137
3 Ersetzbarkeit und Untertypen
von payOut durch eine if-Anweisung zu überprüfen. Dann ist nicht der Client
für die Einhaltung der Bedingung verantwortlich, sondern der Server.
Die Vermeidung unnötiger Zusicherungen zielt darauf ab, dass Client und
Server als relativ unabhängig voneinander angesehen werden können. Die Wartbarkeit wird dadurch natürlich nur dann verbessert, wenn diese Unabhängigkeit
tatsächlich gegeben ist. Ein äußerst unerwünschter Effekt ergibt sich, wenn wir
Zusicherungen (= nötige Kommentare) einfach aus Bequemlichkeit nicht in den
Programmcode schreiben, der Client aber trotzdem bestimmte Eigenschaften
vom Server erwartet (oder umgekehrt), also beispielsweise implizit voraussetzt,
dass jeder Einzahlungsbetrag positiv ist. In diesem Fall haben wir die Abhängigkeiten zwischen Client und Server nur versteckt. Wegen der Abhängigkeiten können Programmänderungen zu unerwarteten Fehlern führen, die wir nur
schwer finden, da die Abhängigkeiten nicht offensichtlich sind. Es sollen alle
verwendeten Zusicherungen explizit im Programmcode stehen, außer solchen,
die einschlägig geschulte Personen als ohnehin immer gültig betrachten. Andererseits sollen Client und Server so unabhängig wie möglich bleiben.
Faustregel: Alle von geschulten Personen nicht in jedem Fall erwarteten, aber vorausgesetzten Eigenschaften sollen explizit als Zusicherungen im Programm stehen.
Sprechende Namen sagen viel darüber aus, wofür Typen und Methoden gedacht sind. Namen implizieren damit die wichtigsten Zusicherungen. Beispielsweise wird eine Methode insert in einem Objekt von Set ein Element zu einer
Menge hinzufügen. Darauf werden sich Clients verlassen, auch wenn dieses Verhalten nicht durch explizite Kommentare spezifiziert ist. Trotzdem schadet es
nicht, wenn das Verhalten zusätzlich als Kommentar beschrieben ist, da Kommentare den Detailiertheitsgrad viel besser angeben können als aus den Namen
hervorgeht. Kommentare und Namen müssen in Einklang zueinander stehen.
Allgemein erwartete Eigenschaften gelten auch dann, wenn Namen und Kommentare nichts darüber aussagen. Diese Eigenschaften werden immer angenommen, außer Namen oder Kommentare besagen das Gegenteil. So darf keine Methode eine Datenstruktur zerstören oder beobachtbar verändern, wenn Namen
oder Kommentare das nicht implizieren.
Meist finden wir bei Klassen und Interfaces Kommentare, die den dahinter stehenden abstrakten Datentyp erläutern und Beispiele für die Anwendung
geben. Solche Kommentare sind häufig nicht direkt einzelnen Arten von Zusicherungen zuordenbar, hängen aber doch mit den Zusicherungen zusammen.
Im Wesentlichen geben sie einen Kontext vor, in dem die Zusicherungen bei den
Methoden und Variablendeklarationen zu verstehen sind. Abstrakte Datentypen abstrahieren über die reale Welt. Beschreibungen der abstrakten Datentypen legen auch die Beziehung zur realen Welt fest und klären, welche Aspekte
der realen Welt gedanklich in die Softwarewelt übertragen werden. Implizit
ergeben sich dadurch Zusicherungen: Die aus der realen Welt übernommenen
Eigenschaften dürfen nicht verletzt werden. Sie gelten auch dann, wenn sie nicht
explizit als Zusicherungen angeschrieben sind.
138
3.2 Ersetzbarkeit und Objektverhalten
3.2.2 Untertypen und Verhalten
Zusicherungen müssen in Untertypen beachtet werden. Auch für Zusicherungen
gilt das Ersetzbarkeitsprinzip bei der Feststellung, ob ein Typ Untertyp eines
anderen Typs ist. Neben den Bedingungen, die wir in Abschnitt 3.1 kennen
gelernt haben, müssen folgende Bedingungen gelten, damit ein Typ U Untertyp
eines Typs T ist [24]:
Vorbedingung: Vorbedingungen auf einer Methode in T müssen Vorbedingungen auf der entsprechenden Methode in U implizieren. Vorbedingungen
in Untertypen können schwächer, dürfen aber nicht stärker sein als entsprechende Vorbedingungen in Obertypen. Der Grund liegt darin, dass
ein Aufrufer der Methode, der nur T kennt, nur die Erfüllung der Vorbedingungen in T sicherstellen kann, auch wenn die Methode tatsächlich in
U statt T aufgerufen wird. Daher müssen die Vorbedingung in U automatisch erfüllt sein, wenn sie in T erfüllt sind. Werden Vorbedingungen in
U aus T übernommen, können sie mittels Oder-Verknüpfungen schwächer
werden. Ist eine Vorbedingung in T zum Beispiel „x > 0“, kann sie in U
auch „x > 0 oder x = 0“, also abgekürzt „x ≥ 0“ lauten.
Nachbedingung: Nachbedingungen auf einer Methode in U müssen Nachbedingungen auf der entsprechenden Methode in T implizieren. Das heißt,
Nachbedingungen in Untertypen können stärker, dürfen aber nicht schwächer sein als entsprechende Nachbedingungen in Obertypen. Der Grund
liegt darin, dass ein Aufrufer der Methode, der nur T kennt, sich auf
die Erfüllung der Nachbedingungen in T verlassen kann, auch wenn die
Methode tatsächlich in U statt T aufgerufen wird. Daher müssen die
Nachbedingungen in T automatisch erfüllt sein, wenn ihre Entsprechungen in U erfüllt sind. Werden Nachbedingungen in U aus T übernommen, können sie mittels Und-Verknüpfungen stärker werden. Lautet eine
Nachbedingung in T zum Beispiel „result > 0“, kann sie in U auch
„result > 0 und result > 2“, also „result > 2“ sein.
Invariante: Invarianten in U müssen Invarianten in T implizieren. Das heißt,
Invarianten in Untertypen können stärker, dürfen aber nicht schwächer
sein als Invarianten in Obertypen. Der Grund liegt darin, dass ein Client,
der nur T kennt, sich auf die Erfüllung der Invarianten in T verlassen
kann, auch wenn tatsächlich ein Objekt von U statt einem von T verwendet wird. Der Server kennt seinen eigenen spezifischsten Typ, weshalb
das Ersetzbarkeitsprinzip aus der Sicht des Servers nicht erfüllt zu sein
braucht. Die Invarianten in T müssen automatisch erfüllt sein, wenn sie in
U erfüllt sind. Wenn Invarianten in U aus T übernommen werden, können
sie, wie Nachbedingungen, mittels Und-Verknüpfungen stärker werden.
Die Begründung geht davon aus, dass Objektvariablen nicht von außen
verändert werden. Ist dies doch der Fall, so müssen Invarianten, die sich
auf von außen änderbare Variablen beziehen, in U und T übereinstimmen.
Beim Schreiben einer solchen Variable muss die Invariante vom Client
139
3 Ersetzbarkeit und Untertypen
überprüft werden, was dem generellen Konzept widerspricht. Außerdem
kann ein Client die Invariante gar nicht überprüfen, wenn in der Bedingung vorkommende Variablen und Methoden nicht öffentlich zugänglich
sind. Daher sollen Objektvariablen nicht von außen änderbar sein.
Server-kontrollierter History-Constraint: Dafür gilt das Gleiche wie für Invarianten. Es ist jedoch nicht so einfach, von stärkeren oder schwächeren
Bedingungen zu sprechen, da viel von der konkreten Formulierung der
Bedingungen abhängt. Einfacher und klarer ist es, die Konsequenzen gegenüberzustellen. Für alle Objektzustände x und y in U und T (wobei ein
Objektzustand die Werte aller gemeinsamen Variablen der Objekte von
U und T widerspiegelt) soll gelten: Wenn Server-kontrollierte HistoryConstraints in T ausschließen, dass ein Objekt von T im Zustand x durch
Veränderungen im Laufe der Zeit in den Zustand y kommt, dann müssen auch Server-kontrollierte History-Constraints in U ausschließen, dass
ein Objekt von U im Zustand x durch Veränderungen im Laufe der Zeit
in den Zustand y kommt. Einschränkungen auf T müssen also auch auf
U gelten. Damit wird sichergestellt, dass ein Client sich auch dann auf
die ihm bekannte Einschränkung in T verlassen kann, wenn statt einem
Objekt von T eines von U verwendet wird. Es ist möglich, dass U die
Entwicklung der Zustände stärker einschränkt als T, solange Clients betroffene Variablen nicht von außen schreiben können. Werden Variablen
von außen geschrieben, müssen auch Clients für die Einhaltung der Serverkontrollierten History-Constraints sorgen, und die Bedingungen in U und
T müssen übereinstimmen.
Client-kontrollierter History-Constraint: Dafür gilt das Gleiche wie für Vorbedingungen, jedoch bezogen auf Einschränkungen in der Reihenfolge
von Methodenaufrufen. Eine Reihenfolge von Methodenaufrufen heißt
Trace, die meist unendlich große Menge aller möglichen (= erlaubten)
Traces ist ein Trace-Set. Jede entsprechend T erlaubte Aufrufreihenfolge muss auch entsprechend U erlaubt sein. Es ist jedoch möglich, dass
U mehr Aufrufreihenfolgen erlaubt als T, wodurch die Einschränkungen
in U schwächer sind als in T. Das durch Client-kontrollierte HistoryConstraints in T beschriebene Trace-Set muss also eine Teilmenge des
durch Client-kontrollierte History-Constraints in U beschriebenen TraceSets sein. Wenn die Clients an ein Objekt Nachrichten in einer durch T
erlaubten Reihenfolge schicken, so ist sichergestellt, dass das Objekt vom
Typ U die entsprechenden Methoden auch in dieser Reihenfolge ausführen kann. Wir müssen bei der Überprüfung Client-kontrollierter HistoryConstraints im Allgemeinen die Menge aller Clients betrachten, nicht nur
einen einzelnen Client, da der Server ja alle Methodenaufrufe nur in eingeschränkter Reihenfolge ausführen kann, nicht nur die Aufrufe, die von
einem einzelnen Client kommen.
Im Prinzip lassen sich obige Bedingungen auch formal überprüfen. In Programmiersprachen wie Eiffel, in denen Zusicherungen formal definiert sind, wird
140
3.2 Ersetzbarkeit und Objektverhalten
das tatsächlich (hinsichtlich Untertypbeziehungen vom Compiler) gemacht, abgesehen davon, dass Eiffel keine History-Constraints kennt. Aber bei Verwendung anderer Programmiersprachen sind Zusicherungen meist nicht formal, sondern nur umgangssprachlich als Kommentare gegeben. Unter diesen Umständen
ist keine formale Überprüfung möglich. Daher müssen wir beim Programmieren
alle nötigen Überprüfungen per Hand durchführen. Es muss gelten, dass
• obige Bedingungen für Untertypbeziehungen eingehalten werden,
• die Server alle Nachbedingungen, Invarianten und Server-kontrollierten
History-Constraints erfüllen und nur voraussetzen, was in Vorbedingungen, Invarianten und History-Constraints festgelegt ist
• die Clients in Aufrufen alle Vorbedingungen und Client-kontrollierten
History-Constraints erfüllen und nur voraussetzen, was durch Nachbedingungen, Invarianten und History-Constraints zugesichert wird.
Es kann sehr aufwändig sein, alle solchen Überprüfungen vorzunehmen. Einfacher geht es, wenn wir während der Code-Erstellung und bei Änderungen stets
an die einzuhaltenden Bedingungen denken, die Überprüfungen also nebenbei
machen. Wichtig ist darauf zu achten, dass die Zusicherungen unmissverständlich formuliert sind. Nach Änderung einer Zusicherung ist die Überprüfung besonders schwierig. Die Änderung einer Zusicherung ohne gleichzeitige Änderung
aller betroffenen Programmteile ist eine häufige Fehlerursache in Programmen.
Faustregel: Zusicherungen sollen unmissverständlich formuliert sein
und während der Programmentwicklung und Wartung ständig bedacht werden.
Betrachten wir ein Beispiel:6
public class Set {
public void insert(int x) {
// inserts x into set iff not already there;
// x is in set immediately after invocation
...;
}
public boolean inSet(int x) {
// returns true if x is in set, otherwise false
...;
}
}
Die Methode insert fügt eine ganze Zahl genau dann („iff“ ist eine übliche
Abkürzung für „if and only if“) in ein Objekt von Set ein, wenn sie noch nicht
6Je nach Programmierstil stehen Zusicherungen vor oder nach Variablendeklarationen sowie
vor oder nach Köpfen von Methoden oder Konstruktoren. In der Regel ist leicht erkennbar,
wo sie dazugehören. Innerhalb eines Programms sollte der Stil möglichst einheitlich sein.
141
3 Ersetzbarkeit und Untertypen
in dieser Menge ist. Unmittelbar nach Aufruf der Methode ist die Zahl in jedem
Fall in der Menge. Die Methode inSet stellt fest, ob eine Zahl in der Menge ist
oder nicht. Dieses Verhalten der Objekte von Set ist durch die Zusicherungen
in den Kommentaren festgelegt. Wenn wir den Inhalt dieser Beschreibungen
von Methoden genauer betrachten, sehen wir, dass es sich dabei um Nachbedingungen handelt. Da Nachbedingungen festlegen, was sich ein Client vom
Aufruf einer Methode erwartet, lesen sich Nachbedingungen oft tatsächlich wie
Beschreibungen von Methoden.
Folgende Klasse unterscheidet sich von Set nur durch einen zusätzlichen
Server-kontrollierten History-Constraint:
public class SetWithoutDelete extends Set {
// elements in the set always remain in the set
}
Eine Zahl, die einmal in der Menge war, soll stets in der Menge bleiben. Offensichtlich ist SetWithoutDelete ein Untertyp von Set, da nur ein vom Server
kontrollierter History-Constraint dazugefügt wird, welcher die zukünftige Entwicklung des Objektzustands gegenüber Set einschränkt.
Sehen wir uns eine kurze Codesequenz für einen Client an:
Set s = new Set();
s.insert(41);
doSomething(s);
if (s.inSet(41)) { doSomeOtherThing(s); }
else { doSomethingElse(); }
Während der Ausführung von doSomething könnte s verändert werden. Es ist
nicht ausgeschlossen, dass 41 dabei aus der Menge gelöscht wird, da die Nachbedingung von insert in Set ja nur zusichert, dass 41 unmittelbar nach dem
Aufruf von insert in der Menge ist. Bevor wir die Methode doSomeOtherThing
aufrufen (von der wir annehmen, dass sie ihren Zweck nur erfüllt, wenn 41 in
der Menge ist), stellen wir sicher, dass 41 tatsächlich in der Menge ist. Dies
geschieht durch Aufruf von inSet.
Verwenden wir ein Objekt von SetWithoutDelete anstatt einem von Set,
ersparen wir uns den Aufruf von inSet. Wegen der stärkeren Zusicherung ist
41 sicher in der Menge:
SetWithoutDelete s = new SetWithoutDelete();
s.insert(41);
doSomething(s);
doSomeOtherThing(s); // s.inSet(41) returns true
Von diesem kleinen Vorteil von SetWithoutDelete dürfen wir uns nicht dazu
verleiten lassen, generell starke Einschränkungen in Zusicherungen zu verwenden. Solche Einschränkungen erschweren die Wartung (siehe Abschnitt 3.2.1).
Als triviales Beispiel können wir Set leicht um eine Methode delete (zum
Löschen einer Zahl aus der Menge) erweitern:
142
3.2 Ersetzbarkeit und Objektverhalten
public class SetWithDelete extends Set {
public void delete(int x) {
// deletes x from the set if it is there
...;
}
}
Aber SetWithoutDelete können wir, wie der Klassenname schon sagt, nicht
auf diese Art erweitern. Jede vernünftige Nachbedingung von delete muss in
Konflikt zum History-Constraint stehen. Wir dürfen nicht zu früh festlegen,
dass es kein delete gibt, nur weil wir es gerade nicht brauchen. Invarianten wie
in SetWithoutDelete sind nur sinnvoll, wenn wir wirklich darauf angewiesen
sind. Andernfalls verbauen wir uns Wiederverwendungsmöglichkeiten.
Kommentare als Zusicherungen setzen voraus, dass wir Untertypbeziehungen explizit deklarieren, also nominale Typen verwenden. Damit bringen wir
den Compiler dazu, beliebige weitere Bedingungen (gegenüber denen in Abschnitt 3.1.1) für eine Untertypbeziehung vorauszusetzen. Beispielsweise müssen wir explizit angeben, dass SetWithoutDelete ein Untertyp von Set ist, da
sich diese Klassen für einen Compiler nur im Namen und in Kommentaren unterscheiden, deren Bedeutung der Compiler nicht kennt. Andernfalls wäre ein
Objekt von Set auch verwendbar, wo eines von SetWithoutDelete erwartet
wird. Es soll auch keine Untertypbeziehung zwischen SetWithoutDelete und
SetWithDelete bestehen, obwohl dafür alle Bedingungen aus Abschnitt 3.1.1
erfüllt sind. Sonst wäre ein Objekt von SetWithDelete verwendbar, wo ein
Objekt von SetWithoutDelete erwartet wird. Daher sind in vielen objektorientierten Sprachen Untertypen und Vererbung zu einem Konstrukt vereint: Vererbungsbeziehungen schließen zufällige Untertypbeziehungen aus, und wo eine
Untertypbeziehung besteht, ist oft (nicht immer) auch Codevererbung sinnvoll.
3.2.3 Abstrakte Klassen
Klassen, die wir bis jetzt betrachtet haben, dienen der Beschreibung der Struktur ihrer Objekte, der Erzeugung und Initialisierung neuer Objekte und der
Festlegung des spezifischsten Typs der Objekte. Im Zusammenhang mit Untertypen ist oft nur eine der Aufgaben nötig, nämlich die Festlegung des Typs. Das
ist dann der Fall, wenn im Programm keine Objekte der Klasse selbst erzeugt
werden sollen, sondern nur Objekte von Unterklassen. Aus diesem Grund unterstützen viele objektorientierte Sprachen abstrakte Klassen, von denen keine
Objekte erzeugt werden können. Interfaces in Java sind eine besondere Form
abstrakter Klassen, die keine Objektvariablen beschreiben, in der Fachliteratur
häufig Trait genannt. Abgesehen davon gilt das hier gesagte auch für Interfaces.
Nehmen wir als Beispiel folgende Klassenstruktur:
Polygon
% ↑ -
Triangle Square Hexagon
143
3 Ersetzbarkeit und Untertypen
Jede Unterklasse von Polygon beschreibt ein z. B. am Bildschirm darstellbares
Vieleck. Polygon selbst beschreibt keine bestimmte Anzahl von Ecken, sondern
fasst nur die Menge aller Vielecke zusammen. Wenn wir eine Liste unterschiedlicher Vielecke benötigen, werden wir den Typ der Vielecke in der Liste mit
Polygon festlegen, obwohl in der Liste tatsächlich nur Objekte von Triangle,
Square und Hexagon vorkommen. Es werden keine Objekte der Klasse Polygon
selbst benötigt, sondern nur Objekte der Unterklassen. Polygon ist ein typischer
Fall einer abstrakten Klasse.
In Java sieht die abstrakte Klasse etwa so aus:
public abstract class Polygon {
public abstract void draw();
// draw a polygon on the screen
}
Da die Klasse abstrakt ist, ist die Ausführung von new Polygon() nicht zulässig. Aber Unterklassen sind von Polygon ableitbar. Jede Unterklasse muss eine
Methode draw enthalten, da diese Methode in Polygon deklariert ist. Genaugenommen ist draw als abstrakte Methode deklariert, als Signatur mit einem
Kommentar als Zusicherung. Wir brauchen keine Implementierung in Polygon,
da diese Methode ohnehin nicht ausgeführt wird; es gibt ja keine Objekte der
Klasse Polygon, wohl aber Objekte des Typs Polygon. Nicht-abstrakte Unterklassen, das sind konkrete Klassen, müssen Implementierungen für abstrakte
Methoden bereitstellen, diese also überschreiben. Abstrakte Unterklassen müssen abstrakte Methoden nicht überschreiben. Neben Variablen (außer in JavaInterfaces) und abstrakten Methoden dürfen abstrakte Klassen auch konkrete
(also implementierte) Methoden enthalten, die wie üblich vererbt werden.
Die konkrete Klasse Triangle könnte so aussehen:
public class Triangle extends Polygon {
public void draw() {
// draw a triangle on the screen
...;
}
}
Auch Square und Hexagon müssen die Methode draw implementieren.
So wie in diesem Beispiel kommt es vor allem in gut faktorisierten Programmen häufig vor, dass der Großteil der Implementierungen von Methoden in
Klassen steht, die keine Unterklassen haben. Abstrakte Klassen sind eher stabil
als konkrete. Zur Verbesserung der Wartbarkeit werden neue Klassen vor allem
von stabilen Klassen abgeleitet. Außerdem werden möglichst stabile Typen für
Parameter und Variablen verwendet. Da es leichter ist, abstrakte Klassen stabil
zu halten, sind wir gut beraten, hauptsächlich solche Klassen, in Java vor allem
Interfaces, für Parameter- und Variablentypen zu verwenden.
Faustregel: Es ist empfehlenswert, als Obertypen und Parametertypen hauptsächlich abstrakte Klassen (in Java vor allem Interfaces)
zu verwenden.
144
3.3 Vererbung versus Ersetzbarkeit
Parametertypen sollen keine Bedingungen an Argumente stellen, die nicht
benötigt werden. Konkrete Klassen legen aber oft zahlreiche Bedingungen in
Form von Zusicherungen und Methoden in der Schnittstelle fest. Dieser Konflikt
ist lösbar, indem für die Typen der Parameter nur abstrakte Klassen verwendet
werden. Es ist ja leicht, zu jeder konkreten Klasse eine oder mehrere abstrakte
Klassen als Obertypen zu schreiben, die die benötigten Bedingungen möglichst
genau angeben. Damit werden unnötige Abhängigkeiten vermieden.